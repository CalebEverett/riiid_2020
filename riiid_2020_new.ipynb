{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML, clear_output\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "orpoYGXZdptj"
   },
   "outputs": [],
   "source": [
    "!curl ipinfo.io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "git.push()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     1
    ],
    "id": "N4lHWTOFrRgT"
   },
   "outputs": [],
   "source": [
    "# <hide-input>\n",
    "def show_version_history():\n",
    "    from IPython.display import HTML\n",
    "    style_header = 'mui--align-bottom mui--bg-primary mui--text-light mui--text-center'\n",
    "    style_cell = 'mui--align-top mui--text-center'\n",
    "\n",
    "    # print(('\\n').join(list(map(make_li, sorted(dtypes.keys())))))\n",
    "\n",
    "    html_str = f\"\"\"\n",
    "    <link href=\"//cdn.muicss.com/mui-0.10.3/css/mui.min.css\" rel=\"stylesheet\" type=\"text/css\" />\n",
    "    <div class=\"mui-container-fluid\">\n",
    "        <h2>Version History</h2>\n",
    "        <div style=\"max-width:1016px\" class=\"mui-row\">\n",
    "            <div class=\"mui-col-8\">\n",
    "                <table class=\"mui-table mui-table--bordered\">\n",
    "                    <tr>\n",
    "                        <th width=\"12%\" class=\"{style_header}\">Version</th>\n",
    "                        <th width=\"12%\" class=\"{style_header}\">Date</th>\n",
    "                        <th width=\"12%\" class=\"{style_header}\">Local CV</th>\n",
    "                        <th width=\"12%\" class=\"{style_header}\">Public<br>Leaderboard</th>\n",
    "                        <th class=\"{style_header} mui--align-bottom mui--bg-primary\n",
    "                            mui--text-dark mui--text-left\">Notes</th>                    \n",
    "                    </tr>\n",
    "                    <tr>\n",
    "                        <td class=\"{style_cell}\">58</td>\n",
    "                        <td class=\"{style_cell}\">2020-11-15</td>\n",
    "                        <td class=\"{style_cell}\">0.756</td>\n",
    "                        <td class=\"{style_cell}\">0.762</td>\n",
    "                        <td><ul>\n",
    "                                <li>Completed submission pipeline with minimal feature set:\n",
    "                                    <ul>\n",
    "                                        <li><code>answered_correctly_content_id_cumsum</code></li>\n",
    "                                        <li><code>answered_correctly_cumsum</code></li>\n",
    "                                        <li><code>answered_correctly_cumsum_pct</code></li>\n",
    "                                        <li><code>answered_incorrectly_content_id_cumsum</code></li>\n",
    "                                        <li><code>answered_incorrectly_cumsum</code></li>\n",
    "                                        <li><code>part</code></li>\n",
    "                                        <li><code>part_correct_pct</code></li>\n",
    "                                        <li><code>question_id_correct_pct</code></li>\n",
    "                                        <li><code>tag__0</code></li>\n",
    "                                        <li><code>tag__0_correct_pct</code></li>\n",
    "                                        <li><code>task_container_id</code></li>\n",
    "                                        <li><code>timestamp</code></li>\n",
    "                                    </ul>\n",
    "                                </li>\n",
    "                                <li>Changed logic on roll sum to be over trailing\n",
    "                                    rows preceding the current <code>task_container_id</code> instead\n",
    "                                    of over trailing task containers\n",
    "                                    (expensive)\n",
    "                                </li>\n",
    "                            </ul>\n",
    "                        </td>\n",
    "                    </tr>\n",
    "                    <tr>\n",
    "                        <td class=\"{style_cell}\">53</td>\n",
    "                        <td class=\"{style_cell}\">2020-11-07</td>\n",
    "                        <td class=\"{style_cell}\">0.761</td>\n",
    "                        <td class=\"{style_cell}\">--</td>\n",
    "                        <td><ul><li>Housekeeping:\n",
    "                                    <ul>\n",
    "                                        <li>Consolidated notebook and modules in single repo</li>\n",
    "                                        <li>Streamlined Colab repo workflow using Drive</>\n",
    "                                        <li>Included modules in notebook when pushed to Kaggle</li>\n",
    "                                        <li>Eliminated CONFIG requirement when run in Kaggle</li>\n",
    "                                    </ul>\n",
    "                                </li>\n",
    "                            </ul>\n",
    "                        </td>\n",
    "                    </tr>\n",
    "                    <tr>\n",
    "                        <td class=\"{style_cell}\">40</td>\n",
    "                        <td class=\"{style_cell}\">2020-11-05</td>\n",
    "                        <td class=\"{style_cell}\">0.761</td>\n",
    "                        <td class=\"{style_cell}\">--</td>\n",
    "                        <td>\n",
    "                            <ul>\n",
    "                                <li>Features added:\n",
    "                                    <ul>\n",
    "                                        <li><code>answered_correctly_content_id_cumsum</code></li>\n",
    "                                        <li><code>answered_correctly_content_id_cumsum_pct</code></li>\n",
    "                                        <li><code>answered_correctly_cumsum10</code></li>\n",
    "                                        <li><code>answered_correctly_cumsum_pct</code></li>\n",
    "                                        <li><code>answered_correctly_rollsum_pct</code></li>\n",
    "                                        <li><code>answered_incorrectly_content_id_cumsum</code></li>\n",
    "                                        <li><code>lectures_cumcount</code></li>\n",
    "                                        <li><code>prior_question_elapsed_time_rollavg</code></li>\n",
    "                                    </ul>\n",
    "                                </li>\n",
    "                                <li>Single model, single fold</li>\n",
    "                                <li>No public leaderboard - efficient inference in progress</li>\n",
    "                                <li>Refactored code to move queries and helper functions into\n",
    "                                    separate modules</li>\n",
    "                                <li>Completed set up to commit code to Github from Colab and</li>\n",
    "                                <li>Completed set up to push kernels to Kaggle from Colab</li>\n",
    "                            </ul>\n",
    "                        </td>\n",
    "                    </tr>\n",
    "                    <tr>\n",
    "                        <td class=\"{style_cell}\">37</td>\n",
    "                        <td class=\"{style_cell}\">2020-11-04</td>\n",
    "                        <td class=\"{style_cell}\">0.751</td>\n",
    "                        <td class=\"{style_cell}\">0.748</td>\n",
    "                        <td>\n",
    "                            <ul>\n",
    "                                <li>Features added:\n",
    "                                    <ul>\n",
    "                                        <li><code>answered_correctly_cumsum</code></li>\n",
    "                                        <li><code>answered_correctly_rollsum</code></li>\n",
    "                                        <li><code>answered_incorrectly_cumsum</code></li>\n",
    "                                        <li><code>answered_incorrectly_rollsum</code></li>\n",
    "                                        <li><code>part</code></li>\n",
    "                                        <li><code>part_correct_pct</code></li>\n",
    "                                        <li><code>question_id_correct_pct</code></li>\n",
    "                                        <li><code>tag__0</code></li>\n",
    "                                        <li><code>tag__0_correct_pct</code></li>\n",
    "                                    </ul>\n",
    "                                </li>\n",
    "                                <li>Single model, single fold</li>\n",
    "                                <li>Model for public leaderboard didn't include\n",
    "                                    rolling features - still working out how to\n",
    "                                    efficiently calculate for inference</li>\n",
    "                            </ul>\n",
    "                        </td>\n",
    "                    </tr>\n",
    "                </table>\n",
    "            </div>\n",
    "        </div>\n",
    "        <p>\n",
    "            <a href=\"https://colab.research.google.com/github/CalebEverett/riiid_2020/blob/master/riiid-2020.ipynb\" target=\"_blank\" rel=\"nofollow\">\n",
    "            <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a>\n",
    "        </p>\n",
    "    </div>\n",
    "    \"\"\"\n",
    "\n",
    "    html = HTML(html_str)\n",
    "    display(html)\n",
    "show_version_history()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8QCO0cr_juQA"
   },
   "source": [
    "## Intro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L1mQ0DiRnfer"
   },
   "source": [
    "This kernel is an end to end pipeline that uses BigQuery to store data and perform feature engineering, and trains a model using XGBoost. I was resorting to breaking up tables and still waiting a long time to see the results of my analysis and to process my engineered features, so I decided to learn about BigQuery. This kernel is the current state of my setup, which is working very well. It is much faster than my previous local setup, even with having to download files. It also is making it easier to keep the structure of the data and and code clean, which in turn makes it easier to stay focused on thinking about and executing ideas without getting bogged down waiting for things to finish or wading through extraneous processing code.\n",
    "\n",
    "I've attempted to put  this book together in such a way that somebody else can fork it, update a few environment variables, run it and then be in the game engineering features and improving the model. The only requirements are a GCP project and storage bucket. Other than that, it is turn key, starting with creating a BigQuery dataset and ending with a saved model and two feature tables that get uploaded to a Kaggle dataset where they are used in a separate kernel to make predictions and submit to the competition api.\n",
    "\n",
    "A couple of cool features:\n",
    "* Uses the gcs version of the competition datset to create a dataset and upload to BigQuery in around a minute\n",
    "* Transformations get run on the entire train table at once and run in under 10 minutes\n",
    "* Feature engineering gets done on a sample of the train table, taking advantage of BigQuery' graphical query editing interface that includes tab completion, syntax checking and the ability to run queries and inspect results\n",
    "* Stores queries as methods on a dedicated class, where they can be easily reused\n",
    "* Dtypes for local dataframes, schema for BigQuery tables and all tranformations are maintained locally so that the transformed tables can be recreated from the original competition dataset files automatically at any time (see description of workflow below to continue with this practice)\n",
    "* Exports to gcs using temporary tables created by BigQuery avoiding unnecessary storage and wasted time rerunning and exporting duplicate queries\n",
    "* Separate [submission kernel](https://www.kaggle.com/calebeverett/riiid-submit) uses sqlite3 to achieve sub two hour submission times while maintaining state for questions, users and user-content (80+ million rows)\n",
    "\n",
    "I've engineered a few features as a starting point to demonstrate how additional features can be efficiently developed and processed, including:\n",
    "* Cumulative and rolling sums of questions answered correctly and incorrectly by user\n",
    "* Percent of questions answered correctly by question id, part and the first question tag\n",
    "\n",
    "The model is also just a starting point, with a first pass at a train/validation split and no hyperparameter tuning. I have included some basic diagnostics on both the train/validtion split and model performance as a starting place for further development. Trained on a small subset of the overall training data with a small number of engineered features, it is producing a local validation AUC score of around 0.75 and slighly less than that on the public leaderboard.\n",
    "\n",
    "I have the table creation and transformation functions set to not run, but you can set them to run, by changing the flags to `True` for:\n",
    "* Loading tables - one flag for the questions table and another for the train and lectures tables\n",
    "* Updating the schemas in BigQuery\n",
    "* Performing the transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "savju5R9juQB"
   },
   "source": [
    "## Resources\n",
    "* [BigQuery Console](https://console.cloud.google.com/bigquery)\n",
    "* [Python Client for Google BigQuery](https://googleapis.dev/python/bigquery/latest/index.html)\n",
    "* [Analytic function concepts in Standard SQL](https://cloud.google.com/bigquery/docs/reference/standard-sql/analytic-function-concepts)\n",
    "* [XGBoost Documentation](https://xgboost.readthedocs.io/en/latest/index.html)\n",
    "* [Storge Client](https://googleapis.dev/python/storage/latest/client.html)\n",
    "* [pandas documentation](https://pandas.pydata.org/docs/)\n",
    "* [Plotly Python Open Source Graphing Library](https://plotly.com/python/)\n",
    "* [PEP 8 -- Style Guide for Python Code](https://www.python.org/dev/peps/pep-0008/)\n",
    "* [Comet.ml Experiment API](https://www.comet.ml/docs/python-sdk/Experiment/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "88t93RZzj982"
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# <hide-input>\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tKe4S7M4nbSU"
   },
   "outputs": [],
   "source": [
    "# <hide-input>\n",
    "\n",
    "from datetime import datetime\n",
    "import gc\n",
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "import random\n",
    "import re\n",
    "import subprocess\n",
    "import sys\n",
    "import time\n",
    "\n",
    "import ipywidgets as widgets\n",
    "from google.cloud import storage, bigquery\n",
    "from google.cloud.bigquery import SchemaField\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "BUCKET = 'caleb-riiid'\n",
    "DATASET = 'data'\n",
    "LOCATION = 'europe-west4'\n",
    "KAGGLE_SUBMIT_DATASET = 'riiid-submission-private'\n",
    "PROJECT = 'fastai-caleb'\n",
    "REPO = 'riiid_2020'\n",
    "NOT_KAGGLE = os.getenv('KAGGLE_URL_BASE') is None\n",
    "\n",
    "# if NOT_KAGGLE:\n",
    "#     from google.colab import drive\n",
    "#     DRIVE = Path('/content/drive/My Drive')\n",
    "#     if not DRIVE.exists():\n",
    "#         drive.mount(str(DRIVE.parent))\n",
    "#     sys.path.append(str(DRIVE))\n",
    "#     g_creds_path = 'credentials/riiid-caleb-faddd0c9d900.json'\n",
    "#     os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = str(DRIVE/g_creds_path)\n",
    "\n",
    "bucket = storage.Client(project=PROJECT).get_bucket(BUCKET)\n",
    "dataset = bigquery.Dataset(f'{PROJECT}.{DATASET}')\n",
    "bq_client = bigquery.Client(project=PROJECT, location=LOCATION)\n",
    "\n",
    "if NOT_KAGGLE:\n",
    "    CONFIG = json.loads(bucket.get_blob('config.json').download_as_string())\n",
    "    os.environ = {**os.environ, **CONFIG}\n",
    "    sys.path.append('/home/jupyter')\n",
    "    from riiid_2020.utilities import check_packages, Git\n",
    "    from riiid_2020.bqhelpers import BQHelper\n",
    "    from riiid_2020.queries import Queries\n",
    "    \n",
    "    git = Git(REPO, CONFIG.get('GIT_USERNAME'), CONFIG.get('GIT_PASSWORD'),\n",
    "              CONFIG.get('EMAIL'), Path('../'))\n",
    "\n",
    "#     packages = {\n",
    "#         'comet-ml': ('3.2.5',''),\n",
    "#         'gcsfs': ('0.7.1',''),\n",
    "#         'kaggle': ('1.5.10',''),\n",
    "#         'plotly': ('4.12.0',''),\n",
    "#         'xgboost': ('1.2.0','')\n",
    "#         # 'lightgbm': ('3.1.0',' --install-option=--gpu')\n",
    "#     }\n",
    "#     check_packages(packages)\n",
    "\n",
    "    from comet_ml import APIExperiment, Experiment\n",
    "    from kaggle.api.kaggle_api_extended import KaggleApi\n",
    "    kaggle_api = KaggleApi()\n",
    "    kaggle_api.authenticate()\n",
    "\n",
    "import plotly\n",
    "import plotly.express as px\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "import tensorflow as tf\n",
    "import xgboost as xgb\n",
    "pd.options.plotting.backend = 'plotly'\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_strategy():\n",
    "\n",
    "    try:\n",
    "        tpu = tf.distribute.cluster_resolver.TPUClusterResolver('tpu-1')\n",
    "        print('Running on TPU ', tpu.master())\n",
    "    except:\n",
    "        tpu = None\n",
    "\n",
    "    if tpu:\n",
    "        tf.config.experimental_connect_to_cluster(tpu)\n",
    "        tf.tpu.experimental.initialize_tpu_system(tpu)\n",
    "        strategy = tf.distribute.TPUStrategy(tpu)\n",
    "    \n",
    "    else:\n",
    "        strategy = tf.distribute.get_strategy()\n",
    "        for d in tf.config.list_physical_devices():\n",
    "            print(d)\n",
    "            \n",
    "    return strategy\n",
    "\n",
    "strategy = get_strategy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3I6-yy4QBxlR"
   },
   "source": [
    "## Modules\n",
    "Included in notebook for convenience when in a Kaggle kernel. Github repo [here](https://github.com/CalebEverett/riiid_2020)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "--uRkD6w0EhS"
   },
   "outputs": [],
   "source": [
    "# <include-bqhelpers.py><hide-input>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dTP7zTkV0qhK"
   },
   "outputs": [],
   "source": [
    "# <include-queries.py><hide-input>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FWx9otMx1O2Y"
   },
   "outputs": [],
   "source": [
    "# <include-utilities.py><hide-input>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r4nP2GeV09FG"
   },
   "outputs": [],
   "source": [
    "# <include-config.json><hide-input><hide-output>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WO6DVIxuQydR"
   },
   "outputs": [],
   "source": [
    "Q = Queries(DATASET)\n",
    "bqh = BQHelper(bucket, DATASET, bq_client)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "id": "cs6bXr35juQM"
   },
   "source": [
    "## Create BigQuery Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "MVloHU3wjuQQ"
   },
   "outputs": [],
   "source": [
    "if False:\n",
    "    delete_contents=False\n",
    "    bq_client.delete_dataset(DATASET, delete_contents=delete_contents)\n",
    "    print(f'Dataset {dataset.dataset_id} deleted from project {dataset.project}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "y2-E_hTejuQO"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    dataset = bq_client.get_dataset(dataset.dataset_id)\n",
    "    print(f'Dataset {dataset.dataset_id} already exists '\n",
    "          f'in location {dataset.location} in project {dataset.project}.')\n",
    "except:\n",
    "    dataset = bq_client.create_dataset(dataset)\n",
    "    print(f'Dataset {dataset.dataset_id} created '\n",
    "          f'in location {dataset.location} in project {dataset.project}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HItb6CWGjuQS"
   },
   "source": [
    "## Load Tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BjTuu8CJjuQT"
   },
   "source": [
    "### Dataframe dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8dxMzeCsjuQT"
   },
   "outputs": [],
   "source": [
    "# <hide-input>\n",
    "dtypes_orig = {\n",
    "    'lectures': {\n",
    "        'lecture_id': 'uint16',\n",
    "        'tag': 'uint8',\n",
    "        'part': 'uint8',\n",
    "        'type_of': 'str',\n",
    "    },\n",
    "    'questions': {\n",
    "        'question_id': 'uint16',\n",
    "        'bundle_id': 'uint16',\n",
    "        'correct_answer': 'uint8',\n",
    "        'part': 'uint8',\n",
    "        'tags': 'str',\n",
    "        \n",
    "    },\n",
    "    'train': {\n",
    "        'row_id': 'int64',\n",
    "        'timestamp': 'int64',\n",
    "        'user_id': 'int32',\n",
    "        'content_id': 'int16',\n",
    "        'content_type_id': 'int8',\n",
    "        'task_container_id': 'int16',\n",
    "        'user_answer': 'int8',\n",
    "        'answered_correctly': 'int8',\n",
    "        'prior_question_elapsed_time': 'float32', \n",
    "        'prior_question_had_explanation': 'bool',\n",
    "    }\n",
    "    \n",
    "}\n",
    "\n",
    "dtypes_new = {\n",
    "    'lectures': {},\n",
    "    'questions': {},\n",
    "    'train': {\n",
    "        'ql_id': 'int16',\n",
    "        'fold': 'int8',\n",
    "        'l_cumcnt': 'int16',\n",
    "        'l_cumcnt_part': 'int16',\n",
    "        'l_cumcnt_tags': 'int16',\n",
    "        'l_cumcnt_part_tags': 'int16', #this should come out on the next run\n",
    "        'l_cumcnt_session': 'int16',\n",
    "        'session': 'uint8',\n",
    "        'r_cumcnt_clip': 'int16',\n",
    "        'task_container_id_q': 'int16',\n",
    "        'pqet_current': 'int32',\n",
    "        'ts_delta': 'int32',\n",
    "    }\n",
    "}\n",
    "\n",
    "dtypes_content_tags = {\n",
    "    'ql_id': 'int16',\n",
    "    'question_id': 'int16',\n",
    "    'lecture_id': 'int16',\n",
    "    'bundle_id': 'uint16',\n",
    "    'correct_answer': 'uint8',\n",
    "    'part': 'int8',\n",
    "    'tags': 'str',\n",
    "    'tags_array': 'str',\n",
    "    'part_correct_pct': 'int8',\n",
    "    'part_tags_correct_pct': 'int8',\n",
    "    'question_id_correct_pct': 'int8',\n",
    "    'tags_correct_pct': 'int8',\n",
    "    'tags_code': 'int16',\n",
    "    'part_pqet_avg': 'int32',\n",
    "    'question_id_pqet_avg': 'int32',\n",
    "    'tags_pqet_avg': 'int32',\n",
    "    'part_tags_pqet_avg': 'int32'\n",
    "}\n",
    "\n",
    "# each of these gets cumsums for correct and\n",
    "# incorrect, row count and pct correct\n",
    "cumsum_cols = {\n",
    "    'ac_cumsum': 'int16',\n",
    "    'ac_cumsum_content_id': 'int16',\n",
    "    'ac_cumsum_part': 'int16',\n",
    "    'ac_cumsum_session': 'int16',\n",
    "    'ac_cumsum_tags': 'int16',\n",
    "    'ac_cumsum_upto': 'int8',\n",
    "    'ac_cumsum_part_tags': 'int16'\n",
    "}\n",
    "\n",
    "for c, t in cumsum_cols.items():\n",
    "    dtypes_new['train'][c] = t\n",
    "    r_col = c.replace('ac_cumsum', 'r_cumcnt')\n",
    "    dtypes_new['train'][r_col] = t\n",
    "    p_col = c.replace('ac_cumsum', 'ac_cumsum_pct')\n",
    "    dtypes_new['train'][p_col] = 'int8'\n",
    "    pqs_col = c.replace('ac_cumsum', 'pqet_cumsum')\n",
    "    dtypes_new['train'][pqs_col] = 'int64'\n",
    "    pqa_col = c.replace('ac_cumsum', 'pqet_cumavg')\n",
    "    dtypes_new['train'][pqa_col] = 'int32'\n",
    "\n",
    "    \n",
    "dtypes = {}\n",
    "for table_id in dtypes_orig:\n",
    "    dtypes[table_id] = {\n",
    "        **dtypes_orig[table_id],\n",
    "        **dtypes_new[table_id]\n",
    "    }\n",
    "\n",
    "dtypes = {\n",
    "    **dtypes['train'],\n",
    "    **dtypes_content_tags\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "id": "QhPzQmvUjuQV"
   },
   "source": [
    "### BigQuery Table Schemas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "WdW0igS4juQW"
   },
   "outputs": [],
   "source": [
    "# <hide-input>\n",
    "type_map = {\n",
    "    'int64': 'INTEGER',\n",
    "    'int32': 'INTEGER',\n",
    "    'int16': 'INTEGER',\n",
    "    'int8': 'INTEGER',\n",
    "    'uint8': 'INTEGER',\n",
    "    'uint16': 'INTEGER',\n",
    "    'str': 'STRING',\n",
    "    'bool': 'BOOL',\n",
    "    'float32': 'FLOAT'\n",
    "}\n",
    "\n",
    "schemas_orig = {table: [SchemaField(f, type_map[t]) for f, t in\n",
    "                   fields.items()] for table, fields in dtypes_orig.items()}\n",
    "\n",
    "schemas = {}\n",
    "for table_id, fields in dtypes_new.items():\n",
    "    new_fields = [SchemaField(f, type_map[t]) for\n",
    "                  f, t in fields.items()]\n",
    "    schemas[table_id] = schemas_orig[table_id] + new_fields"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "id": "GOpOrtr9juQY"
   },
   "source": [
    "### Load Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "XXwb4YkWjuQe"
   },
   "outputs": [],
   "source": [
    "# <hide-input>\n",
    "if False:\n",
    "    for table_id in dtypes_orig:\n",
    "        bqh.del_table(table_id)\n",
    "        lj = bqh.load_csv_uri(table_id, schemas_orig).result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "CHkK8JhQjuQg"
   },
   "outputs": [],
   "source": [
    "# <hide-input><hide-output>\n",
    "df_jobs = bqh.get_df_jobs()\n",
    "df_jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "rdk5EpmsQ7Pl"
   },
   "outputs": [],
   "source": [
    "if False:\n",
    "    job = bq_client.get_job(df_jobs.iloc[1].job_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "5cGVHAtMjuQi"
   },
   "outputs": [],
   "source": [
    "# <hide-input>\n",
    "df_table_list = bqh.get_df_table_list()\n",
    "df_table_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "id": "sgTZkanIjuQk"
   },
   "source": [
    "### Update Table Schemas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "_t7T4NftjuQk"
   },
   "outputs": [],
   "source": [
    "# <hide-input>\n",
    "if False:\n",
    "    for table_id, schema in schemas.items():\n",
    "        table = bqh.get_table(table_id)\n",
    "        table.schema = schema\n",
    "        table = bq_client.update_table(table, ['schema'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ouoo1oQCjuQm"
   },
   "source": [
    "## Engineer Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CU8m9kx9juQn"
   },
   "source": [
    "A good workflow here is:\n",
    "* Create a sample of the train table.\n",
    "* Use the BigQuery query editor user interface to get the SQL for a new feature worked out as a selection from the `train_sample` table. The user interface there has tab completion, syntax checking and displays results, which makes creating and debugging queries a snap.\n",
    "    * [BigQuery Console](https://console.cloud.google.com/bigquery?project=riiid-caleb) (Update project query string for your project.)\n",
    "    * [BigQuery Query syntax in Standard SQL](https://cloud.google.com/bigquery/docs/reference/standard-sql/query-syntax) is your friend.\n",
    "* Optional: create a local dataframe, using the export functions below, to confirm that it is working the right way.\n",
    "* Add a column to the appropriate table by adding a value to `dtypes_new`\n",
    "* Update the schema for the table in BigQuery by running the Update Table Schemas cell above\n",
    "* Recreate the `train_sample` table by running the cell below.\n",
    "* Use the BigQuery query editor user interface add the logic to update the new column.\n",
    "* Optional: create a local dataframe, using the export functions below, to confirm that the update is working the right way.\n",
    "* Copy the SQL to a new method in the `Queries` class above\n",
    "* Add the query to the appropriate `run_transformations` function above\n",
    "* Run transformations on `train_sample` table\n",
    "* Inspect `train_sample` table in BigQuery to confirm everything is working correctly\n",
    "* Optional: load load local dataframe using `get_df_query` function for further inspection\n",
    "* Run transformations on `train` table\n",
    "* Inspect `train` table in BigQuery to confirm everything is working correctly\n",
    "* Optional: load local dataframe using `get_df_query` function for further inspection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CfKY_WsvjuQp"
   },
   "source": [
    "### Perform Transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZOnBP6sfjuQp"
   },
   "source": [
    "#### Train Table\n",
    "* Update task_container_id to increase monotonically with timestamp\n",
    "    * There were some `task_conatiner_id`s that were out of order with respect to timestamp. They needed to be be ordered correctly so that cumulative and rolling sums partitioned by `task_container_id` would be include only interactions with earlier `timestamps`. Even though all interactions with the same `task_container_id` have the same `timestamp`, partioning by `timestamp` is much slower (because the range of values is so much wider?).\n",
    "* Calc answered_incorrectly\n",
    "    * `answered_correctly` for lectures was recorded as -1 and needed to be set to 0 to calculate cumulative and rolling sums correctly including lectures. As a consequence, `answered_incorrectly` could be calculated as the inverse of `answered_correctly`.\n",
    "* Calc cumsum for `answered_correctly` and `answered_incorrectly` by `user_id` and by `user_id` and `content_id` and rolling avg for `prior_question_elapsed_time` by user \n",
    "    * This is done so that the totals are as of the preceding `task_container_id`\n",
    "* Calculate rolling sum for `answered_correctly` and `answered_incorrectly` by `user_id`\n",
    "    * Includes the 10 rows preceding the current `task_container_id`\n",
    "    * I couldn't figure out how to get this done with the standard window functionality since I wanted a set number of rows preceding the current task container (as opposed to just the current row), so it joins on `user_id` with a `task_container_id` less than the current one, which takes a while to complete.\n",
    "* Calculate answered correctly percentages for `answered_correctly_cumsum`, `answered_correctly_rollsum` and `answered_correctly_content_id_cumsum_pct`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p56xi-SBW8-N",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if False:\n",
    "    q = Q.create_train_sample()\n",
    "    qj = bqh.run_query(*q, wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r2LZdBETjuQs"
   },
   "outputs": [],
   "source": [
    "# <hide-input>\n",
    "def run_train_transforms(table_id=None):\n",
    "    # Run serially to avoid update conflicts\n",
    "\n",
    "    train_queries = [\n",
    "#         Q.update_task_container_id(table_id=table_id,\n",
    "#                                    column_id='task_container_id'),\n",
    "        \n",
    "#         Q.update_task_container_id(table_id=table_id,\n",
    "#                                    column_id='task_container_id_q',\n",
    "#                                    excl_lectures=True),\n",
    "\n",
    "        Q.update_ql_id(table_id=table_id),\n",
    "        \n",
    "        Q.update_pqet_current(),\n",
    "        \n",
    "        Q.update_ts_delta(),\n",
    "                                   \n",
    "        Q.update_missing_values(table_id=table_id,\n",
    "                                column_id='prior_question_had_explanation',\n",
    "                                value='false'),\n",
    "        \n",
    "        Q.update_missing_values(table_id=table_id,\n",
    "                                column_id='prior_question_elapsed_time',\n",
    "                                value='0'),\n",
    "        \n",
    "        Q.update_answered_correctly(table_id=table_id),\n",
    "        \n",
    "        Q.update_train_window_containers(table_id=table_id),\n",
    "        \n",
    "        Q.create_tag_response(),\n",
    "        \n",
    "        Q.update_train_window_containers_tags(table_id=table_id),\n",
    "        \n",
    "        Q.update_train_window_containers_session(table_id=table_id,\n",
    "                                                 session_hours=18),\n",
    "        \n",
    "        Q.update_train_window_containers_upto(table_id=table_id),\n",
    "        \n",
    "    ]\n",
    "\n",
    "    train_queries_not_run = [\n",
    "        Q.update_train_window_rows(table_id=table_id, window=10)\n",
    "    ]\n",
    "\n",
    "    _ = [bqh.run_query(*q, wait=True) for q in train_queries]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2JEU6G0hMLkP"
   },
   "source": [
    "#### Questions Table\n",
    "* Calculate percent answered correctly for `question_id`, `part` and `tag__0`\n",
    "* Add question columns\n",
    "    * Adding question part and the first associated tag. (There wasn't any official information regarding the order of the tags as recorded for each question, but they did not appear to be sorted so it seems possible the order in which they are recorded is significant.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jG596OjGtm5Y"
   },
   "outputs": [],
   "source": [
    "# <hide-input>\n",
    "def run_questions_transforms_before():\n",
    "    \"\"\"These have to be run BEFORE the transforms are run on the full\n",
    "    train table.\n",
    "    \"\"\"\n",
    "\n",
    "    questions_queries = [Q.update_missing_values('questions', 'tags', '\"188\"'),\n",
    "                         Q.create_table_content_tags()]\n",
    "\n",
    "    _ = [bqh.run_query(*q, wait=True).result() for q in questions_queries]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8Ay24JcgjuQv"
   },
   "outputs": [],
   "source": [
    "# <hide-input>\n",
    "def run_questions_transforms_after():\n",
    "    \"\"\"These have to be run AFTER the transforms are run on the full\n",
    "    train table.\n",
    "    \"\"\"\n",
    "    questions_queries = []\n",
    "    for column_id in ['question_id', 'part']:\n",
    "        questions_queries.append(Q.update_content_tags_correct_pct(column_id))\n",
    "        questions_queries.append(Q.update_content_tags_pqet_avg(column_id))\n",
    "    \n",
    "    questions_queries.append(Q.update_content_tags_tags())\n",
    "    questions_queries.append(Q.update_content_tags_part_tags())\n",
    "        \n",
    "    _ = [bqh.run_query(*q, wait=True).result() for q in questions_queries]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-P_wUT-90IHi"
   },
   "source": [
    "### Run Transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tuh_k3VfjuQx",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# <hide-input>\n",
    "if True:\n",
    "#     run_questions_transforms_before()\n",
    "    run_train_transforms('train')\n",
    "    run_questions_transforms_after()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Ewma Stats "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas = [5,10,20,50]\n",
    "n_ewma_weights = 100\n",
    "\n",
    "ewma_update_specs = [\n",
    "    {\n",
    "        'prefix': 'ts_delta_ewma',\n",
    "        'column_calc': 'MAX(ts_delta)',\n",
    "        'offset': 0,\n",
    "        'alphas': alphas,\n",
    "        'n_weights': n_ewma_weights        \n",
    "    },\n",
    "    {\n",
    "        'prefix': 'ac_pct_ewma',\n",
    "        'column_calc': 'CAST(SAFE_DIVIDE(SUM(answered_correctly) * 100, COUNT(answered_correctly)) AS INT64)',\n",
    "        'offset': -1,\n",
    "        'alphas': alphas,\n",
    "        'n_weights': n_ewma_weights        \n",
    "    },\n",
    "    {\n",
    "        'prefix': 'pqet_ewma',\n",
    "        'column_calc': 'MAX(pqet_current)',\n",
    "        'offset': -1,\n",
    "        'alphas': alphas,\n",
    "        'n_weights': n_ewma_weights\n",
    "    },\n",
    "    {\n",
    "        'prefix': 'l_cnt_ewma',\n",
    "        'column_calc': 'SUM(content_type_id * 10000)',\n",
    "        'offset': 0,\n",
    "        'alphas': alphas,\n",
    "        'n_weights': n_ewma_weights,\n",
    "        'tid': 'task_container_id'\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    qj = bqh.run_query(*Q.update_ewma_stats(**ewma_update_specs[0], add_columns=False), wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if False:\n",
    "    for spec in ewma_update_specs[:-1]:\n",
    "        qj = bqh.run_query(*Q.update_ewma_stats(**spec, add_columns=False), wait=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dtypes here means they won't be added to schemas\n",
    "ewma_stats_types = {\n",
    "    'ts_delta_ewma': 'int64',\n",
    "    'pqet_ewma': 'int32',\n",
    "    'ac_pct_ewma': 'int8',\n",
    "    'l_cnt_ewma': 'int16'\n",
    "}\n",
    "\n",
    "dtypes_ewma_stats = {}\n",
    "for c, t in ewma_stats_types.items():\n",
    "    for a in alphas:\n",
    "        dtypes_ewma_stats[f'{c}_{a:02d}'] = t\n",
    "dtypes = {**dtypes, **dtypes_ewma_stats}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.5\n",
    "avg = 0\n",
    "for e in df_temp.current_val[0:3]:\n",
    "    avg = e * alpha + avg * (1-alpha)\n",
    "avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_temp.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Old Roll Stats "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "win_lens = [1, 3, 5, 10, 20, 50, 100, 200]\n",
    "\n",
    "calc_list_q_0 = [\n",
    "    'IF(COUNT(row_id) OVER({win_label}) < {win_len}, -1, IFNULL(CAST(AVG(ts_delta) OVER({win_label}) AS INT64), -1)) ts_delta_rollavg_{win_len:03d}',\n",
    "    'IF(COUNT(row_id) OVER({win_label}) < {win_len}, -1, IFNULL(CAST(AVG(prior_question_elapsed_time) OVER({win_label}) AS INT64), -1)) pqet_rollavg_{win_len:03d}'\n",
    "]\n",
    "\n",
    "calc_list_q_1 = [\n",
    "    'IF(COUNT(row_id) OVER({win_label}) < {win_len}, -1, IFNULL(CAST(AVG(SAFE_DIVIDE(answered_correctly * 100, CAST(content_type_id = 0 AS INT64))) OVER ({win_label}) AS INT64), -1)) ac_rollsum_pct_{win_len:03d}',\n",
    "    'IF(COUNT(row_id) OVER({win_label}) < {win_len}, -1, IFNULL(answered_correctly, -1)) ac_rollsum_{win_len:03d}',\n",
    "    'IFNULL(COUNT(row_id) OVER({win_label}), 0) r_rollcnt_{win_len:03d}'\n",
    "]\n",
    "\n",
    "calc_list_l = [\n",
    "    'IF(COUNT(row_id) OVER({win_label}) < {win_len}, -1, IFNULL(SUM(content_type_id) OVER ({win_label}), -1)) l_rollcnt_{win_len:03d}',\n",
    "]\n",
    "\n",
    "# dtypes here means they won't be added to schemas\n",
    "roll_stat_types = {\n",
    "    'ts_delta_rollavg': 'int64',\n",
    "    'pqet_rollavg': 'int16',\n",
    "    'l_rollcnt': 'int16',\n",
    "    'ac_rollsum_pct': 'int8',\n",
    "    'ac_rollsum': 'int16',\n",
    "    'r_rollcnt': 'int16'\n",
    "}\n",
    "\n",
    "dtypes_roll_stats = {}\n",
    "for c, t in roll_stat_types.items():\n",
    "    for w in win_lens:\n",
    "        dtypes_roll_stats[f'{c}_{w:03d}'] = t\n",
    "dtypes = {**dtypes, **dtypes_roll_stats}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print(*Q.update_roll_stats_lectures(win_lens, calc_list_l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "if False:\n",
    "    qj = bqh.run_query(*Q.create_roll_stats(win_lens, calc_list_q_0, n_prec=0), wait=True)\n",
    "    qj = bqh.run_query(*Q.update_roll_stats(win_lens, calc_list_q_1, n_prec=1), wait=True)\n",
    "    qj = bqh.run_query(*Q.update_roll_stats(win_lens, calc_list_l, n_prec=1), wait=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Top Content Ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_content_ids = [7218, 2593, 7217, 1278, 2065]\n",
    "\n",
    "top_cid_types = {\n",
    "    'ac_cumsum_top_cid': 'int16',\n",
    "    'r_cumcnt_top_cid': 'int16',\n",
    "    'ac_cumsum_pct_top_cid': 'int16',\n",
    "}\n",
    "\n",
    "dtypes_top_cids = {}\n",
    "for c, t in top_cid_types.items():\n",
    "    for cid in top_content_ids:\n",
    "        dtypes_top_cids[f'{c}_{cid}'] = t\n",
    "dtypes = {**dtypes, **dtypes_top_cids}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(*Q.create_top_content_ids(top_content_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    qj = bqh.run_query(*Q.create_top_content_ids(top_content_ids), wait=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Analyze Tags and Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "q = f\"\"\"\n",
    "    SELECT content_id, SUM(r_cumcnt) r_cumcnt, SUM(ac_cumsum) / SUM(r_cumcnt) ac_cumsum_pct,\n",
    "    CORR(ac_cumsum_pct_uc, ac_cumsum_pct_u) corr\n",
    "    FROM (\n",
    "        SELECT uc.user_id, uc.content_id, uc.r_cumcnt, uc.ac_cumsum,\n",
    "          SAFE_DIVIDE(uc.ac_cumsum, uc.r_cumcnt) ac_cumsum_pct_uc,\n",
    "          SAFE_DIVIDE(c.ac_cumsum, c.r_cumcnt) ac_cumsum_pct_u,\n",
    "        FROM (\n",
    "          SELECT user_id, content_id, SUM(answered_correctly) ac_cumsum, COUNT(answered_correctly) r_cumcnt\n",
    "          FROM data.train\n",
    "          WHERE content_type_id = 0\n",
    "          GROUP BY user_id, content_id\n",
    "        ) uc\n",
    "         JOIN (\n",
    "          SELECT user_id, SUM(answered_correctly) ac_cumsum, COUNT(answered_correctly) r_cumcnt\n",
    "          FROM data.train\n",
    "          WHERE content_type_id = 0\n",
    "          GROUP BY user_id\n",
    "        ) c ON uc.user_id = c.user_id\n",
    "    )\n",
    "    GROUP BY content_id\n",
    "    ORDER BY corr DESC\n",
    "\"\"\"\n",
    "\n",
    "qj = bq_client.query(q)\n",
    "\n",
    "df_q = qj.to_dataframe(create_bqstorage_client=True, progress_bar_type='tqdm_notebook')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df_q[df_q.r_cumcnt > 150e3].sort_values('corr', ascending=False).iloc[:20].content_id.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "fig = df_q.sort_values('r_cumcnt', ascending=False).iloc[:100].plot(kind='bar', x='content_id', y='corr')\n",
    "fig.update_xaxes(type='category')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df_q.plot(kind='scatter', x='r_cumcnt', y='corr')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "id": "SiG8eLze0kEM"
   },
   "source": [
    "### Create One-Hots Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "xgfCVpAB0qSt"
   },
   "outputs": [],
   "source": [
    "if False:\n",
    "    bqh.del_table('one_hots')\n",
    "    df_ct = bqh.get_df_query_gcs(('select * from data.content_tags', '_q_'), dtypes=dtypes, file_format='json')\n",
    "\n",
    "    mlb = MultiLabelBinarizer()\n",
    "    one_hots = (mlb.fit_transform(df_ct.tags_array\n",
    "        .apply(lambda l: [f'tag_{int(t):03d}' for t in eval(l)])))\n",
    "    df_one_hots = pd.DataFrame(one_hots, columns=mlb.classes_)\n",
    "    df_one_hots['ql_id'] = df_ct.ql_id\n",
    "    df_one_hots['part'] = df_ct.part\n",
    "    df_one_hots.ql_id = df_one_hots.ql_id.astype('int16')\n",
    "    df_one_hots = pd.get_dummies(df_one_hots, columns=['part'])\n",
    "\n",
    "    for c in [c for c in df_one_hots.columns if c != 'ql_id']:\n",
    "        dtypes[c] = 'int8'\n",
    "    \n",
    "    schemas['one_hots'] = [SchemaField(c, 'INTEGER', 'NULLABLE', None, ())\n",
    "                            for c in df_one_hots.columns]\n",
    "\n",
    "    df_one_hots.to_json('one_hots.json', orient=\"records\", lines=True)\n",
    "    lj = bqh.load_json_file('one_hots', schemas).result()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "id": "YTG_h2a2juQy"
   },
   "source": [
    "### Check Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "12-1u5TdjuQ1"
   },
   "outputs": [],
   "source": [
    "query = Q.select_train(table_id='train', excl_lectures=True)\n",
    "df_query = bqh.get_df_query(query, dtypes=dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "ub-U0nPgjuQ3"
   },
   "outputs": [],
   "source": [
    "# <hide-input>\n",
    "cols = [\n",
    "        'row_id',\n",
    "        'task_container_id_orig',\n",
    "        'timestamp',\n",
    "        'content_type_id',\n",
    "        'user_id',\n",
    "        'task_container_id',\n",
    "        'part',\n",
    "        'tag__0',\n",
    "        'answered_correctly',\n",
    "        'answered_incorrectly',\n",
    "        'answered_correctly_cumsum',\n",
    "        'answered_incorrectly_cumsum',\n",
    "        'answered_correctly_content_id_cumsum',\n",
    "        'answered_correctly_rollsum',\n",
    "        'answered_incorrectly_rollsum',\n",
    "        'answered_incorrectly_content_id_cumsum',\n",
    "        'part_correct_pct',\n",
    "        'tag__0_correct_pct',\n",
    "        'question_id_correct_pct',\n",
    "        'prior_question_elapsed_time',\n",
    "        'prior_question_elapsed_time_rollavg',\n",
    "        'prior_question_had_explanation',\n",
    "        'lectures_cumcount',\n",
    "        'answered_correctly_cumsum_upto'\n",
    "]\n",
    "\n",
    "df_user = df_query[cols].copy()\n",
    "df_user.timestamp = df_user.timestamp / (1000*60*60)\n",
    "\n",
    "df_user.loc[df_user.user_id == 44331].head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "id": "zO6GM_CLjuQ5"
   },
   "source": [
    "### Visually Inspect Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "jpokxssbjuQ6"
   },
   "source": [
    "The charts below can also be used to visually inspect whether the transformations have been performed correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "0c-xlk2TjuQ6"
   },
   "outputs": [],
   "source": [
    "# <hide-input>\n",
    "groups = {\n",
    "    'cum': {\n",
    "        'columns': {\n",
    "            'task_container_id': 0,\n",
    "            'answered_correctly_cumsum': 2,\n",
    "            'answered_incorrectly_cumsum': 1\n",
    "        },\n",
    "        'xaxis': 'elapsed_hours'\n",
    "    },\n",
    "    'roll': {\n",
    "        'columns': {\n",
    "            'answered_correctly_rollsum': 2,\n",
    "            'answered_correctly': 7,\n",
    "            'answered_incorrectly_rollsum': 1,\n",
    "            'answered_incorrectly': 8,\n",
    "            'part': 9\n",
    "        },\n",
    "        'xaxis': 'row_id'\n",
    "    },  \n",
    "    'correct_pct': {\n",
    "        'columns': {\n",
    "            'question_id_correct_pct': 0,\n",
    "            'part_correct_pct': 5,\n",
    "            'tag__0_correct_pct': 6\n",
    "        },\n",
    "        'xaxis': 'row_id'\n",
    "    },  \n",
    "    'prior_question_elapsed_time': {\n",
    "        'columns': {\n",
    "            'prior_question_elapsed_time': 0,\n",
    "        },\n",
    "        'xaxis': 'row_id'\n",
    "    },  \n",
    "    'prior_question_had_explanation': {\n",
    "        'columns': {\n",
    "            'prior_question_had_explanation': 0,\n",
    "        },\n",
    "        'xaxis': 'row_id'\n",
    "    }\n",
    "}\n",
    "\n",
    "def plot_user_learning(user_id=None, group=None, suffix=None):\n",
    "    theme = px.colors.qualitative.Plotly\n",
    "    columns = list(group['columns'].keys())\n",
    "    colors = [theme[c] for c in group['columns'].values()]\n",
    "\n",
    "    df_query['elapsed_hours'] = df_query.timestamp / (1000*60*60)\n",
    "\n",
    "    df = (df_query.loc[(df_user.user_id == user_id) &\n",
    "                       (df_user.content_type_id == 0)])\n",
    "\n",
    "    # labels = {'value': 'answer count'}\n",
    "\n",
    "    fig = df.plot(x=group['xaxis'], y=columns, color_discrete_sequence=colors,\n",
    "                  title=f'Learning Progress - user_id = {user_id} - {suffix}')\n",
    "    fig.data\n",
    "\n",
    "    return fig\n",
    "\n",
    "user_id_random = np.random.choice(df_query.user_id.unique(), (1,))[0]\n",
    "use_random = False\n",
    "user_id =  user_id_random if use_random else 5382\n",
    "\n",
    "for k, v in groups.items():\n",
    "    fig = plot_user_learning(user_id, v, k)\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "id": "Axb2oD77juRM"
   },
   "source": [
    "### Create Folds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "jNsk1_JsjuRN"
   },
   "source": [
    "The objectives for the validation split are as follows:\n",
    "* Include some users that don't exist in the training set\n",
    "* Include records for some users in the validation set with timestamps greater than all of those users' records in the training set\n",
    "\n",
    "This is achieved by first setting the percentage of users that will only occur in the training set and then setting the percentage of the remaining users that will have records in both.\n",
    "\n",
    "The split between the validation and training sets for the users with records in both is determined by randomly selecting a number between zero and the maximum task_container_id for each user and including in the validation set all records with a task_container_id greater than that for each user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "A_nmEi_1gMFf"
   },
   "outputs": [],
   "source": [
    "# <hide-input>\n",
    "# create validation split table\n",
    "# this needs to run after the task container Ids have been updated\n",
    "\n",
    "table_id_folds = 'folds'\n",
    "if False:\n",
    "#     qj = bqh.run_query(*Q.create_table_folds(table_id=table_id_folds), wait=True)\n",
    "    qj = bqh.run_query(*Q.update_folds(table_id_folds=table_id_folds), wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "PX-J3AaG5FxM"
   },
   "outputs": [],
   "source": [
    "# <hide-input>\n",
    "# TODO: bring in diagnostic charts from folding is fun\n",
    "if False:\n",
    "    df_folds = bqh.get_df_table('folds', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "7GBDKOZ2juRA"
   },
   "source": [
    "With feature engineering being performed in BigQuery, data has to be exported to train models locally. The [Python Client for Google BigQuery](https://googleapis.dev/python/bigquery/latest/index.html) [to_dataframe()](https://googleapis.dev/python/bigquery/latest/generated/google.cloud.bigquery.job.QueryJob.html?highlight=to_dataframe#google.cloud.bigquery.job.QueryJob.to_dataframe) makes it possible to create dataframes directly, but is prohibitively slow for large datasets. While it is not possible to export table directly to the local file system, it is possible to export to cloud storage and then download locally from there. This is reasonably efficient, taking a couple of minutes to run a query, export to cloud storage, download to the local file system and then read the files into a dataframe. The is another api, the [BigQuery Storage API](https://cloud.google.com/bigquery/docs/reference/storage), that a client can be created with that is really fast and works with the `to_dataframe` method, but unforunatley it isn't working with the current Kaggle kernel environment.\n",
    "\n",
    "The functions below take advantage of the fact BigQuery stores queries in temporary tables so that preveiously requested queries can be retrieved without having to run them again. Similarly, the functions below name the exported files with the reference to the BigQuery temporary table, so that if a function is run to create a dataframe from a query for which the files already exist in cloud storage or locally, they won't be exported or downloaded again. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F_B3C-9QjuQ9"
   },
   "source": [
    "### Create Sample of Train Table for R&D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "id": "tuNFCT1OjuQ9"
   },
   "outputs": [],
   "source": [
    "# <hide-input>\n",
    "if False:\n",
    "    ts_id = 'train_sample'\n",
    "    bqh.del_table(ts_id)\n",
    "    bqh.run_query(*Q.create_train_sample(ts_id), wait=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wbaxOZFVjuRA"
   },
   "source": [
    "## Create Local Dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xV-qHbTc2oly"
   },
   "source": [
    "### Select Features to Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "id": "rnDz1XqY5g-n",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# <hide-input><hide-output>\n",
    "if False:\n",
    "    feats = {d: [True, True] for d in dtypes}\n",
    "    for f in features:\n",
    "        feats[f] = features[f]\n",
    "    \n",
    "#     for f in exp_cols_new:\n",
    "#         feats[f] = [True, True]\n",
    "    \n",
    "    feats = sorted([f\"\"\"'{k+\"':\":<32} {v},\"\"\" for k,v in feats.items()])\n",
    "\n",
    "    for f in feats:\n",
    "        print(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "id": "QUkacRjajuRG"
   },
   "outputs": [],
   "source": [
    "# <hide-input>\n",
    "features = {\n",
    "'ac_cumsum':                      [True, False],\n",
    "'ac_cumsum_content_id':           [True, False],\n",
    "'ac_cumsum_part':                 [True, False],\n",
    "'ac_cumsum_part_tags':            [True, False],\n",
    "'ac_cumsum_pct':                  [True, True],\n",
    "'ac_cumsum_pct_content_id':       [True, True],\n",
    "'ac_cumsum_pct_part':             [True, True],\n",
    "'ac_cumsum_pct_part_tags':        [True, False],\n",
    "'ac_cumsum_pct_session':          [True, True],\n",
    "'ac_cumsum_pct_tags':             [True, True],\n",
    "'ac_cumsum_pct_top_cid_1278':     [True, True],\n",
    "'ac_cumsum_pct_top_cid_2065':     [True, True],\n",
    "'ac_cumsum_pct_top_cid_2593':     [True, True],\n",
    "'ac_cumsum_pct_top_cid_7217':     [True, True],\n",
    "'ac_cumsum_pct_top_cid_7218':     [True, True],\n",
    "'ac_cumsum_pct_upto':             [True, True],\n",
    "'ac_cumsum_session':              [True, False],\n",
    "'ac_cumsum_tags':                 [True, False],\n",
    "'ac_cumsum_top_cid_1278':         [True, False],\n",
    "'ac_cumsum_top_cid_2065':         [True, False],\n",
    "'ac_cumsum_top_cid_2593':         [True, False],\n",
    "'ac_cumsum_top_cid_7217':         [True, False],\n",
    "'ac_cumsum_top_cid_7218':         [True, False],\n",
    "'ac_cumsum_upto':                 [True, False],\n",
    "'ac_pct_ewma_05':                 [True, True],\n",
    "'ac_pct_ewma_10':                 [True, True],\n",
    "'ac_pct_ewma_20':                 [True, True],\n",
    "'ac_pct_ewma_50':                 [True, True],\n",
    "'answered_correctly':             [True, False],\n",
    "'bundle_id':                      [False, False],\n",
    "'content_id':                     [True, False],\n",
    "'content_type_id':                [True, False],\n",
    "'correct_answer':                 [False, False],\n",
    "'fold':                           [True, False],\n",
    "'l_cnt_ewma_05':                  [True, True],\n",
    "'l_cnt_ewma_10':                  [True, True],\n",
    "'l_cnt_ewma_20':                  [True, True],\n",
    "'l_cnt_ewma_50':                  [True, True],\n",
    "'l_cumcnt':                       [True, True],\n",
    "'l_cumcnt_part':                  [True, True],\n",
    "'l_cumcnt_part_tags':             [True, False],\n",
    "'l_cumcnt_session':               [True, True],\n",
    "'l_cumcnt_tags':                  [True, True],\n",
    "'lecture_id':                     [False, False],\n",
    "'part':                           [True, True],\n",
    "'part_correct_pct':               [True, True],\n",
    "'part_pqet_avg':                  [True, True],\n",
    "'part_tags_correct_pct':          [True, False],\n",
    "'part_tags_pqet_avg':             [True, False],\n",
    "'pqet_cumavg':                    [True, True],\n",
    "'pqet_cumavg_content_id':         [True, True],\n",
    "'pqet_cumavg_part':               [True, True],\n",
    "'pqet_cumavg_part_tags':          [True, False],\n",
    "'pqet_cumavg_session':            [True, True],\n",
    "'pqet_cumavg_tags':               [True, True],\n",
    "'pqet_cumavg_upto':               [True, True],\n",
    "'pqet_cumsum':                    [False, False],\n",
    "'pqet_cumsum_content_id':         [False, False],\n",
    "'pqet_cumsum_part':               [False, False],\n",
    "'pqet_cumsum_part_tags':          [False, False],\n",
    "'pqet_cumsum_session':            [False, False],\n",
    "'pqet_cumsum_tags':               [False, False],\n",
    "'pqet_cumsum_upto':               [False, False],\n",
    "'pqet_current':                   [False, False],\n",
    "'pqet_ewma_05':                   [True, True],\n",
    "'pqet_ewma_10':                   [True, True],\n",
    "'pqet_ewma_20':                   [True, True],\n",
    "'pqet_ewma_50':                   [True, True],\n",
    "'prior_question_elapsed_time':    [True, True],\n",
    "'prior_question_had_explanation': [True, True],\n",
    "'ql_id':                          [False, False],\n",
    "'question_id':                    [False, False],\n",
    "'question_id_correct_pct':        [True, True],\n",
    "'question_id_pqet_avg':           [True, True],\n",
    "'r_cumcnt':                       [True, True],\n",
    "'r_cumcnt_clip':                  [True, True],\n",
    "'r_cumcnt_content_id':            [True, True],\n",
    "'r_cumcnt_part':                  [True, True],\n",
    "'r_cumcnt_part_tags':             [True, False],\n",
    "'r_cumcnt_session':               [True, True],\n",
    "'r_cumcnt_tags':                  [True, True],\n",
    "'r_cumcnt_top_cid_1278':          [True, True],\n",
    "'r_cumcnt_top_cid_2065':          [True, True],\n",
    "'r_cumcnt_top_cid_2593':          [True, True],\n",
    "'r_cumcnt_top_cid_7217':          [True, True],\n",
    "'r_cumcnt_top_cid_7218':          [True, True],\n",
    "'r_cumcnt_upto':                  [True, True],\n",
    "'row_id':                         [True, False],\n",
    "'session':                        [True, True],\n",
    "'tags':                           [True, False],\n",
    "'tags_array':                     [False, False],\n",
    "'tags_code':                      [True, True],\n",
    "'tags_correct_pct':               [True, True],\n",
    "'tags_pqet_avg':                  [True, True],\n",
    "'task_container_id':              [True, True],\n",
    "'task_container_id_q':            [False, False],\n",
    "'timestamp':                      [True, True],\n",
    "'ts_delta':                       [True, True],\n",
    "'ts_delta_ewma_05':               [True, True],\n",
    "'ts_delta_ewma_10':               [True, True],\n",
    "'ts_delta_ewma_20':               [True, True],\n",
    "'ts_delta_ewma_50':               [True, True],\n",
    "'user_answer':                    [False, False],\n",
    "'user_id':                        [True, False],\n",
    "}\n",
    "\n",
    "columns_export = [f for f, v in features.items() if v[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "id": "LFvJDKLI9ttR"
   },
   "outputs": [],
   "source": [
    "if False:\n",
    "    df_one_hots = pd.read_gbq('select * from data.one_hots').set_index('ql_id')\n",
    "    for c in df_one_hots.columns:\n",
    "        features[c] = [True, True]\n",
    "        dtypes[c] = 'int8'\n",
    "    features['tag_0'] = [True, False]\n",
    "    features['part'] = [True, False]\n",
    "    features['tags_code'] = [True, False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     1
    ],
    "id": "32Wt1DvjpnJZ"
   },
   "outputs": [],
   "source": [
    "# <hide-input>\n",
    "def get_features_widget(features_dict, columns_list, idx):\n",
    "\n",
    "    names = []\n",
    "    widget_list = []\n",
    "    for key, v in features_dict.items():\n",
    "        widget_list.append(widgets.ToggleButton(value=v[idx],\n",
    "                                                description=key,\n",
    "                                                layout={'width': '290px'},\n",
    "                                                button_style='primary'))\n",
    "        names.append(key)\n",
    "\n",
    "    arg_dict = {names[i]: widget for i, widget in enumerate(widget_list)}\n",
    "\n",
    "    layout = widgets.Layout(grid_template_columns=\"repeat(3, 300px)\")\n",
    "    ui = widgets.GridBox(widget_list, layout=layout)\n",
    "\n",
    "    def select_data(**kwargs):\n",
    "        columns_list.clear()\n",
    "\n",
    "        for key in kwargs:\n",
    "            features_dict[key][idx] = False\n",
    "            if kwargs[key]:\n",
    "                columns_list.append(key)\n",
    "                features_dict[key][idx] = True\n",
    "\n",
    "        print(f'{len(columns_list)} columns selected')\n",
    "\n",
    "    output = widgets.interactive_output(select_data, arg_dict)\n",
    "    return ui, output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "id": "B6ddqQ4HBT3R"
   },
   "outputs": [],
   "source": [
    "# <hide-input>\n",
    "display(*get_features_widget(features, columns_export, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_export = columns_train + ['fold', 'answered_correctly']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EXRXQ86UjuRK",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# <hide-output>\n",
    "\n",
    "from_bq_trn = True\n",
    "filename_trn = 'df_train.pkl'\n",
    "\n",
    "folds = list(range(10))\n",
    "query_trn = Q.select_train(columns=columns_export, folds=folds,\n",
    "                           excl_lectures=True, limit=None, null_fold=False)[0]\n",
    "\n",
    "df_train = bqh.get_df_query_bqs(query_trn, filename_trn, from_bq_trn, dtypes, save=True, fillna=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Log Transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df_train['timestamp_log'] = np.log(df_train.timestamp.clip(1))\n",
    "df_train['r_cumcnt_log'] = np.log(df_train.r_cumcnt.clip(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df_train['ac_cumsum_log_odds'] = np.log(df_train.ac_cumsum_pct.clip(1, 99) / 100) - np.log(1 - df_train.ac_cumsum_pct.clip(1, 99) / 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "cuts = pd.qcut(df_train.l_cnt_ewma_20, 10, duplicates='drop').value_counts().sort_index()\n",
    "px.bar(x=cuts.index.astype(str), y=cuts.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "id": "Nnwa3B8jiKV0"
   },
   "source": [
    "### Collaborative Filtering Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "cat_cols = ['user_id', 'content_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "T-_ypfopXB96"
   },
   "outputs": [],
   "source": [
    "def get_collab_weights(file_name):\n",
    "\n",
    "    if not Path(file_name).exists():\n",
    "        bucket.blob(file_name).download_to_filename(file_name)\n",
    "    weights = np.load(file_name, allow_pickle=True).item()\n",
    "\n",
    "#     for c in [('user_id_embedding', 'user_embedding'),\n",
    "#     ('user_id_bias', 'user_bias'),\n",
    "#     ('content_id_embedding', 'question_embedding'),\n",
    "#     ('content_id_bias', 'question_bias')]:\n",
    "#         weights[c[0]] = weights.pop(c[1])\n",
    "\n",
    "    for c in cat_cols:\n",
    "        embed_key = f'{c}_embedding'\n",
    "        weights[embed_key] = np.reshape(weights[embed_key], (len(weights[c]) + 1, -1))\n",
    "        \n",
    "    return weights\n",
    "\n",
    "weights = get_collab_weights('weights_pre.npy')\n",
    "weights.keys()\n",
    "# np.save('weights_pre.npy', weights, allow_pickle=True)\n",
    "# bucket.blob('weights_pre.npy').upload_from_filename('weights_pre.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "Kt7rKoBnnEXO"
   },
   "outputs": [],
   "source": [
    "def get_preds(df, weights, logits=True):\n",
    "    user_codes, content_codes = [pd.Categorical(df[c], categories=weights[c]).codes  for c in cat_cols]\n",
    "    user_col, content_col = cat_cols\n",
    "    logit = np.sum(weights[f'{user_col}_embedding'][user_codes] * weights[f'{content_col}_embedding'][content_codes], axis=1)\n",
    "    logit += weights[f'{user_col}_bias'][user_codes] + weights[f'{content_col}_bias'][content_codes]\n",
    "    \n",
    "    if logits:\n",
    "        return logit\n",
    "    else:\n",
    "        return 1 / (1 + np.exp(-logit))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "M96Uut_hsQHV"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "preds = get_preds(df_train, weights, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "kkMduw8hmno4"
   },
   "outputs": [],
   "source": [
    "roc_auc_score(df_train.answered_correctly, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "SvkPeuVrsnHL"
   },
   "outputs": [],
   "source": [
    "df_train['pred_collab_logit'] = preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "id": "kN26v5z2t_hG"
   },
   "source": [
    "### Add Max Task Container Id from Folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "0Z9i0km5JhKj"
   },
   "outputs": [],
   "source": [
    "# add on max task_container_id to filter out smaller number of containers\n",
    "if True:\n",
    "    df_folds = pd.read_gbq('select * from data.folds', use_bqstorage_api=True, progress_bar_type='tqdm_notebook')\n",
    "    df_train = df_train.merge(df_folds[['user_id_s', 'task_container_id_max']], how='left', left_on='user_id', right_on='user_id_s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SI-MVAI4uJ8w"
   },
   "source": [
    "### Inspect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "id": "TAIxa0UkIZCF",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if True:\n",
    "    \n",
    "    base_cols = [\n",
    "            'user_id',\n",
    "            'task_container_id',\n",
    "            'row_id',\n",
    "            'content_id',\n",
    "            'content_type_id',\n",
    "            'prior_question_had_explanation',\n",
    "            'answered_correctly',\n",
    "            'timestamp',\n",
    "            'ts_delta',\n",
    "            'prior_question_elapsed_time',\n",
    "            'pqet_current',\n",
    "            'part'\n",
    "    ]\n",
    "    \n",
    "    col = 'content_id'\n",
    "    \n",
    "    cum_cols = [\n",
    "            'ac_cumsum',\n",
    "            'r_cumcnt',\n",
    "            'r_cumcnt_clip',\n",
    "            'ac_cumsum_pct',\n",
    "            'l_cumcnt',\n",
    "            f'ac_cumsum_{col}',\n",
    "            f'r_cumcnt_{col}',\n",
    "            f'ac_cumsum_pct_{col}',\n",
    "#             f'l_cumcnt_{col}'\n",
    "    ]\n",
    "\n",
    "    rollsum_session_cols = [\n",
    "            'session',\n",
    "            f'ac_rollsum_{col}',\n",
    "            f'aic_rollsum_{col}',\n",
    "            f'r_rollcnt_{col}',\n",
    "            f'ac_rollsum_pct_{col}',\n",
    "            f'lectures_rollcnt_{col}'\n",
    "    ]\n",
    "\n",
    "    q_cols = [\n",
    "              'tags',\n",
    "              'tag_0',\n",
    "              'question_id_correct_pct',\n",
    "              'part_correct_pct',\n",
    "              'tag_0_correct_pct',\n",
    "              'tags_correct_pct',\n",
    "              'tag_0_part_correct_pct',\n",
    "              'question_id_pqet_avg',\n",
    "              'part_pqet_avg',\n",
    "              'tag_0_pqet_avg',\n",
    "              'tags_pqet_avg'\n",
    "    ]\n",
    "\n",
    "    time_cols = [\n",
    "            'prior_question_elapsed_time',\n",
    "            'ts_minute',\n",
    "            'session_minute_max',\n",
    "            'pqet_sec',\n",
    "            'pqet_sec_rollavg',\n",
    "            'session'\n",
    "    ]\n",
    "    \n",
    "#     roll_col = 'ac_rollsum_pct'\n",
    "#     roll_cols = [f'{roll_col}_{w:03d}' for w in win_lens]\n",
    "    \n",
    "    ewma_cols = [c for c in dtypes if 'ewma' in c]\n",
    "    \n",
    "    top_cid_cols = [c for c in dtypes if 'top_cid' in c]\n",
    "    \n",
    "    pqet_cols = [c for c in dtypes if 'pqet' in c]\n",
    "\n",
    "#     df_test = bqh.get_df_query_gcs((\"\"\"\n",
    "#         SELECT *\n",
    "#         FROM data.train t\n",
    "#         JOIN data.content_tags c\n",
    "#         ON t.ql_id = c.ql_id AND t.user_id IN (115, 124, 1827855198, 1066383521)\n",
    "#         LEFT JOIN data.ewma_stats e\n",
    "#         ON t.row_id = e.row_id_e\n",
    "#         ORDER BY user_id, task_container_id, row_id\n",
    "#         \"\"\", '_test_'),dtypes=None, file_format='json')\n",
    "\n",
    "df_test[base_cols + cum_cols][df_test.user_id == 1827855198].tail(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert df_test.content_type_id.sum() > 0, \\\n",
    "        'there should be some lectures in the test set'\n",
    "\n",
    "assert ~(df_test.content_type_id * df_test.answered_correctly).all(), \\\n",
    "        'all of the lecture records should have answered correctly equal to 0'\n",
    "\n",
    "assert (~df_test[df_test.content_type_id ==0][[c for c in df_test.columns if c != 'lecture_id']]\n",
    "        .isna().any().all()), \\\n",
    "        'there should not be any nas'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8VM6n9FKyczN"
   },
   "source": [
    "### Select Columns for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MUSDoZoLEKP_"
   },
   "outputs": [],
   "source": [
    "# <hide-input>\n",
    "columns_train = []\n",
    "display(*get_features_widget(features, columns_train, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['ac_cumsum_pct_content_id_idx'] = (df_train.ac_cumsum_pct_content_id / df_train.question_id_correct_pct).apply(lambda x: x if x > 0 else -1)\n",
    "df_train['ac_cumsum_pct_part_idx'] = (df_train.ac_cumsum_pct_part / df_train.part_correct_pct).apply(lambda x: x if x > 0 else -1)\n",
    "df_train['ac_cumsum_pct_part_tags_idx'] = (df_train.ac_cumsum_pct_part_tags / df_train.part_tags_correct_pct).apply(lambda x: x if x > 0 else -1)\n",
    "df_train['pqet_cumavg_content_id_idx'] = (df_train.pqet_cumavg_content_id / df_train.question_id_pqet_avg).apply(lambda x: x if x > 0 else -1)\n",
    "df_train['pqet_cumavg_part_idx'] = (df_train.pqet_cumavg_part / df_train.part_pqet_avg).apply(lambda x: x if x > 0 else -1)\n",
    "df_train['pqet_cumavg_part_tags_idx'] = (df_train.pqet_cumavg_part_tags / df_train.part_tags_pqet_avg).apply(lambda x: x if x > 0 else -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_train.extend(['ac_cumsum_pct_content_id_idx',\n",
    "                      'ac_cumsum_pct_part_idx', \n",
    "                      'ac_cumsum_pct_part_tags_idx', \n",
    "                      'pqet_cumavg_content_id_idx', \n",
    "                      'pqet_cumavg_part_idx', \n",
    "                      'pqet_cumavg_part_tags_idx'])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zfBtnf3TsyS_"
   },
   "outputs": [],
   "source": [
    "columns_train.append('pred_collab_logit')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q-zrGOfDwjw9"
   },
   "outputs": [],
   "source": [
    "# <hide-input>\n",
    "def show_features():\n",
    "    df_features = pd.DataFrame(features).T.reset_index()\n",
    "    df_features.columns = ['feature', 'export', 'train']\n",
    "    df_features\n",
    "\n",
    "    def highlight_true(s):\n",
    "        return ['background-color: lightskyblue' if v else '' for v in s]\n",
    "    return df_features.style.apply(highlight_true, subset=['export', 'train'])\n",
    "show_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "reut9PmcjuRV"
   },
   "outputs": [],
   "source": [
    "# <hide-input>\n",
    "def get_dmatrices(folds_train=None, folds_val=None, x_train_cols=None, matrix=xgb.DMatrix, null_fold=False):\n",
    "    \n",
    "    y_train_col = ['answered_correctly']\n",
    "\n",
    "    min_tcid_max = 0\n",
    "        \n",
    "    mask_train = (df_train.fold.isin(folds_train) | (df_train.fold.isna() & null_fold)) # & (df_train.task_container_id_max > min_tcid_max)\n",
    "    mask_valid = df_train.fold.isin(folds_val) # & (df_train.task_container_id_max > min_tcid_max)\n",
    "\n",
    "    train_matrix = matrix(data=df_train.loc[mask_train][x_train_cols],\n",
    "                            label=df_train.loc[mask_train][y_train_col])\n",
    "\n",
    "    valid_matrix = matrix(data=df_train.loc[mask_valid][x_train_cols],\n",
    "                            label=df_train.loc[mask_valid][y_train_col])\n",
    "    \n",
    "    return {'train_matrix': train_matrix,\n",
    "            'valid_matrix': valid_matrix,\n",
    "            'folds_train': folds_train,\n",
    "            'folds_val': folds_val\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a05rC3CdDYg0"
   },
   "outputs": [],
   "source": [
    "folds_dict = {}\n",
    "if False:\n",
    "    folds = list(range(30))\n",
    "    \n",
    "    for fold in folds:\n",
    "        query = Q.select_train(columns=['answered_correctly'] + columns_train, folds=[fold],\n",
    "                            excl_lectures=True, limit=None)\n",
    "        \n",
    "        prefix = bqh.export_query_gcs(query, header=False)\n",
    "        file_paths = bqh.get_table_gcs(prefix)\n",
    "        folds_dict[fold] = dict(prefix=prefix,file_paths=file_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Nluvlo-SGun9"
   },
   "outputs": [],
   "source": [
    "def get_dmatrices_csv(folds_train=None, folds_val=None):\n",
    "    \n",
    "    def combine_folds(folds, cache_fp=None):\n",
    "        if cache_fp is not None and Path(cache_fp).exists():\n",
    "            Path(cache_fp).unlink()\n",
    "            Path(f'{cache_fp}.row.page').unlink()\n",
    "            \n",
    "        fp_combined = Path('combined.csv')\n",
    "        if fp_combined.exists():\n",
    "            fp_combined.unlink()\n",
    "        \n",
    "        with open(fp_combined,\"wb\") as fout:\n",
    "            for fold in tqdm(folds):\n",
    "                for fp in folds_dict[fold]['file_paths']:\n",
    "                    with open(fp, \"rb\") as fin:\n",
    "                        fout.write(fin.read())\n",
    "\n",
    "        uri = f'{fp_combined.name}?format=csv&label_column=0'\n",
    "\n",
    "        if cache_fp is not None:\n",
    "            uri += f'#{cache_fp}'\n",
    "        \n",
    "        return xgb.DMatrix(uri, feature_names=columns_train)\n",
    "        \n",
    "    return {'train_matrix': combine_folds(folds_train, 'train.cache'),\n",
    "            'valid_matrix': combine_folds(folds_val, 'valid.cache'),\n",
    "            'folds_train': folds_train,\n",
    "            'folds_val': folds_val\n",
    "            }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oMqolxVmjuRL"
   },
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t6AeR8lBjuRX",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# <hide-output>\n",
    "params = {\n",
    "    'booster': 'gbtree',\n",
    "    # 'rate_drop': 0.1,\n",
    "    'eta': 0.1,\n",
    "    'max_depth': 10,\n",
    "#     'max_leaves': 256,\n",
    "    'max_bin': 512,\n",
    "    # 'min_child_weight': 100,\n",
    "    'tree_method': 'gpu_hist',\n",
    "    'grow_policy': 'lossguide',\n",
    "    'objective': 'binary:logistic',\n",
    "    'eval_metric': ['logloss', 'error', 'auc'],\n",
    "    # 'subsample': 0.5,\n",
    "    # 'sampling_method': 'gradient_based',\n",
    "    'lambda': 4,\n",
    "    'alpha': 100\n",
    "}\n",
    "\n",
    "log_experiment = True\n",
    "\n",
    "def train_model(params=params, train_matrix=None, valid_matrix=None,\n",
    "                folds_train=None, folds_val=None, verbose_eval=True, comet_summary=1):\n",
    "    \n",
    "    params['seed'] = random.randint(0, 100000)\n",
    "    \n",
    "    if NOT_KAGGLE and log_experiment:\n",
    "        experiment = Experiment(display_summary_level=comet_summary)\n",
    "        # experiment.set_name('with gap_minute_avg')\n",
    "        experiment.log_parameter('folds_train', folds_train)\n",
    "        experiment.log_parameter('folds_val', folds_val)\n",
    "        experiment.log_text(('\\n').join(columns_train))\n",
    "                \n",
    "        # experiment.log_parameter('df_train_len', len(df_train))\n",
    "\n",
    "    evals_result = {}\n",
    "    model = xgb.train(params=params, dtrain=train_matrix, num_boost_round=1000,\n",
    "                    evals=[(train_matrix, 'train'), (valid_matrix, 'valid')],\n",
    "                    evals_result=evals_result, early_stopping_rounds=10,\n",
    "                    verbose_eval=verbose_eval)\n",
    "\n",
    "    exp_key = 'locals_only'\n",
    "    if NOT_KAGGLE and log_experiment:\n",
    "        exp_key = experiment.get_key()\n",
    "        model.save_model(f'models/{exp_key}.xgb')\n",
    "        experiment.log_model(exp_key, f'models/{exp_key}.xgb')\n",
    "        df_imp = pd.DataFrame({'gain': model.get_score(importance_type='gain')})\n",
    "        experiment.log_table('df_importances.json',  df_imp)\n",
    "        experiment.end()\n",
    "        \n",
    "    return model, exp_key\n",
    "    \n",
    "exps = {}\n",
    "\n",
    "folds_list = [\n",
    "    {'folds_train': folds[1:], 'folds_val': folds[:1]}\n",
    "]\n",
    "\n",
    "# folds_list = [{'folds_train': [[f for f in folds if f != val_fold]], 'folds_val': [folds[val_fold]]} for val_fold in folds]\n",
    "\n",
    "for folds_run in folds_list:\n",
    "    model, exp_key = train_model(params, **get_dmatrices(**folds_run, x_train_cols=columns_train, null_fold=True),\n",
    "                                 verbose_eval=True, comet_summary=1)\n",
    "    exps[exp_key] = model.attributes()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fe7JKQf0juRZ"
   },
   "source": [
    "## Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qPTKUFJMjuRZ"
   },
   "outputs": [],
   "source": [
    "# <hide-input>\n",
    "def get_evals_df(evals_result):\n",
    "    evals_list = []\n",
    "    for k,v in evals_result.items():\n",
    "        for j,u in v.items():\n",
    "            evals_list.extend([{'epoch': i,\n",
    "                                'split': k,\n",
    "                                'metric': j,\n",
    "                                'result': r} for i,r in enumerate(u)])\n",
    "    \n",
    "    df_evals = (pd.DataFrame(evals_list).set_index(['split', 'metric', 'epoch'])\n",
    "                .unstack('metric'))\n",
    "    df_evals.columns = df_evals.columns.get_level_values(1)\n",
    "    df_evals.columns.name = None\n",
    "    \n",
    "    return df_evals.reset_index()\n",
    "\n",
    "df_evals = get_evals_df(evals_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oa7h74fSjuRb"
   },
   "outputs": [],
   "source": [
    "# <hide-input>\n",
    "df_evals.plot(x='epoch', y=['auc', 'logloss'],\n",
    "              facet_col='split', title='Learning Curves')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AxlglavIYpXn"
   },
   "outputs": [],
   "source": [
    "model = xgb.Booster(model_file='127dba4ca30042328c57ec2197e813f2.xgb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EMrYRgCF4V9i"
   },
   "outputs": [],
   "source": [
    "model.feature_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Jgt5w3zHkvh2"
   },
   "outputs": [],
   "source": [
    "matrix_dict = get_dmatrices([0], [1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Gxlm82xRlCPk"
   },
   "outputs": [],
   "source": [
    "model.eval(matrix_dict['valid_matrix'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kzWs_7Azk9H3"
   },
   "outputs": [],
   "source": [
    "print(matrix_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oWnRrYLNt6TA"
   },
   "outputs": [],
   "source": [
    "df_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_hq7-PWBGt1C"
   },
   "outputs": [],
   "source": [
    "df_train.pred_collab_logit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iIK9aXVpjuRd"
   },
   "outputs": [],
   "source": [
    "# <hide-input>\n",
    "imps = model.get_score(importance_type='gain').items()\n",
    "df_imp = pd.DataFrame(imps, columns=['feature', 'importance'])\n",
    "df_imp = df_imp.set_index('feature').sort_values('importance', ascending=False)\n",
    "df_imp.iloc[:25].plot(kind='bar', y='importance', title='Feature Importances - Gain')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_imp[df_imp.index.isin([c for c in columns_train if 'pqet_' in c])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(df_imp.importance / df_imp.importance.sum()).cumsum().plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = shap.TreeExplainer(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_values = explainer.shap_values(df_train[df_train.fold.isin(folds[:1])][model.feature_names])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.summary_plot(shap_values, df_train[df_train.fold.isin(folds[:1])][model.feature_names])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_train = list(pd.DataFrame({'feature': columns_train, 'shap': np.sum(np.abs(shap_values), axis=0)}).sort_values('shap', ascending=False).iloc[:50].feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_imp.iloc[:30].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Mf899tHmctmo"
   },
   "outputs": [],
   "source": [
    "if False:\n",
    "    tree = xgb.to_graphviz(model, rankdir='LR')\n",
    "    tree.save('tree')\n",
    "    tree.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xVXlSxE-t2Bt"
   },
   "outputs": [],
   "source": [
    "model_dump = model.get_dump(dump_format='json', with_stats=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AtIDcslpvY7f"
   },
   "outputs": [],
   "source": [
    "print(model_dump[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ouZNqNl9p5zI"
   },
   "outputs": [],
   "source": [
    "model.dump_model('model.json', with_stats=True, dump_format='json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Tensorflow Boosted Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "CATEGORICAL_COLUMNS = ['part', 'tag_0']\n",
    "NUMERIC_COLUMNS = ['ac_cumsum_pct', 'ac_cumsum_pct_part', 'timestamp']\n",
    "\n",
    "val_fold = 0\n",
    "dftrain = df_train[CATEGORICAL_COLUMNS + NUMERIC_COLUMNS + ['fold', 'answered_correctly']]\n",
    "dfeval = dftrain[dftrain.fold == 0]\n",
    "dftrain = dftrain[df_train.fold != val_fold]\n",
    "del dftrain['fold']\n",
    "y_train = dftrain.pop('answered_correctly')\n",
    "y_eval = dfeval.pop('answered_correctly')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def one_hot_cat_column(feature_name, vocab):\n",
    "    return tf.feature_column.indicator_column(\n",
    "        tf.feature_column.categorical_column_with_vocabulary_list(feature_name,\n",
    "                                                                  vocab))\n",
    "feature_columns = []\n",
    "for feature_name in CATEGORICAL_COLUMNS:\n",
    "    # Need to one-hot encode categorical features.\n",
    "    vocabulary = dftrain[feature_name].unique()\n",
    "    feature_columns.append(one_hot_cat_column(feature_name, vocabulary))\n",
    "\n",
    "for feature_name in NUMERIC_COLUMNS:\n",
    "    feature_columns.append(tf.feature_column.numeric_column(feature_name,\n",
    "                                                            dtype=tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Use entire batch since this is such a small dataset.\n",
    "NUM_EXAMPLES = len(y_train)\n",
    "\n",
    "def make_input_fn(X, y, n_epochs=None, shuffle=True):\n",
    "  def input_fn():\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((dict(X), y))\n",
    "    if shuffle:\n",
    "      dataset = dataset.shuffle(NUM_EXAMPLES)\n",
    "    # For training, cycle thru dataset as many times as need (n_epochs=None).\n",
    "    dataset = dataset.repeat(n_epochs)\n",
    "    # In memory training doesn't use batching.\n",
    "    dataset = dataset.batch(NUM_EXAMPLES)\n",
    "    return dataset\n",
    "  return input_fn\n",
    "\n",
    "# Training and evaluation input functions.\n",
    "train_input_fn = make_input_fn(dftrain, y_train)\n",
    "eval_input_fn = make_input_fn(dfeval, y_eval, shuffle=False, n_epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "with strategy.scope():\n",
    "    linear_est = tf.estimator.LinearClassifier(feature_columns)\n",
    "\n",
    "    # Train model.\n",
    "    linear_est.train(train_input_fn, max_steps=100)\n",
    "\n",
    "    # Evaluation.\n",
    "    result = linear_est.evaluate(eval_input_fn)\n",
    "    clear_output()\n",
    "    print(pd.Series(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "with strategy.scope():\n",
    "    n_batches = 1\n",
    "    est = tf.estimator.BoostedTreesClassifier(feature_columns,\n",
    "                                              n_batches_per_layer=n_batches)\n",
    "    \n",
    "    est.train(train_input_fn, max_steps=100)\n",
    "\n",
    "    # Eval.\n",
    "    result = est.evaluate(eval_input_fn)\n",
    "    clear_output()\n",
    "    print(pd.Series(result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "id": "MwBUgoWhI6hu"
   },
   "source": [
    "## Light GBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "qbLXXiPkQAt6"
   },
   "outputs": [],
   "source": [
    "cat_cols = ['part', 'tag_0']\n",
    "\n",
    "lgbm_params = {\n",
    "    'objective': 'binary',\n",
    "    'metric': ['auc'],\n",
    "    'device': 'gpu',\n",
    "    'num_leaves': 127,\n",
    "    'min_data_in_leaf': 100,\n",
    "    'max_bin': 255,\n",
    "    'learning_rate': 0.1,\n",
    "    'boosting': 'gbdt',\n",
    "    'reg_lambda': 4,\n",
    "    'reg_alpha': 1\n",
    "}\n",
    "\n",
    "folds_run = {'folds_train': folds[0:-1], 'folds_val': folds[-1:]}\n",
    "\n",
    "matrices = get_dmatrices(folds_train=folds_run['folds_train'],\n",
    "                         folds_val=folds_run['folds_val'],\n",
    "                         matrix=lgb.Dataset)\n",
    "\n",
    "# experiment = Experiment(display_summary_level=1)\n",
    "# experiment.log_parameter('folds_train', matrices['folds_train'])\n",
    "# experiment.log_parameter('folds_val', matrices['folds_val'])\n",
    "# experiment.log_parameter('df_train_len', len(df_train))\n",
    "\n",
    "evals_result = {}\n",
    "model = lgb.train(\n",
    "            params=lgbm_params,\n",
    "            train_set= matrices['train_matrix'],\n",
    "            valid_sets=[matrices['train_matrix'], matrices['valid_matrix']],\n",
    "            valid_names=['train', 'valid'],\n",
    "            verbose_eval=True,\n",
    "            evals_result=evals_result,\n",
    "            num_boost_round=1000,\n",
    "            early_stopping_rounds=20,\n",
    "            categorical_feature=cat_cols\n",
    "    )\n",
    "\n",
    "# experiment.end()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "bFq-OqTCgd4-"
   },
   "outputs": [],
   "source": [
    "pd.DataFrame({'feature': columns_train, 'gain': model.feature_importance('gain').astype('int')}).set_index('feature').sort_values('gain', ascending=False).plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "Pb9rB7XX_DJw"
   },
   "outputs": [],
   "source": [
    "model.save_model('model.lgb')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t49TzqNPq5pg"
   },
   "source": [
    "## Prepare Prediction Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GXZ10jxVjuRf"
   },
   "source": [
    "### Download Final Users State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3eJlJt1FjuRg",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# <hide-input><hide-output>\n",
    "# %%time\n",
    "user_state_cols = [\n",
    "    'user_id',\n",
    "    'timestamp',\n",
    "    'ts_delta',\n",
    "    'session',\n",
    "    'ac_cumsum',\n",
    "    'r_cumcnt',\n",
    "    'l_cumcnt',\n",
    "    'r_cumcnt_clip',\n",
    "    'pqet_cumsum',\n",
    "    'ac_cumsum_upto',\n",
    "    'r_cumcnt_upto',\n",
    "    'pqet_cumsum_upto',\n",
    "    'ac_cumsum_session',\n",
    "    'r_cumcnt_session',\n",
    "    'l_cumcnt_session',\n",
    "    'pqet_cumsum_session'\n",
    "]\n",
    "\n",
    "# user_state_cols.extend(list(dtypes_ewma_stats.keys()))\n",
    "\n",
    "from_bq_u = False\n",
    "query_u = Q.select_user_final_state()[0]\n",
    "filename_u = 'df_users.pkl'\n",
    "\n",
    "df_users = bqh.get_df_query_bqs(query_u, filename_u, from_bq_u, dtypes, fillna=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xok_9xlD7c0H"
   },
   "source": [
    "### Download Final Users-Content State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RUVdoI5V8FD1"
   },
   "outputs": [],
   "source": [
    "# <hide-input><hide-output>\n",
    "# %%time\n",
    "\n",
    "from_bq_uc = False\n",
    "query_uc = Q.select_users_content_final_state(table_id='train')[0]\n",
    "filename_uc = 'df_users_content.pkl'\n",
    "\n",
    "df_users_content = bqh.get_df_query_bqs(query_uc, filename_uc, from_bq_uc, dtypes, fillna=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_users_content.head().isna"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7zfU3NaqRbte"
   },
   "source": [
    "### Download Final Users-Part State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rkeXg0r5Rd_C"
   },
   "outputs": [],
   "source": [
    "# <hide-input><hide-output>\n",
    "# %%time\n",
    "\n",
    "from_bq_up = False\n",
    "query_up = Q.select_users_part_final_state(table_id='train')[0]\n",
    "filename_up = 'df_users_part.pkl'\n",
    "\n",
    "df_users_part = bqh.get_df_query_bqs(query_up, filename_up, from_bq_up, dtypes, fillna=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_users_part.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EVS-PapIkfPG"
   },
   "source": [
    "### Download Final Users-Tag State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-v6vBK7vkdzf"
   },
   "outputs": [],
   "source": [
    "# <hide-input><hide-output>\n",
    "# This is on ice for the time being - didn't improve model\n",
    "from_bq_ut = False\n",
    "query_ut = Q.select_users_tag_final_state(table_id='tag_response')\n",
    "filename_ut = 'df_users_tag.pkl'\n",
    "\n",
    "df_users_tag = bqh.get_df_query_bqs(query_ut, filename_ut, from_bq_ut, dtypes, fillna=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m_RJCpSxcrGy"
   },
   "outputs": [],
   "source": [
    "df_users_tag.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EVS-PapIkfPG"
   },
   "source": [
    "### Download Final Users-Part-Tag State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-v6vBK7vkdzf"
   },
   "outputs": [],
   "source": [
    "# <hide-input><hide-output>\n",
    "from_bq_upt = False\n",
    "query_upt = Q.select_users_part_tag_final_state(table_id='tag_response')[0]\n",
    "filename_upt = 'df_users_part_tag.pkl'\n",
    "\n",
    "df_users_tag = bqh.get_df_query_bqs(query_upt, filename_upt, from_bq_upt, dtypes, fillna=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tWvj0FYpjuRj"
   },
   "source": [
    "### Download Questions Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_cols = [\n",
    "    'question_id',\n",
    "    'part',\n",
    "    'tags',\n",
    "    'tags_array',\n",
    "    'tags_code',\n",
    "    'part_correct_pct',\n",
    "    'question_id_correct_pct',\n",
    "    'tags_correct_pct',\n",
    "    'part_tags_correct_pct',\n",
    "    'part_pqet_avg',\n",
    "    'question_id_pqet_avg',\n",
    "    'tags_pqet_avg',\n",
    "    'part_tags_pqet_avg'\n",
    "]\n",
    "\n",
    "from_bq_q = False\n",
    "\n",
    "query_q = f\"\"\"\n",
    "    SELECT {(',').join(q_cols)}\n",
    "    FROM {DATASET}.content_tags\n",
    "    WHERE question_id IS NOT NULL\n",
    "\"\"\"\n",
    "\n",
    "filename_q = 'df_questions.pkl'\n",
    "\n",
    "df_questions = bqh.get_df_query_bqs(query_q, filename_q, from_bq_q, dtypes, fillna=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tWvj0FYpjuRj"
   },
   "source": [
    "### Download Lectures Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_cols = [\n",
    "    'lecture_id',\n",
    "    'part',\n",
    "    'tags',\n",
    "]\n",
    "\n",
    "from_bq_l = False\n",
    "\n",
    "query_l = f\"\"\"\n",
    "    SELECT {(',').join(l_cols)}\n",
    "    FROM {DATASET}.content_tags\n",
    "    WHERE lecture_id IS NOT NULL\n",
    "\"\"\"\n",
    "\n",
    "filename_l = 'df_lectures.pkl'\n",
    "\n",
    "df_lectures = bqh.get_df_query_bqs(query_l, filename_l, from_bq_l, dtypes, fillna=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_NH5WM3VjuRn"
   },
   "source": [
    "## Update Kaggle Submission Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6GV4lXzRKvE8"
   },
   "outputs": [],
   "source": [
    "if True:\n",
    "    exps={}\n",
    "    for e in [\n",
    "              '746ded3859ac4e0d8750df0fc057c469'\n",
    "              ]:\n",
    "        \n",
    "        exp = APIExperiment(previous_experiment=e)\n",
    "        exp.download_model(e, 'models')\n",
    "        e_model = xgb.Booster(model_file=f'models/{e}.xgb')\n",
    "        exps[e] = e_model.attributes()\n",
    "        \n",
    "        asset_id = [asset.get('assetId') for asset in exp.get_asset_list() if asset.get('fileName') == 'df_importances.json'][0]\n",
    "        df_imp = pd.DataFrame(exp.get_asset(asset_id, return_type='json'))\n",
    "\n",
    "        columns_train = df_imp.index.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x0B8sbkGjuRo"
   },
   "outputs": [],
   "source": [
    "# <hide-input>\n",
    "if True:\n",
    "    Path(KAGGLE_SUBMIT_DATASET).mkdir(exist_ok=True)\n",
    "\n",
    "    with open(f'{KAGGLE_SUBMIT_DATASET}/columns.json', 'w') as cj:\n",
    "            json.dump(columns_train, cj)\n",
    "\n",
    "    with open(f'{KAGGLE_SUBMIT_DATASET}/dtypes.json', 'w') as dj:\n",
    "            json.dump(dtypes, dj)\n",
    "    \n",
    "    with open(f'{KAGGLE_SUBMIT_DATASET}/models.json', 'w') as mj:\n",
    "        json.dump(exps, mj)\n",
    "        \n",
    "    with open(f'{KAGGLE_SUBMIT_DATASET}/alphas.json', 'w') as aj:\n",
    "        json.dump(alphas, aj)\n",
    "        \n",
    "    with open(f'{KAGGLE_SUBMIT_DATASET}/top_content_ids.json', 'w') as tj:\n",
    "        json.dump(top_content_ids, tj)\n",
    "        \n",
    "    for m in exps:\n",
    "        src = Path(f'models/{m}.xgb')\n",
    "        Path(f'{KAGGLE_SUBMIT_DATASET}/{src.name}').write_bytes(src.read_bytes())\n",
    "    \n",
    "    df_files = {\n",
    "        'df_users.pkl': df_users,\n",
    "        'df_users_content.pkl': df_users_content,\n",
    "        'df_users_part.pkl': df_users_content,\n",
    "        'df_users_tag.pkl': df_users_tag,\n",
    "#         'df_users_part_tag.pkl': df_users_tag,\n",
    "        'df_questions.pkl': df_questions,\n",
    "        'df_lectures.pkl': df_lectures,\n",
    "        'df_users_update.pkl': df_users_update\n",
    "    }\n",
    "\n",
    "    for file_path, df in df_files.items():\n",
    "        if Path(file_path).exists():\n",
    "            (Path(f'{KAGGLE_SUBMIT_DATASET}/{file_path}')\n",
    "            .write_bytes(Path(file_path).read_bytes()))\n",
    "        else:\n",
    "            df.to_pickle(f'{KAGGLE_SUBMIT_DATASET}/{file_path}')\n",
    "\n",
    "    (Path(f'{KAGGLE_SUBMIT_DATASET}/weights_all.npy')\n",
    "    .write_bytes(Path('weights_all.npy').read_bytes()))\n",
    "            \n",
    "    kaggle_id = f\"{os.getenv('KAGGLE_USERNAME')}/{KAGGLE_SUBMIT_DATASET}\"\n",
    "    \n",
    "    metadata = {\n",
    "        \"licenses\": [{\"name\": \"CC0-1.0\"}],\n",
    "        \"id\": kaggle_id,\n",
    "        \"title\": KAGGLE_SUBMIT_DATASET\n",
    "    }\n",
    "\n",
    "    with open(f'{KAGGLE_SUBMIT_DATASET}/dataset-metadata.json', 'w') as f:\n",
    "        json.dump(metadata, f)\n",
    "            \n",
    "    if kaggle_api.dataset_status(kaggle_id):\n",
    "        kaggle_api.dataset_create_version(KAGGLE_SUBMIT_DATASET,\n",
    "                                          version_notes='update dataset',\n",
    "                                          delete_old_versions=True,\n",
    "                                          dir_mode='tar',\n",
    "                                          quiet=True\n",
    "                                         )\n",
    "    else:\n",
    "        kaggle_api.dataset_create_new(KAGGLE_SUBMIT_DATASET,\n",
    "                                      dir_mode='tar', quiet=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d-RK0etUwdw3"
   },
   "source": [
    "## Push Kernel to Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XoJcDapFwhCT"
   },
   "outputs": [],
   "source": [
    "# <hide-input>\n",
    "if NOT_KAGGLE:\n",
    "    if False:\n",
    "        \n",
    "        code_file = 'riiid-2020.ipynb'\n",
    "        with open(DRIVE/REPO/code_file, 'r') as nb:\n",
    "            nb_json = json.load(nb)       \n",
    "        \n",
    "        for i, cell in enumerate(nb_json['cells']):\n",
    "            if cell['cell_type'] == 'code':\n",
    "                \n",
    "                # update show/hide code cells\n",
    "                for h in ['input', 'output']:c\n",
    "                    if cell['source'][0].find(f'<hide-{h}') > 1:\n",
    "                        nb_json['cells'][i]['metadata'].update({f'_kg_hide-{h}': True})\n",
    "                    else:\n",
    "                        nb_json['cells'][i]['metadata'].pop(f'_kg_hide-{h}', None)\n",
    "\n",
    "                # add modules as cells\n",
    "                if len(cell['source']) == 1:\n",
    "                    groups = re.search(r'(?<=\\<include-)(.*?)(?=\\>)', cell['source'][0])\n",
    "                    \n",
    "                    if groups:\n",
    "                        with open(DRIVE/REPO/groups.group(0), 'r') as m:\n",
    "                            nb_json['cells'][i]['source'] = m.readlines() + nb_json['cells'][i]['source']    \n",
    "\n",
    "\n",
    "        if Path(code_file).exists():\n",
    "            Path(code_file).unlink()\n",
    "        \n",
    "        with open(f'{code_file}', 'w') as f:\n",
    "            json.dump(nb_json, f)\n",
    "\n",
    "        data = {'id': 'calebeverett/riiid-bigquery-xgboost-end-to-end',\n",
    "                        'title': 'RIIID: BigQuery-XGBoost End-to-End',\n",
    "                        'code_file': code_file,\n",
    "                        'language': 'python',\n",
    "                        'kernel_type': 'notebook',\n",
    "                        'is_private': 'false',\n",
    "                        'enable_gpu': 'true',\n",
    "                        'enable_internet': 'true',\n",
    "                        'dataset_sources': [],\n",
    "                        'competition_sources': ['riiid-test-answer-prediction'],\n",
    "                        'kernel_sources': []}\n",
    "        \n",
    "        with open('kernel-metadata.json', 'w') as f:\n",
    "            json.dump(data, f)\n",
    "\n",
    "        kaggle_api.kernels_push('.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HN5k3_N-juRp"
   },
   "source": [
    "## Submit From Kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BGXlGXmjjuRq"
   },
   "source": [
    "* Go to [RIIID Submit](https://www.kaggle.com/calebeverett/riiid-submit), fork and update to reference your dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vmHGcMTtEDZi"
   },
   "source": [
    "## Push Submit Kernel to Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h5oJG61mU9ES"
   },
   "outputs": [],
   "source": [
    "dataset_status = None\n",
    "while dataset_status != 'ready':\n",
    "    print('updating dataset...')\n",
    "    time.sleep(3)\n",
    "    dataset_status = kaggle_api.datasets_status(CONFIG.get('KAGGLE_USERNAME'),\n",
    "                                                KAGGLE_SUBMIT_DATASET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iO6rPRH6EQJZ"
   },
   "outputs": [],
   "source": [
    "# <hide-input>\n",
    "if NOT_KAGGLE:\n",
    "    if True:\n",
    "        submit_kernel = 'calebeverett/riiid-submit-private'\n",
    "        kernel_path = Path('submit_kernel')\n",
    "        kaggle_api.kernels_pull('calebeverett/riiid-submit-private', kernel_path, metadata=True)\n",
    "        kaggle_api.kernels_push(kernel_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I4xRZVyHWFLT"
   },
   "outputs": [],
   "source": [
    "kaggle_api.kernels_status('calebeverett/riiid-submit-private')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_VVtvmvU3s9T"
   },
   "source": [
    "## Update Experiments with Submission Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Qfp_-0ZMN7xT"
   },
   "outputs": [],
   "source": [
    "kaggle_api.kernels_list(mine=True, competition='riiid-test-answer-prediction')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ct8kmjFx30aw"
   },
   "outputs": [],
   "source": [
    "kaggle_api.competition_submissions_cli('riiid-test-answer-prediction')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wDpy6AmWx_U8"
   },
   "outputs": [],
   "source": [
    "if True:\n",
    "    exp_keys = ['ae1aabe76df44072ae163b8068fe51ad']\n",
    "\n",
    "    for exp_key in exp_keys:\n",
    "        exp = APIExperiment(previous_experiment=exp_key)\n",
    "        exp.log_other('submitted', True)\n",
    "        exp.log_other('submitDate', '2020-12-09 20:17:57')\n",
    "        exp.log_other('publicScore', 0.774)\n",
    "        exp.log_other('kernelUrl', 'https://www.kaggle.com/calebeverett/riiid-submit-private?scriptVersionId=48932349')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "cs6bXr35juQM",
    "QhPzQmvUjuQV",
    "GOpOrtr9juQY",
    "sgTZkanIjuQk",
    "YTG_h2a2juQy",
    "zO6GM_CLjuQ5"
   ],
   "machine_shape": "hm",
   "name": "riiid-2020-new.ipynb",
   "private_outputs": true,
   "provenance": [],
   "toc_visible": true
  },
  "environment": {
   "name": "tf2-2-3-gpu.2-3.m59",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-2-3-gpu.2-3:m59"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "396.794px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
