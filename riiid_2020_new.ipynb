{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "orpoYGXZdptj"
   },
   "outputs": [],
   "source": [
    "!curl ipinfo.io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "id": "N4lHWTOFrRgT"
   },
   "outputs": [],
   "source": [
    "# <hide-input>\n",
    "def show_version_history():\n",
    "    from IPython.display import HTML\n",
    "    style_header = 'mui--align-bottom mui--bg-primary mui--text-light mui--text-center'\n",
    "    style_cell = 'mui--align-top mui--text-center'\n",
    "\n",
    "    # print(('\\n').join(list(map(make_li, sorted(dtypes.keys())))))\n",
    "\n",
    "    html_str = f\"\"\"\n",
    "    <link href=\"//cdn.muicss.com/mui-0.10.3/css/mui.min.css\" rel=\"stylesheet\" type=\"text/css\" />\n",
    "    <div class=\"mui-container-fluid\">\n",
    "        <h2>Version History</h2>\n",
    "        <div style=\"max-width:1016px\" class=\"mui-row\">\n",
    "            <div class=\"mui-col-8\">\n",
    "                <table class=\"mui-table mui-table--bordered\">\n",
    "                    <tr>\n",
    "                        <th width=\"12%\" class=\"{style_header}\">Version</th>\n",
    "                        <th width=\"12%\" class=\"{style_header}\">Date</th>\n",
    "                        <th width=\"12%\" class=\"{style_header}\">Local CV</th>\n",
    "                        <th width=\"12%\" class=\"{style_header}\">Public<br>Leaderboard</th>\n",
    "                        <th class=\"{style_header} mui--align-bottom mui--bg-primary\n",
    "                            mui--text-dark mui--text-left\">Notes</th>                    \n",
    "                    </tr>\n",
    "                    <tr>\n",
    "                        <td class=\"{style_cell}\">58</td>\n",
    "                        <td class=\"{style_cell}\">2020-11-15</td>\n",
    "                        <td class=\"{style_cell}\">0.756</td>\n",
    "                        <td class=\"{style_cell}\">0.762</td>\n",
    "                        <td><ul>\n",
    "                                <li>Completed submission pipeline with minimal feature set:\n",
    "                                    <ul>\n",
    "                                        <li><code>answered_correctly_content_id_cumsum</code></li>\n",
    "                                        <li><code>answered_correctly_cumsum</code></li>\n",
    "                                        <li><code>answered_correctly_cumsum_pct</code></li>\n",
    "                                        <li><code>answered_incorrectly_content_id_cumsum</code></li>\n",
    "                                        <li><code>answered_incorrectly_cumsum</code></li>\n",
    "                                        <li><code>part</code></li>\n",
    "                                        <li><code>part_correct_pct</code></li>\n",
    "                                        <li><code>question_id_correct_pct</code></li>\n",
    "                                        <li><code>tag__0</code></li>\n",
    "                                        <li><code>tag__0_correct_pct</code></li>\n",
    "                                        <li><code>task_container_id</code></li>\n",
    "                                        <li><code>timestamp</code></li>\n",
    "                                    </ul>\n",
    "                                </li>\n",
    "                                <li>Changed logic on roll sum to be over trailing\n",
    "                                    rows preceding the current <code>task_container_id</code> instead\n",
    "                                    of over trailing task containers\n",
    "                                    (expensive)\n",
    "                                </li>\n",
    "                            </ul>\n",
    "                        </td>\n",
    "                    </tr>\n",
    "                    <tr>\n",
    "                        <td class=\"{style_cell}\">53</td>\n",
    "                        <td class=\"{style_cell}\">2020-11-07</td>\n",
    "                        <td class=\"{style_cell}\">0.761</td>\n",
    "                        <td class=\"{style_cell}\">--</td>\n",
    "                        <td><ul><li>Housekeeping:\n",
    "                                    <ul>\n",
    "                                        <li>Consolidated notebook and modules in single repo</li>\n",
    "                                        <li>Streamlined Colab repo workflow using Drive</>\n",
    "                                        <li>Included modules in notebook when pushed to Kaggle</li>\n",
    "                                        <li>Eliminated CONFIG requirement when run in Kaggle</li>\n",
    "                                    </ul>\n",
    "                                </li>\n",
    "                            </ul>\n",
    "                        </td>\n",
    "                    </tr>\n",
    "                    <tr>\n",
    "                        <td class=\"{style_cell}\">40</td>\n",
    "                        <td class=\"{style_cell}\">2020-11-05</td>\n",
    "                        <td class=\"{style_cell}\">0.761</td>\n",
    "                        <td class=\"{style_cell}\">--</td>\n",
    "                        <td>\n",
    "                            <ul>\n",
    "                                <li>Features added:\n",
    "                                    <ul>\n",
    "                                        <li><code>answered_correctly_content_id_cumsum</code></li>\n",
    "                                        <li><code>answered_correctly_content_id_cumsum_pct</code></li>\n",
    "                                        <li><code>answered_correctly_cumsum10</code></li>\n",
    "                                        <li><code>answered_correctly_cumsum_pct</code></li>\n",
    "                                        <li><code>answered_correctly_rollsum_pct</code></li>\n",
    "                                        <li><code>answered_incorrectly_content_id_cumsum</code></li>\n",
    "                                        <li><code>lectures_cumcount</code></li>\n",
    "                                        <li><code>prior_question_elapsed_time_rollavg</code></li>\n",
    "                                    </ul>\n",
    "                                </li>\n",
    "                                <li>Single model, single fold</li>\n",
    "                                <li>No public leaderboard - efficient inference in progress</li>\n",
    "                                <li>Refactored code to move queries and helper functions into\n",
    "                                    separate modules</li>\n",
    "                                <li>Completed set up to commit code to Github from Colab and</li>\n",
    "                                <li>Completed set up to push kernels to Kaggle from Colab</li>\n",
    "                            </ul>\n",
    "                        </td>\n",
    "                    </tr>\n",
    "                    <tr>\n",
    "                        <td class=\"{style_cell}\">37</td>\n",
    "                        <td class=\"{style_cell}\">2020-11-04</td>\n",
    "                        <td class=\"{style_cell}\">0.751</td>\n",
    "                        <td class=\"{style_cell}\">0.748</td>\n",
    "                        <td>\n",
    "                            <ul>\n",
    "                                <li>Features added:\n",
    "                                    <ul>\n",
    "                                        <li><code>answered_correctly_cumsum</code></li>\n",
    "                                        <li><code>answered_correctly_rollsum</code></li>\n",
    "                                        <li><code>answered_incorrectly_cumsum</code></li>\n",
    "                                        <li><code>answered_incorrectly_rollsum</code></li>\n",
    "                                        <li><code>part</code></li>\n",
    "                                        <li><code>part_correct_pct</code></li>\n",
    "                                        <li><code>question_id_correct_pct</code></li>\n",
    "                                        <li><code>tag__0</code></li>\n",
    "                                        <li><code>tag__0_correct_pct</code></li>\n",
    "                                    </ul>\n",
    "                                </li>\n",
    "                                <li>Single model, single fold</li>\n",
    "                                <li>Model for public leaderboard didn't include\n",
    "                                    rolling features - still working out how to\n",
    "                                    efficiently calculate for inference</li>\n",
    "                            </ul>\n",
    "                        </td>\n",
    "                    </tr>\n",
    "                </table>\n",
    "            </div>\n",
    "        </div>\n",
    "        <p>\n",
    "            <a href=\"https://colab.research.google.com/github/CalebEverett/riiid_2020/blob/master/riiid-2020.ipynb\" target=\"_blank\" rel=\"nofollow\">\n",
    "            <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a>\n",
    "        </p>\n",
    "    </div>\n",
    "    \"\"\"\n",
    "\n",
    "    html = HTML(html_str)\n",
    "    display(html)\n",
    "show_version_history()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "id": "8QCO0cr_juQA"
   },
   "source": [
    "## Intro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "L1mQ0DiRnfer"
   },
   "source": [
    "This kernel is an end to end pipeline that uses BigQuery to store data and perform feature engineering, and trains a model using XGBoost. I was resorting to breaking up tables and still waiting a long time to see the results of my analysis and to process my engineered features, so I decided to learn about BigQuery. This kernel is the current state of my setup, which is working very well. It is much faster than my previous local setup, even with having to download files. It also is making it easier to keep the structure of the data and and code clean, which in turn makes it easier to stay focused on thinking about and executing ideas without getting bogged down waiting for things to finish or wading through extraneous processing code.\n",
    "\n",
    "I've attempted to put  this book together in such a way that somebody else can fork it, update a few environment variables, run it and then be in the game engineering features and improving the model. The only requirements are a GCP project and storage bucket. Other than that, it is turn key, starting with creating a BigQuery dataset and ending with a saved model and two feature tables that get uploaded to a Kaggle dataset where they are used in a separate kernel to make predictions and submit to the competition api.\n",
    "\n",
    "A couple of cool features:\n",
    "* Uses the gcs version of the competition datset to create a dataset and upload to BigQuery in around a minute\n",
    "* Transformations get run on the entire train table at once and run in under 10 minutes\n",
    "* Feature engineering gets done on a sample of the train table, taking advantage of BigQuery' graphical query editing interface that includes tab completion, syntax checking and the ability to run queries and inspect results\n",
    "* Stores queries as methods on a dedicated class, where they can be easily reused\n",
    "* Dtypes for local dataframes, schema for BigQuery tables and all tranformations are maintained locally so that the transformed tables can be recreated from the original competition dataset files automatically at any time (see description of workflow below to continue with this practice)\n",
    "* Exports to gcs using temporary tables created by BigQuery avoiding unnecessary storage and wasted time rerunning and exporting duplicate queries\n",
    "* Separate [submission kernel](https://www.kaggle.com/calebeverett/riiid-submit) uses sqlite3 to achieve sub two hour submission times while maintaining state for questions, users and user-content (80+ million rows)\n",
    "\n",
    "I've engineered a few features as a starting point to demonstrate how additional features can be efficiently developed and processed, including:\n",
    "* Cumulative and rolling sums of questions answered correctly and incorrectly by user\n",
    "* Percent of questions answered correctly by question id, part and the first question tag\n",
    "\n",
    "The model is also just a starting point, with a first pass at a train/validation split and no hyperparameter tuning. I have included some basic diagnostics on both the train/validtion split and model performance as a starting place for further development. Trained on a small subset of the overall training data with a small number of engineered features, it is producing a local validation AUC score of around 0.75 and slighly less than that on the public leaderboard.\n",
    "\n",
    "I have the table creation and transformation functions set to not run, but you can set them to run, by changing the flags to `True` for:\n",
    "* Loading tables - one flag for the questions table and another for the train and lectures tables\n",
    "* Updating the schemas in BigQuery\n",
    "* Performing the transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "id": "savju5R9juQB"
   },
   "source": [
    "## Resources\n",
    "* [BigQuery Console](https://console.cloud.google.com/bigquery)\n",
    "* [Python Client for Google BigQuery](https://googleapis.dev/python/bigquery/latest/index.html)\n",
    "* [Analytic function concepts in Standard SQL](https://cloud.google.com/bigquery/docs/reference/standard-sql/analytic-function-concepts)\n",
    "* [XGBoost Documentation](https://xgboost.readthedocs.io/en/latest/index.html)\n",
    "* [Storge Client](https://googleapis.dev/python/storage/latest/client.html)\n",
    "* [pandas documentation](https://pandas.pydata.org/docs/)\n",
    "* [Plotly Python Open Source Graphing Library](https://plotly.com/python/)\n",
    "* [PEP 8 -- Style Guide for Python Code](https://www.python.org/dev/peps/pep-0008/)\n",
    "* [Comet.ml Experiment API](https://www.comet.ml/docs/python-sdk/Experiment/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "88t93RZzj982"
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "tKe4S7M4nbSU"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "\n",
      "\n",
      "Git global user.name and user.email set.\n"
     ]
    }
   ],
   "source": [
    "# <hide-input>\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from datetime import datetime\n",
    "import gc\n",
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "import re\n",
    "import subprocess\n",
    "import sys\n",
    "import time\n",
    "\n",
    "import ipywidgets as widgets\n",
    "from google.cloud import storage, bigquery\n",
    "from google.cloud.bigquery import SchemaField\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "BUCKET = 'caleb-riiid'\n",
    "DATASET = 'data'\n",
    "LOCATION = 'europe-west4'\n",
    "KAGGLE_SUBMIT_DATASET = 'riiid-submission-private'\n",
    "PROJECT = 'fastai-caleb'\n",
    "REPO = 'riiid_2020'\n",
    "NOT_KAGGLE = os.getenv('KAGGLE_URL_BASE') is None\n",
    "\n",
    "# if NOT_KAGGLE:\n",
    "#     from google.colab import drive\n",
    "#     DRIVE = Path('/content/drive/My Drive')\n",
    "#     if not DRIVE.exists():\n",
    "#         drive.mount(str(DRIVE.parent))\n",
    "#     sys.path.append(str(DRIVE))\n",
    "#     g_creds_path = 'credentials/riiid-caleb-faddd0c9d900.json'\n",
    "#     os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = str(DRIVE/g_creds_path)\n",
    "\n",
    "bucket = storage.Client(project=PROJECT).get_bucket(BUCKET)\n",
    "dataset = bigquery.Dataset(f'{PROJECT}.{DATASET}')\n",
    "bq_client = bigquery.Client(project=PROJECT, location=LOCATION)\n",
    "\n",
    "if NOT_KAGGLE:\n",
    "    CONFIG = json.loads(bucket.get_blob('config.json').download_as_string())\n",
    "    os.environ = {**os.environ, **CONFIG}\n",
    "    sys.path.append('/home/jupyter')\n",
    "    from riiid_2020.utilities import check_packages, Git\n",
    "    from riiid_2020.bqhelpers import BQHelper\n",
    "    from riiid_2020.queries import Queries\n",
    "    \n",
    "    git = Git(REPO, CONFIG.get('GIT_USERNAME'), CONFIG.get('GIT_PASSWORD'),\n",
    "              CONFIG.get('EMAIL'), Path('../'))\n",
    "\n",
    "#     packages = {\n",
    "#         'comet-ml': ('3.2.5',''),\n",
    "#         'gcsfs': ('0.7.1',''),\n",
    "#         'kaggle': ('1.5.10',''),\n",
    "#         'plotly': ('4.12.0',''),\n",
    "#         'xgboost': ('1.2.0','')\n",
    "#         # 'lightgbm': ('3.1.0',' --install-option=--gpu')\n",
    "#     }\n",
    "#     check_packages(packages)\n",
    "\n",
    "    from comet_ml import APIExperiment, Experiment\n",
    "    from kaggle.api.kaggle_api_extended import KaggleApi\n",
    "    kaggle_api = KaggleApi()\n",
    "    kaggle_api.authenticate()\n",
    "\n",
    "import plotly\n",
    "import plotly.express as px\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "pd.options.plotting.backend = 'plotly'\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3I6-yy4QBxlR"
   },
   "source": [
    "## Modules\n",
    "Included in notebook for convenience when in a Kaggle kernel. Github repo [here](https://github.com/CalebEverett/riiid_2020)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "--uRkD6w0EhS"
   },
   "outputs": [],
   "source": [
    "# <include-bqhelpers.py><hide-input>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dTP7zTkV0qhK"
   },
   "outputs": [],
   "source": [
    "# <include-queries.py><hide-input>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FWx9otMx1O2Y"
   },
   "outputs": [],
   "source": [
    "# <include-utilities.py><hide-input>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r4nP2GeV09FG"
   },
   "outputs": [],
   "source": [
    "# <include-config.json><hide-input><hide-output>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "WO6DVIxuQydR"
   },
   "outputs": [],
   "source": [
    "Q = Queries(DATASET)\n",
    "bqh = BQHelper(bucket, DATASET, bq_client)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cs6bXr35juQM"
   },
   "source": [
    "## Create BigQuery Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MVloHU3wjuQQ"
   },
   "outputs": [],
   "source": [
    "if False:\n",
    "    delete_contents=False\n",
    "    bq_client.delete_dataset(DATASET, delete_contents=delete_contents)\n",
    "    print(f'Dataset {dataset.dataset_id} deleted from project {dataset.project}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "y2-E_hTejuQO"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset data already exists in location europe-west4 in project fastai-caleb.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    dataset = bq_client.get_dataset(dataset.dataset_id)\n",
    "    print(f'Dataset {dataset.dataset_id} already exists '\n",
    "          f'in location {dataset.location} in project {dataset.project}.')\n",
    "except:\n",
    "    dataset = bq_client.create_dataset(dataset)\n",
    "    print(f'Dataset {dataset.dataset_id} created '\n",
    "          f'in location {dataset.location} in project {dataset.project}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HItb6CWGjuQS"
   },
   "source": [
    "## Load Tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "id": "BjTuu8CJjuQT"
   },
   "source": [
    "### Dataframe dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "hidden": true,
    "id": "8dxMzeCsjuQT"
   },
   "outputs": [],
   "source": [
    "# <hide-input>\n",
    "dtypes_orig = {\n",
    "    'lectures': {\n",
    "        'lecture_id': 'uint16',\n",
    "        'tag': 'uint8',\n",
    "        'part': 'uint8',\n",
    "        'type_of': 'str',\n",
    "    },\n",
    "    'questions': {\n",
    "        'question_id': 'uint16',\n",
    "        'bundle_id': 'uint16',\n",
    "        'correct_answer': 'uint8',\n",
    "        'part': 'uint8',\n",
    "        'tags': 'str',\n",
    "        \n",
    "    },\n",
    "    'train': {\n",
    "        'row_id': 'int64',\n",
    "        'timestamp': 'int64',\n",
    "        'user_id': 'int32',\n",
    "        'content_id': 'int16',\n",
    "        'content_type_id': 'int8',\n",
    "        'task_container_id': 'int16',\n",
    "        'user_answer': 'int8',\n",
    "        'answered_correctly': 'int8',\n",
    "        'prior_question_elapsed_time': 'float32', \n",
    "        'prior_question_had_explanation': 'bool',\n",
    "    }\n",
    "    \n",
    "}\n",
    "\n",
    "dtypes_new = {\n",
    "    'lectures': {},\n",
    "    'questions': {},\n",
    "    'train': {\n",
    "        'ql_id': 'int16',\n",
    "        'fold': 'int8',\n",
    "        'lectures_cumcnt': 'int16',\n",
    "        'lectures_cumcnt_part': 'int16',\n",
    "        'lectures_cumcnt_tag_0': 'int16',\n",
    "        'lectures_cumcnt_tags': 'int16',\n",
    "        'lectures_cumcnt_session': 'int16',\n",
    "        'task_container_id_orig': 'int16',\n",
    "        'ts_minute': 'int16',\n",
    "        'session_minute_max': 'int16',\n",
    "        'pqet_sec': 'int16',\n",
    "        'pqet_sec_rollavg': 'int16',\n",
    "        'session': 'uint8',\n",
    "        'r_cumcnt_clip': 'int16',\n",
    "        'ac_rollsum_session': 'int16',\n",
    "        'r_rollcnt_session': 'int16',\n",
    "        'ac_rollsum_pct_session': 'int8',\n",
    "        'aic_rollsum_session': 'int16',\n",
    "        'lectures_rollcnt_session': 'int16'\n",
    "    }\n",
    "}\n",
    "\n",
    "dtypes_content_tags = {\n",
    "    'ql_id': 'int16',\n",
    "    'question_id': 'int16',\n",
    "    'lecture_id': 'int16',\n",
    "    'bundle_id': 'uint16',\n",
    "    'correct_answer': 'uint8',\n",
    "    'part': 'int8',\n",
    "    'tags': 'str',\n",
    "    'tags_array': 'str',\n",
    "    'tag_0': 'uint8',\n",
    "    'part_correct_pct': 'int8',\n",
    "    'tag_0_correct_pct': 'int8',\n",
    "    'tag_0_part_correct_pct': 'int8',\n",
    "    'question_id_correct_pct': 'int8',\n",
    "    'tags_correct_pct': 'int8',\n",
    "    'tags_code': 'int16',\n",
    "    'tags_correct_pct': 'int16',\n",
    "    'part_pqet_avg': 'int16',\n",
    "    'tag_0_pqet_avg': 'int16',\n",
    "    'question_id_pqet_avg': 'int16',\n",
    "    'tags_pqet_avg': 'int16'\n",
    "}\n",
    "\n",
    "# each of these gets cumsums for correct and\n",
    "# incorrect, row count and pct correct\n",
    "cumsum_cols = {\n",
    "    'ac_cumsum': 'int16',\n",
    "    'ac_cumsum_content_id': 'int16',\n",
    "    'ac_cumsum_part': 'int16',\n",
    "    'ac_cumsum_session': 'int16',\n",
    "    'ac_cumsum_tag_0': 'int16',\n",
    "    'ac_cumsum_tags': 'int16',\n",
    "    'ac_cumsum_upto': 'int8',\n",
    "}\n",
    "\n",
    "for c, t in cumsum_cols.items():\n",
    "    dtypes_new['train'][c] = t\n",
    "    r_col = c.replace('ac_cumsum', 'r_cumcnt')\n",
    "    dtypes_new['train'][r_col] = t\n",
    "    i_col = c.replace('ac_cumsum', 'aic_cumsum')\n",
    "    dtypes_new['train'][i_col] = t\n",
    "    p_col = c.replace('ac_cumsum', 'ac_cumsum_pct')\n",
    "    dtypes_new['train'][p_col] = 'int8'\n",
    "\n",
    "dtypes = {}\n",
    "for table_id in dtypes_orig:\n",
    "    dtypes[table_id] = {\n",
    "        **dtypes_orig[table_id],\n",
    "        **dtypes_new[table_id]\n",
    "    }\n",
    "\n",
    "dtypes = {\n",
    "    **dtypes['train'],\n",
    "    **dtypes_content_tags\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "id": "QhPzQmvUjuQV"
   },
   "source": [
    "### BigQuery Table Schemas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "hidden": true,
    "id": "WdW0igS4juQW"
   },
   "outputs": [],
   "source": [
    "# <hide-input>\n",
    "type_map = {\n",
    "    'int64': 'INTEGER',\n",
    "    'int32': 'INTEGER',\n",
    "    'int16': 'INTEGER',\n",
    "    'int8': 'INTEGER',\n",
    "    'uint8': 'INTEGER',\n",
    "    'uint16': 'INTEGER',\n",
    "    'str': 'STRING',\n",
    "    'bool': 'BOOL',\n",
    "    'float32': 'FLOAT'\n",
    "}\n",
    "\n",
    "schemas_orig = {table: [SchemaField(f, type_map[t]) for f, t in\n",
    "                   fields.items()] for table, fields in dtypes_orig.items()}\n",
    "\n",
    "schemas = {}\n",
    "for table_id, fields in dtypes_new.items():\n",
    "    new_fields = [SchemaField(f, type_map[t]) for\n",
    "                  f, t in fields.items()]\n",
    "    schemas[table_id] = schemas_orig[table_id] + new_fields"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GOpOrtr9juQY"
   },
   "source": [
    "### Load Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XXwb4YkWjuQe"
   },
   "outputs": [],
   "source": [
    "# <hide-input>\n",
    "if False:\n",
    "    for table_id in dtypes_orig:\n",
    "        bqh.del_table(table_id)\n",
    "        lj = bqh.load_csv_uri(table_id, schemas_orig).result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CHkK8JhQjuQg"
   },
   "outputs": [],
   "source": [
    "# <hide-input><hide-output>\n",
    "df_jobs = bqh.get_df_jobs()\n",
    "df_jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rdk5EpmsQ7Pl"
   },
   "outputs": [],
   "source": [
    "if False:\n",
    "    job = bq_client.get_job(df_jobs.iloc[1].job_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "5cGVHAtMjuQi"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>table_id</th>\n",
       "      <th>cols</th>\n",
       "      <th>rows</th>\n",
       "      <th>kb</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>content_tags</td>\n",
       "      <td>14</td>\n",
       "      <td>13941</td>\n",
       "      <td>1141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>lectures</td>\n",
       "      <td>4</td>\n",
       "      <td>418</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>questions</td>\n",
       "      <td>5</td>\n",
       "      <td>13523</td>\n",
       "      <td>548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>train</td>\n",
       "      <td>57</td>\n",
       "      <td>101230332</td>\n",
       "      <td>40593363</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       table_id  cols       rows        kb\n",
       "0  content_tags    14      13941      1141\n",
       "1      lectures     4        418        15\n",
       "2     questions     5      13523       548\n",
       "3         train    57  101230332  40593363"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# <hide-input>\n",
    "df_table_list = bqh.get_df_table_list()\n",
    "df_table_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sgTZkanIjuQk"
   },
   "source": [
    "### Update Table Schemas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_t7T4NftjuQk"
   },
   "outputs": [],
   "source": [
    "# <hide-input>\n",
    "if False:\n",
    "    for table_id, schema in schemas.items():\n",
    "        table = bqh.get_table(table_id)\n",
    "        table.schema = schema\n",
    "        table = bq_client.update_table(table, ['schema'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ouoo1oQCjuQm"
   },
   "source": [
    "## Engineer Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CU8m9kx9juQn"
   },
   "source": [
    "A good workflow here is:\n",
    "* Create a sample of the train table.\n",
    "* Use the BigQuery query editor user interface to get the SQL for a new feature worked out as a selection from the `train_sample` table. The user interface there has tab completion, syntax checking and displays results, which makes creating and debugging queries a snap.\n",
    "    * [BigQuery Console](https://console.cloud.google.com/bigquery?project=riiid-caleb) (Update project query string for your project.)\n",
    "    * [BigQuery Query syntax in Standard SQL](https://cloud.google.com/bigquery/docs/reference/standard-sql/query-syntax) is your friend.\n",
    "* Optional: create a local dataframe, using the export functions below, to confirm that it is working the right way.\n",
    "* Add a column to the appropriate table by adding a value to `dtypes_new`\n",
    "* Update the schema for the table in BigQuery by running the Update Table Schemas cell above\n",
    "* Recreate the `train_sample` table by running the cell below.\n",
    "* Use the BigQuery query editor user interface add the logic to update the new column.\n",
    "* Optional: create a local dataframe, using the export functions below, to confirm that the update is working the right way.\n",
    "* Copy the SQL to a new method in the `Queries` class above\n",
    "* Add the query to the appropriate `run_transformations` function above\n",
    "* Run transformations on `train_sample` table\n",
    "* Inspect `train_sample` table in BigQuery to confirm everything is working correctly\n",
    "* Optional: load load local dataframe using `get_df_query` function for further inspection\n",
    "* Run transformations on `train` table\n",
    "* Inspect `train` table in BigQuery to confirm everything is working correctly\n",
    "* Optional: load local dataframe using `get_df_query` function for further inspection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CfKY_WsvjuQp"
   },
   "source": [
    "### Perform Transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZOnBP6sfjuQp"
   },
   "source": [
    "#### Train Table\n",
    "* Update task_container_id to increase monotonically with timestamp\n",
    "    * There were some `task_conatiner_id`s that were out of order with respect to timestamp. They needed to be be ordered correctly so that cumulative and rolling sums partitioned by `task_container_id` would be include only interactions with earlier `timestamps`. Even though all interactions with the same `task_container_id` have the same `timestamp`, partioning by `timestamp` is much slower (because the range of values is so much wider?).\n",
    "* Calc answered_incorrectly\n",
    "    * `answered_correctly` for lectures was recorded as -1 and needed to be set to 0 to calculate cumulative and rolling sums correctly including lectures. As a consequence, `answered_incorrectly` could be calculated as the inverse of `answered_correctly`.\n",
    "* Calc cumsum for `answered_correctly` and `answered_incorrectly` by `user_id` and by `user_id` and `content_id` and rolling avg for `prior_question_elapsed_time` by user \n",
    "    * This is done so that the totals are as of the preceding `task_container_id`\n",
    "* Calculate rolling sum for `answered_correctly` and `answered_incorrectly` by `user_id`\n",
    "    * Includes the 10 rows preceding the current `task_container_id`\n",
    "    * I couldn't figure out how to get this done with the standard window functionality since I wanted a set number of rows preceding the current task container (as opposed to just the current row), so it joins on `user_id` with a `task_container_id` less than the current one, which takes a while to complete.\n",
    "* Calculate answered correctly percentages for `answered_correctly_cumsum`, `answered_correctly_rollsum` and `answered_correctly_content_id_cumsum_pct`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p56xi-SBW8-N"
   },
   "outputs": [],
   "source": [
    "if False:\n",
    "    q = Q.update_content_tags_part_tag_correct_pct()\n",
    "    qj = bqh.run_query(*q, wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "r2LZdBETjuQs"
   },
   "outputs": [],
   "source": [
    "# <hide-input>\n",
    "def run_train_transforms(table_id=None):\n",
    "    # Run serially to avoid update conflicts\n",
    "    \n",
    "    train_queries = [\n",
    "#         Q.update_ql_id(table_id=table_id),\n",
    "#         Q.update_task_container_id(table_id=table_id),\n",
    "#         Q.update_missing_values(table_id=table_id,\n",
    "#                                 column_id='prior_question_had_explanation',\n",
    "#                                 value='false'),\n",
    "#         Q.update_missing_values(table_id=table_id,\n",
    "#                                 column_id='prior_question_elapsed_time',\n",
    "#                                 value='0'),\n",
    "#         Q.update_answered_correctly(table_id=table_id),\n",
    "        Q.update_train_window_containers_pqet(table_id=table_id),\n",
    "        Q.update_train_window_containers_session(table_id=table_id, session_hours=72),\n",
    "        Q.update_train_window_containers(table_id=table_id),\n",
    "        Q.update_train_window_containers_tag_0(table_id=table_id),\n",
    "        Q.update_train_window_containers_tags(table_id=table_id),\n",
    "        Q.update_answered_correctly_cumsum_upto(table_id=table_id)\n",
    "    ]\n",
    "\n",
    "    train_queries_not_run = [\n",
    "        Q.update_train_window_rows(table_id=table_id, window=10)\n",
    "    ]\n",
    "    \n",
    "    _ = [bqh.run_query(*q, wait=True) for q in train_queries]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2JEU6G0hMLkP"
   },
   "source": [
    "#### Questions Table\n",
    "* Calculate percent answered correctly for `question_id`, `part` and `tag__0`\n",
    "* Add question columns\n",
    "    * Adding question part and the first associated tag. (There wasn't any official information regarding the order of the tags as recorded for each question, but they did not appear to be sorted so it seems possible the order in which they are recorded is significant.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "jG596OjGtm5Y"
   },
   "outputs": [],
   "source": [
    "# <hide-input>\n",
    "def run_questions_transforms_before():\n",
    "    \"\"\"These have to be run BEFORE the transforms are run on the full\n",
    "    train table.\n",
    "    \"\"\"\n",
    "    \n",
    "    questions_queries = [Q.update_missing_values('questions', 'tags', '\"189\"'),\n",
    "                         Q.create_table_content_tags()]\n",
    "    \n",
    "    _ = [bqh.run_query(*q, wait=True).result() for q in questions_queries]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "8Ay24JcgjuQv"
   },
   "outputs": [],
   "source": [
    "# <hide-input>\n",
    "def run_questions_transforms_after():\n",
    "    \"\"\"These have to be run AFTER the transforms are run on the full\n",
    "    train table.\n",
    "    \"\"\"\n",
    "    questions_queries = []\n",
    "    for column_id in ['question_id', 'part', 'tag_0', 'tags']:\n",
    "        questions_queries.append(Q.update_content_tags_correct_pct(column_id))\n",
    "        questions_queries.append(Q.update_content_tags_pqet_avg(column_id))\n",
    "    \n",
    "    _ = [bqh.run_query(*q, wait=True).result() for q in questions_queries]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-P_wUT-90IHi"
   },
   "source": [
    "### Run Transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "tuh_k3VfjuQx"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job update_missing_values_202fc75e-fe85-40cc-91b7-8c8a56884774 started.\n",
      "Job update_missing_values_202fc75e-fe85-40cc-91b7-8c8a56884774 finished in 0.885 seconds.\n",
      "Job create_table_content_tags_c200fd28-7814-47ac-95c4-56fbd725db0f started.\n",
      "Job create_table_content_tags_c200fd28-7814-47ac-95c4-56fbd725db0f finished in 35.656 seconds.\n",
      "Job update_content_tags_correct_pct_d6fc4024-d0db-41a2-b7a4-dd6b11f9abfc started.\n",
      "Job update_content_tags_correct_pct_d6fc4024-d0db-41a2-b7a4-dd6b11f9abfc finished in 2.731 seconds.\n",
      "Job update_content_tags_pqet_avg_e0e8756c-26c7-4f06-8f75-4a45a9bccec1 started.\n",
      "Job update_content_tags_pqet_avg_e0e8756c-26c7-4f06-8f75-4a45a9bccec1 finished in 10.66 seconds.\n",
      "Job update_content_tags_correct_pct_4a41a870-1d41-4b78-b460-3ed496953e8d started.\n",
      "Job update_content_tags_correct_pct_4a41a870-1d41-4b78-b460-3ed496953e8d finished in 2.427 seconds.\n",
      "Job update_content_tags_pqet_avg_7cad9b9b-d3a3-46f4-8259-467a18711f5d started.\n",
      "Job update_content_tags_pqet_avg_7cad9b9b-d3a3-46f4-8259-467a18711f5d finished in 10.544 seconds.\n",
      "Job update_content_tags_correct_pct_0c7272ab-d880-41a1-8acf-ab8c6d52ed89 started.\n",
      "Job update_content_tags_correct_pct_0c7272ab-d880-41a1-8acf-ab8c6d52ed89 finished in 1.936 seconds.\n",
      "Job update_content_tags_pqet_avg_716eb1f2-99e4-4d41-bbec-f4a5c0c4a0e8 started.\n",
      "Job update_content_tags_pqet_avg_716eb1f2-99e4-4d41-bbec-f4a5c0c4a0e8 finished in 9.999 seconds.\n",
      "Job update_content_tags_correct_pct_11ac276d-075f-47d3-bad6-ebddb709ca92 started.\n",
      "Job update_content_tags_correct_pct_11ac276d-075f-47d3-bad6-ebddb709ca92 finished in 2.099 seconds.\n",
      "Job update_content_tags_pqet_avg_49f58075-0cd7-4023-86a2-814d3f6237a2 started.\n",
      "Job update_content_tags_pqet_avg_49f58075-0cd7-4023-86a2-814d3f6237a2 finished in 11.168 seconds.\n"
     ]
    }
   ],
   "source": [
    "# <hide-input>\n",
    "if True:\n",
    "    run_questions_transforms_before()\n",
    "#     run_train_transforms('train')\n",
    "    run_questions_transforms_after()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SiG8eLze0kEM"
   },
   "source": [
    "### Create One-Hots Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xgfCVpAB0qSt"
   },
   "outputs": [],
   "source": [
    "if False:\n",
    "    bqh.del_table('one_hots')\n",
    "    df_ct = bqh.get_df_query_gcs(('select * from data.content_tags', '_q_'), dtypes=dtypes, file_format='json')\n",
    "\n",
    "    mlb = MultiLabelBinarizer()\n",
    "    one_hots = (mlb.fit_transform(df_ct.tags_array\n",
    "        .apply(lambda l: [f'tag_{int(t):03d}' for t in eval(l)])))\n",
    "    df_one_hots = pd.DataFrame(one_hots, columns=mlb.classes_)\n",
    "    df_one_hots['ql_id'] = df_ct.ql_id\n",
    "    df_one_hots['part'] = df_ct.part\n",
    "    df_one_hots.ql_id = df_one_hots.ql_id.astype('int16')\n",
    "    df_one_hots = pd.get_dummies(df_one_hots, columns=['part'])\n",
    "\n",
    "    for c in [c for c in df_one_hots.columns if c != 'ql_id']:\n",
    "        dtypes[c] = 'int8'\n",
    "    \n",
    "    schemas['one_hots'] = [SchemaField(c, 'INTEGER', 'NULLABLE', None, ())\n",
    "                            for c in df_one_hots.columns]\n",
    "\n",
    "    df_one_hots.to_json('one_hots.json', orient=\"records\", lines=True)\n",
    "    lj = bqh.load_json_file('one_hots', schemas).result()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YTG_h2a2juQy"
   },
   "source": [
    "### Check Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "12-1u5TdjuQ1"
   },
   "outputs": [],
   "source": [
    "query = Q.select_train(table_id='train', excl_lectures=True)\n",
    "df_query = bqh.get_df_query(query, dtypes=dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ub-U0nPgjuQ3"
   },
   "outputs": [],
   "source": [
    "# <hide-input>\n",
    "cols = [\n",
    "        'row_id',\n",
    "        'task_container_id_orig',\n",
    "        'timestamp',\n",
    "        'content_type_id',\n",
    "        'user_id',\n",
    "        'task_container_id',\n",
    "        'part',\n",
    "        'tag__0',\n",
    "        'answered_correctly',\n",
    "        'answered_incorrectly',\n",
    "        'answered_correctly_cumsum',\n",
    "        'answered_incorrectly_cumsum',\n",
    "        'answered_correctly_content_id_cumsum',\n",
    "        'answered_correctly_rollsum',\n",
    "        'answered_incorrectly_rollsum',\n",
    "        'answered_incorrectly_content_id_cumsum',\n",
    "        'part_correct_pct',\n",
    "        'tag__0_correct_pct',\n",
    "        'question_id_correct_pct',\n",
    "        'prior_question_elapsed_time',\n",
    "        'prior_question_elapsed_time_rollavg',\n",
    "        'prior_question_had_explanation',\n",
    "        'lectures_cumcount',\n",
    "        'answered_correctly_cumsum_upto'\n",
    "]\n",
    "\n",
    "df_user = df_query[cols].copy()\n",
    "df_user.timestamp = df_user.timestamp / (1000*60*60)\n",
    "\n",
    "df_user.loc[df_user.user_id == 44331].head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zO6GM_CLjuQ5"
   },
   "source": [
    "### Visually Inspect Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jpokxssbjuQ6"
   },
   "source": [
    "The charts below can also be used to visually inspect whether the transformations have been performed correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0c-xlk2TjuQ6"
   },
   "outputs": [],
   "source": [
    "# <hide-input>\n",
    "groups = {\n",
    "    'cum': {\n",
    "        'columns': {\n",
    "            'task_container_id': 0,\n",
    "            'answered_correctly_cumsum': 2,\n",
    "            'answered_incorrectly_cumsum': 1\n",
    "        },\n",
    "        'xaxis': 'elapsed_hours'\n",
    "    },\n",
    "    'roll': {\n",
    "        'columns': {\n",
    "            'answered_correctly_rollsum': 2,\n",
    "            'answered_correctly': 7,\n",
    "            'answered_incorrectly_rollsum': 1,\n",
    "            'answered_incorrectly': 8,\n",
    "            'part': 9\n",
    "        },\n",
    "        'xaxis': 'row_id'\n",
    "    },  \n",
    "    'correct_pct': {\n",
    "        'columns': {\n",
    "            'question_id_correct_pct': 0,\n",
    "            'part_correct_pct': 5,\n",
    "            'tag__0_correct_pct': 6\n",
    "        },\n",
    "        'xaxis': 'row_id'\n",
    "    },  \n",
    "    'prior_question_elapsed_time': {\n",
    "        'columns': {\n",
    "            'prior_question_elapsed_time': 0,\n",
    "        },\n",
    "        'xaxis': 'row_id'\n",
    "    },  \n",
    "    'prior_question_had_explanation': {\n",
    "        'columns': {\n",
    "            'prior_question_had_explanation': 0,\n",
    "        },\n",
    "        'xaxis': 'row_id'\n",
    "    }\n",
    "}\n",
    "\n",
    "def plot_user_learning(user_id=None, group=None, suffix=None):\n",
    "    theme = px.colors.qualitative.Plotly\n",
    "    columns = list(group['columns'].keys())\n",
    "    colors = [theme[c] for c in group['columns'].values()]\n",
    "\n",
    "    df_query['elapsed_hours'] = df_query.timestamp / (1000*60*60)\n",
    "\n",
    "    df = (df_query.loc[(df_user.user_id == user_id) &\n",
    "                       (df_user.content_type_id == 0)])\n",
    "\n",
    "    # labels = {'value': 'answer count'}\n",
    "\n",
    "    fig = df.plot(x=group['xaxis'], y=columns, color_discrete_sequence=colors,\n",
    "                  title=f'Learning Progress - user_id = {user_id} - {suffix}')\n",
    "    fig.data\n",
    "\n",
    "    return fig\n",
    "\n",
    "user_id_random = np.random.choice(df_query.user_id.unique(), (1,))[0]\n",
    "use_random = False\n",
    "user_id =  user_id_random if use_random else 5382\n",
    "\n",
    "for k, v in groups.items():\n",
    "    fig = plot_user_learning(user_id, v, k)\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Axb2oD77juRM"
   },
   "source": [
    "### Create Folds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jNsk1_JsjuRN"
   },
   "source": [
    "The objectives for the validation split are as follows:\n",
    "* Include some users that don't exist in the training set\n",
    "* Include records for some users in the validation set with timestamps greater than all of those users' records in the training set\n",
    "\n",
    "This is achieved by first setting the percentage of users that will only occur in the training set and then setting the percentage of the remaining users that will have records in both.\n",
    "\n",
    "The split between the validation and training sets for the users with records in both is determined by randomly selecting a number between zero and the maximum task_container_id for each user and including in the validation set all records with a task_container_id greater than that for each user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A_nmEi_1gMFf"
   },
   "outputs": [],
   "source": [
    "# <hide-input>\n",
    "# create validation split table\n",
    "# this needs to run after the task container Ids have been updated\n",
    "\n",
    "table_id_folds = 'folds'\n",
    "if True:\n",
    "    qj = bqh.run_query(*Q.create_table_folds(table_id=table_id_folds), wait=True)\n",
    "    qj = bqh.run_query(*Q.update_folds(table_id_folds=table_id_folds), wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PX-J3AaG5FxM"
   },
   "outputs": [],
   "source": [
    "# <hide-input>\n",
    "# TODO: bring in diagnostic charts from folding is fun\n",
    "if False:\n",
    "    df_folds = bqh.get_df_table('folds', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7GBDKOZ2juRA"
   },
   "source": [
    "With feature engineering being performed in BigQuery, data has to be exported to train models locally. The [Python Client for Google BigQuery](https://googleapis.dev/python/bigquery/latest/index.html) [to_dataframe()](https://googleapis.dev/python/bigquery/latest/generated/google.cloud.bigquery.job.QueryJob.html?highlight=to_dataframe#google.cloud.bigquery.job.QueryJob.to_dataframe) makes it possible to create dataframes directly, but is prohibitively slow for large datasets. While it is not possible to export table directly to the local file system, it is possible to export to cloud storage and then download locally from there. This is reasonably efficient, taking a couple of minutes to run a query, export to cloud storage, download to the local file system and then read the files into a dataframe. The is another api, the [BigQuery Storage API](https://cloud.google.com/bigquery/docs/reference/storage), that a client can be created with that is really fast and works with the `to_dataframe` method, but unforunatley it isn't working with the current Kaggle kernel environment.\n",
    "\n",
    "The functions below take advantage of the fact BigQuery stores queries in temporary tables so that preveiously requested queries can be retrieved without having to run them again. Similarly, the functions below name the exported files with the reference to the BigQuery temporary table, so that if a function is run to create a dataframe from a query for which the files already exist in cloud storage or locally, they won't be exported or downloaded again. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F_B3C-9QjuQ9"
   },
   "source": [
    "### Create Sample of Train Table for R&D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tuNFCT1OjuQ9"
   },
   "outputs": [],
   "source": [
    "# <hide-input>\n",
    "if True:\n",
    "    ts_id = 'train_sample'\n",
    "    bqh.del_table(ts_id)\n",
    "    bqh.run_query(*Q.create_train_sample(ts_id), wait=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wbaxOZFVjuRA"
   },
   "source": [
    "## Create Local Dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oMqolxVmjuRL"
   },
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xV-qHbTc2oly"
   },
   "source": [
    "### Select Features to Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rnDz1XqY5g-n"
   },
   "outputs": [],
   "source": [
    "# <hide-input><hide-output>\n",
    "if False:\n",
    "    feats = {d: [True, True] for d in dtypes}\n",
    "    # for f in features:\n",
    "    #     feats[f] = features[f]\n",
    "    \n",
    "    # for f in exp_cols_new:\n",
    "    #     feats[f] = [True, True]\n",
    "    \n",
    "    feats = sorted([f\"\"\"'{k+\"':\":<32} {v},\"\"\" for k,v in feats.items()])\n",
    "\n",
    "    for f in feats:\n",
    "        print(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QUkacRjajuRG"
   },
   "outputs": [],
   "source": [
    "# <hide-input>\n",
    "features = {\n",
    "'ac_cumsum':                      [True, True],\n",
    "'ac_cumsum_content_id':           [True, True],\n",
    "'ac_cumsum_part':                 [True, True],\n",
    "'ac_cumsum_pct':                  [True, True],\n",
    "'ac_cumsum_pct_content_id':       [True, True],\n",
    "'ac_cumsum_pct_part':             [True, True],\n",
    "'ac_cumsum_pct_session':          [True, True],\n",
    "'ac_cumsum_pct_tag_0':            [True, True],\n",
    "'ac_cumsum_pct_tags':             [True, True],\n",
    "'ac_cumsum_pct_upto':             [True, True],\n",
    "'ac_cumsum_session':              [True, True],\n",
    "'ac_cumsum_tag_0':                [True, True],\n",
    "'ac_cumsum_tags':                 [True, True],\n",
    "'ac_cumsum_upto':                 [True, True],\n",
    "'ac_rollsum_pct_session':         [True, False],\n",
    "'ac_rollsum_session':             [True, False],\n",
    "'aic_cumsum':                     [True, True],\n",
    "'aic_cumsum_content_id':          [True, True],\n",
    "'aic_cumsum_part':                [True, True],\n",
    "'aic_cumsum_session':             [True, True],\n",
    "'aic_cumsum_tag_0':               [True, True],\n",
    "'aic_cumsum_tags':                [True, True],\n",
    "'aic_cumsum_upto':                [True, True],\n",
    "'aic_rollsum_session':            [True, False],\n",
    "'answered_correctly':             [True, False],\n",
    "'bundle_id':                      [False, False],\n",
    "'content_id':                     [True, False],\n",
    "'content_type_id':                [True, False],\n",
    "'correct_answer':                 [False, False],\n",
    "'fold':                           [True, False],\n",
    "'lecture_id':                     [False, False],\n",
    "'lectures_cumcnt':                [True, True],\n",
    "'lectures_cumcnt_part':           [True, True],\n",
    "'lectures_cumcnt_session':        [True, True],\n",
    "'lectures_cumcnt_tag_0':          [True, True],\n",
    "'lectures_cumcnt_tags':           [True, True],\n",
    "'lectures_rollcnt_session':       [True, False],\n",
    "'part':                           [True, True],\n",
    "'part_correct_pct':               [True, True],\n",
    "'part_pqet_avg':                  [True, True],\n",
    "'pqet_sec':                       [True, True],\n",
    "'pqet_sec_rollavg':               [True, False],\n",
    "'prior_question_elapsed_time':    [True, True],\n",
    "'prior_question_had_explanation': [True, True],\n",
    "'ql_id':                          [False, False],\n",
    "'question_id':                    [True, False],\n",
    "'question_id_correct_pct':        [True, True],\n",
    "'question_id_pqet_avg':           [True, True],\n",
    "'r_cumcnt':                       [True, True],\n",
    "'r_cumcnt_clip':                  [True, True],\n",
    "'r_cumcnt_content_id':            [True, True],\n",
    "'r_cumcnt_part':                  [True, True],\n",
    "'r_cumcnt_session':               [True, True],\n",
    "'r_cumcnt_tag_0':                 [True, True],\n",
    "'r_cumcnt_tags':                  [True, True],\n",
    "'r_cumcnt_upto':                  [True, True],\n",
    "'r_rollcnt_session':              [True, False],\n",
    "'row_id':                         [True, False],\n",
    "'session':                        [True, True],\n",
    "'session_minute_max':             [False, False],\n",
    "'tag_0':                          [True, True],\n",
    "'tag_0_correct_pct':              [True, True],\n",
    "'tag_0_part_correct_pct':         [True, True],\n",
    "'tag_0_pqet_avg':                 [True, True],\n",
    "'tags':                           [True, False],\n",
    "'tags_array':                     [False, False],\n",
    "'tags_code':                      [True, True],\n",
    "'tags_correct_pct':               [True, True],\n",
    "'tags_pqet_avg':                  [True, True],\n",
    "'task_container_id':              [True, False],\n",
    "'task_container_id_orig':         [False, False],\n",
    "'timestamp':                      [True, True],\n",
    "'ts_minute':                      [True, True],\n",
    "'user_answer':                    [False, False],\n",
    "'user_id':                        [True, False]\n",
    " }\n",
    "\n",
    "columns_export = [f for f, v in features.items() if v[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LFvJDKLI9ttR"
   },
   "outputs": [],
   "source": [
    "if False:\n",
    "    df_one_hots = pd.read_gbq('select * from data.one_hots').set_index('ql_id')\n",
    "    for c in df_one_hots.columns:\n",
    "        features[c] = [True, True]\n",
    "        dtypes[c] = 'int8'\n",
    "    features['tag_0'] = [True, False]\n",
    "    features['part'] = [True, False]\n",
    "    features['tags_code'] = [True, False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "32Wt1DvjpnJZ"
   },
   "outputs": [],
   "source": [
    "# <hide-input>\n",
    "def get_features_widget(features_dict, columns_list, idx):\n",
    "\n",
    "    names = []\n",
    "    widget_list = []\n",
    "    for key, v in features_dict.items():\n",
    "        widget_list.append(widgets.ToggleButton(value=v[idx],\n",
    "                                                description=key,\n",
    "                                                layout={'width': '290px'},\n",
    "                                                button_style='primary'))\n",
    "        names.append(key)\n",
    "\n",
    "    arg_dict = {names[i]: widget for i, widget in enumerate(widget_list)}\n",
    "\n",
    "    layout = widgets.Layout(grid_template_columns=\"repeat(3, 300px)\")\n",
    "    ui = widgets.GridBox(widget_list, layout=layout)\n",
    "\n",
    "    def select_data(**kwargs):\n",
    "        columns_list.clear()\n",
    "\n",
    "        for key in kwargs:\n",
    "            features_dict[key][idx] = False\n",
    "            if kwargs[key]:\n",
    "                columns_list.append(key)\n",
    "                features_dict[key][idx] = True\n",
    "\n",
    "        print(f'{len(columns_list)} columns selected')\n",
    "\n",
    "    output = widgets.interactive_output(select_data, arg_dict)\n",
    "    return ui, output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B6ddqQ4HBT3R"
   },
   "outputs": [],
   "source": [
    "# <hide-input>\n",
    "display(*get_features_widget(features, columns_export, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EXRXQ86UjuRK"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# <hide-output>\n",
    "\n",
    "folds = list(range(5))\n",
    "if False:\n",
    "    query = Q.select_train(columns=columns_export, folds=folds,\n",
    "                           excl_lectures=True, limit=None)\n",
    "    # df_train = bqh.get_df_query_gcs(query, dtypes=dtypes, file_format='json')\n",
    "    df_train = (pd.read_gbq(query[0], use_bqstorage_api=True,\n",
    "                            progress_bar_type='tqdm_notebook'))\n",
    "                            .astype({c: dtypes[c] for c in columns_export}))\n",
    "    \n",
    "    df_train.to_pickle('df_train.pkl')\n",
    "    bucket.blob('df_train.pkl').upload_from_filename('df_train.pkl')\n",
    "\n",
    "else:\n",
    "    if not Path('df_train.pkl').exists():\n",
    "        bucket.blob('df_train.pkl').download_to_filename('df_train.pkl')\n",
    "    df_train = pd.read_pickle(f'df_train.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nnwa3B8jiKV0"
   },
   "source": [
    "## Collaborative Filtering Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T-_ypfopXB96"
   },
   "outputs": [],
   "source": [
    "cat_cols = ['user_id', 'content_id']\n",
    "\n",
    "weights_fn = 'weights_all.npy'\n",
    "bucket.blob(weights_fn).download_to_filename(weights_fn)\n",
    "weights = np.load(weights_fn, allow_pickle=True).item()\n",
    "\n",
    "for c in cat_cols:\n",
    "    embed_key = f'{c}_embedding'\n",
    "    weights[embed_key] = np.reshape(weights[embed_key], (len(weights[c]) + 1, -1))\n",
    "\n",
    "weights.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Kt7rKoBnnEXO"
   },
   "outputs": [],
   "source": [
    "def get_preds(df, weights, logits=True):\n",
    "    user_codes, content_codes = [pd.Categorical(df[c], categories=weights[c]).codes  for c in cat_cols]\n",
    "    user_col, content_col = cat_cols\n",
    "    logit = np.sum(weights[f'{user_col}_embedding'][user_codes] * weights[f'{content_col}_embedding'][content_codes], axis=1)\n",
    "    logit += weights[f'{user_col}_bias'][user_codes] + weights[f'{content_col}_bias'][content_codes]\n",
    "    \n",
    "    if logits:\n",
    "        return logit\n",
    "    else:\n",
    "        return 1 / (1 + np.exp(-logit))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M96Uut_hsQHV"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "preds = get_preds(df_train, weights, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kkMduw8hmno4"
   },
   "outputs": [],
   "source": [
    "roc_auc_score(df_train.answered_correctly, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SvkPeuVrsnHL"
   },
   "outputs": [],
   "source": [
    "df_train['pred_collab_logit'] = preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kN26v5z2t_hG"
   },
   "source": [
    "### Add Max Task Container Id from Folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0Z9i0km5JhKj"
   },
   "outputs": [],
   "source": [
    "# add on max task_container_id to filter out smaller number of containers\n",
    "if False:\n",
    "    df_folds = pd.read_gbq('select * from data.folds', use_bqstorage_api=True, progress_bar_type='tqdm_notebook')\n",
    "    df_train = df_train.merge(df_folds[['user_id_s', 'task_container_id_max']], how='left', left_on='user_id', right_on='user_id_s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SI-MVAI4uJ8w"
   },
   "source": [
    "### Inspect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TAIxa0UkIZCF"
   },
   "outputs": [],
   "source": [
    "if False:\n",
    "    \n",
    "    base_cols = [\n",
    "            'user_id',\n",
    "            'task_container_id',\n",
    "            'row_id',\n",
    "            'content_id',\n",
    "            'content_type_id',\n",
    "            'prior_question_had_explanation',\n",
    "            'answered_correctly',\n",
    "    ]\n",
    "    \n",
    "    col = 'session'\n",
    "    \n",
    "    cum_cols = [\n",
    "            'ac_cumsum',\n",
    "            'aic_cumsum',\n",
    "            'r_cumcnt',\n",
    "            'r_cumcnt_clip',\n",
    "            'ac_cumsum_pct',\n",
    "            'lectures_cumcnt',\n",
    "            f'ac_cumsum_{col}',\n",
    "            f'aic_cumsum_{col}',\n",
    "            f'r_cumcnt_{col}',\n",
    "            f'ac_cumsum_pct_{col}',\n",
    "            f'lectures_cumcnt_{col}'\n",
    "    ]\n",
    "\n",
    "    rollsum_session_cols = [\n",
    "            'session',\n",
    "            f'ac_rollsum_{col}',\n",
    "            f'aic_rollsum_{col}',\n",
    "            f'r_rollcnt_{col}',\n",
    "            f'ac_rollsum_pct_{col}',\n",
    "            f'lectures_rollcnt_{col}'\n",
    "    ]\n",
    "\n",
    "    q_cols = [\n",
    "              'part',\n",
    "              'tags',\n",
    "              'tag_0',\n",
    "              'question_id_correct_pct',\n",
    "              'part_correct_pct',\n",
    "              'tag_0_correct_pct',\n",
    "              'tags_correct_pct',\n",
    "              'question_id_pqet_avg',\n",
    "              'part_pqet_avg',\n",
    "              'tag_0_pqet_avg',\n",
    "              'tags_pqet_avg'\n",
    "    ]\n",
    "\n",
    "    time_cols = [\n",
    "            'prior_question_elapsed_time',\n",
    "            'timestamp',\n",
    "            'ts_minute',\n",
    "            'session_minute_max',\n",
    "            'pqet_sec',\n",
    "            'pqet_sec_rollavg',\n",
    "            'session'\n",
    "    ]\n",
    "\n",
    "    df_test = bqh.get_df_query_gcs(('SELECT * FROM data.train t JOIN data.content_tags c ON t.ql_id = c.ql_id WHERE user_id IN (115, 124, 1827855198, 1066383521) ORDER BY user_id, task_container_id, row_id', '_test'), dtypes=None, file_format='json')\n",
    "\n",
    "    df_test[base_cols + q_cols][df_test.user_id == 1827855198].tail(60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8VM6n9FKyczN"
   },
   "source": [
    "### Select Columns for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MUSDoZoLEKP_"
   },
   "outputs": [],
   "source": [
    "# <hide-input>\n",
    "columns_train = []\n",
    "display(*get_features_widget(features, columns_train, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "moGm9Vdw9Tui"
   },
   "outputs": [],
   "source": [
    "exp_cols_new = [\n",
    "    'ac_cumsum_content_id',\n",
    "    'aic_cumsum_content_id',\n",
    "    'ac_cumsum_pct_content_id',\n",
    "    'ac_cumsum',\n",
    "    'aic_cumsum',\n",
    "    'ac_cumsum_pct',\n",
    "    'ac_cumsum_tag_0',\n",
    "    'aic_cumsum_tag_0',\n",
    "    'ac_cumsum_pct_tag_0',\n",
    "    'ac_cumsum_tags',\n",
    "    'aic_cumsum_tags',\n",
    "    'ac_cumsum_pct_tags',\n",
    "    'part',\n",
    "    'part_correct_pct',\n",
    "    'prior_question_elapsed_time',\n",
    "    'prior_question_had_explanation',\n",
    "    'question_id_correct_pct',\n",
    "    'tag_0',\n",
    "    'tag_0_correct_pct',\n",
    "    'tags_correct_pct',\n",
    "    'tags_code',\n",
    "    'task_container_id',\n",
    "    'timestamp'\n",
    " ]\n",
    "\n",
    "len(exp_cols_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zfBtnf3TsyS_"
   },
   "outputs": [],
   "source": [
    "columns_train.append('pred_collab_logit')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q-zrGOfDwjw9"
   },
   "outputs": [],
   "source": [
    "# <hide-input>\n",
    "def show_features():\n",
    "    df_features = pd.DataFrame(features).T.reset_index()\n",
    "    df_features.columns = ['feature', 'export', 'train']\n",
    "    df_features\n",
    "\n",
    "    def highlight_true(s):\n",
    "        return ['background-color: lightskyblue' if v else '' for v in s]\n",
    "    return df_features.style.apply(highlight_true, subset=['export', 'train'])\n",
    "show_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "reut9PmcjuRV"
   },
   "outputs": [],
   "source": [
    "# <hide-input>\n",
    "def get_dmatrices(folds_train=None, folds_val=None, x_train_cols=None, matrix=xgb.DMatrix):\n",
    "    y_train_col = ['answered_correctly']\n",
    "\n",
    "    # mask_train = (df_train.fold.isin(folds_train) * df_train.task_container_id_max > 10)\n",
    "    \n",
    "    mask_train = df_train.fold.isin(folds_train)\n",
    "    mask_valid = df_train.fold.isin(folds_val)\n",
    "\n",
    "    train_matrix = matrix(data=df_train.loc[mask_train][x_train_cols],\n",
    "                            label=df_train.loc[mask_train][y_train_col])\n",
    "\n",
    "    valid_matrix = matrix(data=df_train.loc[mask_valid][x_train_cols],\n",
    "                            label=df_train.loc[mask_valid][y_train_col])\n",
    "    \n",
    "    return {'train_matrix': train_matrix,\n",
    "            'valid_matrix': valid_matrix,\n",
    "            'folds_train': folds_train,\n",
    "            'folds_val': folds_val\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a05rC3CdDYg0"
   },
   "outputs": [],
   "source": [
    "folds_dict = {}\n",
    "if False:\n",
    "    folds = list(range(30))\n",
    "    \n",
    "    for fold in folds:\n",
    "        query = Q.select_train(columns=['answered_correctly'] + columns_train, folds=[fold],\n",
    "                            excl_lectures=True, limit=None)\n",
    "        \n",
    "        prefix = bqh.export_query_gcs(query, header=False)\n",
    "        file_paths = bqh.get_table_gcs(prefix)\n",
    "        folds_dict[fold] = dict(prefix=prefix,file_paths=file_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Nluvlo-SGun9"
   },
   "outputs": [],
   "source": [
    "def get_dmatrices_csv(folds_train=None, folds_val=None):\n",
    "    \n",
    "    def combine_folds(folds, cache_fp=None):\n",
    "        if cache_fp is not None and Path(cache_fp).exists():\n",
    "            Path(cache_fp).unlink()\n",
    "            Path(f'{cache_fp}.row.page').unlink()\n",
    "            \n",
    "        fp_combined = Path('combined.csv')\n",
    "        if fp_combined.exists():\n",
    "            fp_combined.unlink()\n",
    "        \n",
    "        with open(fp_combined,\"wb\") as fout:\n",
    "            for fold in tqdm(folds):\n",
    "                for fp in folds_dict[fold]['file_paths']:\n",
    "                    with open(fp, \"rb\") as fin:\n",
    "                        fout.write(fin.read())\n",
    "\n",
    "        uri = f'{fp_combined.name}?format=csv&label_column=0'\n",
    "\n",
    "        if cache_fp is not None:\n",
    "            uri += f'#{cache_fp}'\n",
    "        \n",
    "        return xgb.DMatrix(uri, feature_names=columns_train)\n",
    "        \n",
    "    return {'train_matrix': combine_folds(folds_train, 'train.cache'),\n",
    "            'valid_matrix': combine_folds(folds_val, 'valid.cache'),\n",
    "            'folds_train': folds_train,\n",
    "            'folds_val': folds_val\n",
    "            }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E5gUTzNtjuRX"
   },
   "source": [
    "### Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t6AeR8lBjuRX"
   },
   "outputs": [],
   "source": [
    "# <hide-output>\n",
    "params = {\n",
    "    'booster': 'gbtree',\n",
    "    # 'rate_drop': 0.1,\n",
    "    'eta': 0.1,\n",
    "    'max_depth': 0,\n",
    "    'max_leaves': 256,\n",
    "    'max_bin': 512,\n",
    "    # 'min_child_weight': 100,\n",
    "    'tree_method': 'gpu_hist',\n",
    "    'grow_policy': 'lossguide',\n",
    "    'objective': 'binary:logistic',\n",
    "    'eval_metric': ['logloss', 'error', 'auc'],\n",
    "    # 'subsample': 0.5,\n",
    "    # 'sampling_method': 'gradient_based',\n",
    "    'lambda': 4,\n",
    "    'alpha': 100\n",
    "}\n",
    "\n",
    "def train_model(params=params, train_matrix=None, valid_matrix=None,\n",
    "                folds_train=None, folds_val=None, verbose_eval=True, comet_summary=1):\n",
    "    if NOT_KAGGLE:\n",
    "        experiment = Experiment(display_summary_level=comet_summary)\n",
    "        # experiment.set_name('with gap_minute_avg')\n",
    "        experiment.log_parameter('folds_train', folds_train)\n",
    "        experiment.log_parameter('folds_val', folds_val)\n",
    "        # experiment.log_parameter('df_train_len', len(df_train))\n",
    "\n",
    "    evals_result = {}\n",
    "    model = xgb.train(params=params, dtrain=train_matrix, num_boost_round=1000,\n",
    "                    evals=[(train_matrix, 'train'), (valid_matrix, 'valid')],\n",
    "                    evals_result=evals_result, early_stopping_rounds=20,\n",
    "                    verbose_eval=verbose_eval)\n",
    "\n",
    "    if NOT_KAGGLE:\n",
    "        exp_key = experiment.get_key()\n",
    "        model.set_attr(feature_names = '|'.join(model.feature_names))\n",
    "        model.set_attr(feature_types = '|'.join(model.feature_types))\n",
    "        model.save_model(f'{exp_key}.xgb')\n",
    "        experiment.log_model(exp_key, f'{exp_key}.xgb')\n",
    "        experiment.end()\n",
    "        \n",
    "    return model, exp_key\n",
    "    \n",
    "exps = {}\n",
    "\n",
    "folds_list = [\n",
    "    {'folds_train': folds[1:], 'folds_val': folds[:1]}\n",
    "]\n",
    "\n",
    "# folds_list = [{'folds_train': [[f for f in folds if f != val_fold]], 'folds_val': [folds[val_fold]]} for val_fold in folds]\n",
    "\n",
    "for folds_run in folds_list:\n",
    "    model, exp_key = train_model(params, **get_dmatrices(**folds_run, x_train_cols=columns_train),\n",
    "                                 verbose_eval=True, comet_summary=1)\n",
    "    exps[exp_key] = model.attributes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NH2mHCKVzY6c"
   },
   "outputs": [],
   "source": [
    "\n",
    "np.random.shuffle(folds_array)\n",
    "folds_array[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-5X8mWN2QYN5"
   },
   "outputs": [],
   "source": [
    "folds_array = np.array(folds)\n",
    "rolds\n",
    "for n in range(f):\n",
    "[{'folds_train': [[f for f in folds if f != val_fold]], 'folds_val': [folds[val_fold]]} for val_fold in folds]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fe7JKQf0juRZ"
   },
   "source": [
    "## Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qPTKUFJMjuRZ"
   },
   "outputs": [],
   "source": [
    "# <hide-input>\n",
    "def get_evals_df(evals_result):\n",
    "    evals_list = []\n",
    "    for k,v in evals_result.items():\n",
    "        for j,u in v.items():\n",
    "            evals_list.extend([{'epoch': i,\n",
    "                                'split': k,\n",
    "                                'metric': j,\n",
    "                                'result': r} for i,r in enumerate(u)])\n",
    "    \n",
    "    df_evals = (pd.DataFrame(evals_list).set_index(['split', 'metric', 'epoch'])\n",
    "                .unstack('metric'))\n",
    "    df_evals.columns = df_evals.columns.get_level_values(1)\n",
    "    df_evals.columns.name = None\n",
    "    \n",
    "    return df_evals.reset_index()\n",
    "\n",
    "df_evals = get_evals_df(evals_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oa7h74fSjuRb"
   },
   "outputs": [],
   "source": [
    "# <hide-input>\n",
    "df_evals.plot(x='epoch', y=['auc', 'logloss'],\n",
    "              facet_col='split', title='Learning Curves')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AxlglavIYpXn"
   },
   "outputs": [],
   "source": [
    "model = xgb.Booster(model_file='127dba4ca30042328c57ec2197e813f2.xgb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EMrYRgCF4V9i"
   },
   "outputs": [],
   "source": [
    "model.feature_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Jgt5w3zHkvh2"
   },
   "outputs": [],
   "source": [
    "matrix_dict = get_dmatrices([0], [1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Gxlm82xRlCPk"
   },
   "outputs": [],
   "source": [
    "model.eval(matrix_dict['valid_matrix'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kzWs_7Azk9H3"
   },
   "outputs": [],
   "source": [
    "print(matrix_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oWnRrYLNt6TA"
   },
   "outputs": [],
   "source": [
    "df_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_hq7-PWBGt1C"
   },
   "outputs": [],
   "source": [
    "df_train.pred_collab_logit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iIK9aXVpjuRd"
   },
   "outputs": [],
   "source": [
    "# <hide-input>\n",
    "imps = model.get_score(importance_type='gain').items()\n",
    "df_imp = pd.DataFrame(imps, columns=['feature', 'importance'])\n",
    "df_imp = df_imp.set_index('feature').sort_values('importance', ascending=False)\n",
    "df_imp.plot(kind='bar', y='importance', title='Feature Importances - Gain')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Mf899tHmctmo"
   },
   "outputs": [],
   "source": [
    "if False:\n",
    "    tree = xgb.to_graphviz(model, rankdir='LR')\n",
    "    tree.save('tree')\n",
    "    tree.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xVXlSxE-t2Bt"
   },
   "outputs": [],
   "source": [
    "model_dump = model.get_dump(dump_format='json', with_stats=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AtIDcslpvY7f"
   },
   "outputs": [],
   "source": [
    "print(model_dump[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ouZNqNl9p5zI"
   },
   "outputs": [],
   "source": [
    "model.dump_model('model.json', with_stats=True, dump_format='json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MwBUgoWhI6hu"
   },
   "source": [
    "## Light GBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qbLXXiPkQAt6"
   },
   "outputs": [],
   "source": [
    "cat_cols = ['part', 'tag_0']\n",
    "\n",
    "lgbm_params = {\n",
    "    'objective': 'binary',\n",
    "    'metric': ['auc'],\n",
    "    'device': 'gpu',\n",
    "    'num_leaves': 127,\n",
    "    'min_data_in_leaf': 100,\n",
    "    'max_bin': 255,\n",
    "    'learning_rate': 0.1,\n",
    "    'boosting': 'gbdt',\n",
    "    'reg_lambda': 4,\n",
    "    'reg_alpha': 1\n",
    "}\n",
    "\n",
    "folds_run = {'folds_train': folds[0:-1], 'folds_val': folds[-1:]}\n",
    "\n",
    "matrices = get_dmatrices(folds_train=folds_run['folds_train'],\n",
    "                         folds_val=folds_run['folds_val'],\n",
    "                         matrix=lgb.Dataset)\n",
    "\n",
    "# experiment = Experiment(display_summary_level=1)\n",
    "# experiment.log_parameter('folds_train', matrices['folds_train'])\n",
    "# experiment.log_parameter('folds_val', matrices['folds_val'])\n",
    "# experiment.log_parameter('df_train_len', len(df_train))\n",
    "\n",
    "evals_result = {}\n",
    "model = lgb.train(\n",
    "            params=lgbm_params,\n",
    "            train_set= matrices['train_matrix'],\n",
    "            valid_sets=[matrices['train_matrix'], matrices['valid_matrix']],\n",
    "            valid_names=['train', 'valid'],\n",
    "            verbose_eval=True,\n",
    "            evals_result=evals_result,\n",
    "            num_boost_round=1000,\n",
    "            early_stopping_rounds=20,\n",
    "            categorical_feature=cat_cols\n",
    "    )\n",
    "\n",
    "# experiment.end()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bFq-OqTCgd4-"
   },
   "outputs": [],
   "source": [
    "pd.DataFrame({'feature': columns_train, 'gain': model.feature_importance('gain').astype('int')}).set_index('feature').sort_values('gain', ascending=False).plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Pb9rB7XX_DJw"
   },
   "outputs": [],
   "source": [
    "model.save_model('model.lgb')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t49TzqNPq5pg"
   },
   "source": [
    "## Prepare Prediction Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GXZ10jxVjuRf"
   },
   "source": [
    "### Download Final Users State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3eJlJt1FjuRg"
   },
   "outputs": [],
   "source": [
    "# <hide-input><hide-output>\n",
    "%%time\n",
    "\n",
    "if False:    \n",
    "    query = Q.select_user_final_state(table_id='train', no_upto=10)\n",
    "    df_users = bqh.get_df_query_gcs(query, dtypes=dtypes, file_format='csv')\n",
    "    df_users.to_pickle('df_users.pkl')\n",
    "    bucket.blob('df_users.pkl').upload_from_filename('df_users.pkl')\n",
    "else:\n",
    "    bucket.blob('df_users.pkl').download_to_filename('df_users.pkl')\n",
    "    df_users = pd.read_pickle('df_users.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xok_9xlD7c0H"
   },
   "source": [
    "### Download Final Users-Content State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RUVdoI5V8FD1"
   },
   "outputs": [],
   "source": [
    "# <hide-input><hide-output>\n",
    "%%time\n",
    "\n",
    "if False:\n",
    "    query = Q.select_users_content_final_state(table_id='train')\n",
    "    df_users_content = bqh.get_df_query_gcs(query, dtypes=dtypes)\n",
    "    df_users_content.to_pickle('df_users_content.pkl')\n",
    "    bucket.blob('df_users_content.pkl').upload_from_filename('df_users_content.pkl')\n",
    "else:\n",
    "    bucket.blob('df_users_content.pkl').download_to_filename('df_users_content.pkl')\n",
    "    df_users_content = pd.read_pickle(f'df_users_content.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7zfU3NaqRbte"
   },
   "source": [
    "### Download Final Users-Part State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rkeXg0r5Rd_C"
   },
   "outputs": [],
   "source": [
    "# <hide-input><hide-output>\n",
    "%%time\n",
    "\n",
    "if False:\n",
    "    query = Q.select_users_part_final_state(table_id='train')\n",
    "    df_users_part = bqh.get_df_query_gcs(query, dtypes=dtypes)\n",
    "    df_users_part.to_pickle('df_users_part.pkl')\n",
    "    bucket.blob('df_users_part.pkl').upload_from_filename('df_users_part.pkl')\n",
    "else:\n",
    "    bucket.blob('df_users_part.pkl').download_to_filename('df_users_part.pkl')\n",
    "    df_users_content = pd.read_pickle(f'df_users_part.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EVS-PapIkfPG"
   },
   "source": [
    "### Download Final User-Tags State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-v6vBK7vkdzf"
   },
   "outputs": [],
   "source": [
    "# <hide-input><hide-output>\n",
    "if False:\n",
    "    query = Q.select_users_tag_final_state(table_id='train')\n",
    "    df_users_tag = bqh.get_df_query_gcs(query, dtypes=dtypes, file_format='csv')\n",
    "    df_users_tag.to_pickle('df_users_tag.pkl')\n",
    "    bucket.blob('df_users_tag.pkl').upload_from_filename('df_users_tag.pkl')\n",
    "else:\n",
    "    bucket.blob('df_users_tag.pkl').download_to_filename('df_users_tag.pkl')\n",
    "    df_users_tag = pd.read_pickle('df_users_tag.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m_RJCpSxcrGy"
   },
   "outputs": [],
   "source": [
    "df_users_tag[(df_users_tag.user_id == 8623) & (df_users_tag.tag == 69)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tWvj0FYpjuRj"
   },
   "source": [
    "### Download Content-Tags Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mG3gRUwmjuRl"
   },
   "outputs": [],
   "source": [
    "# <hide-input><hide-output>\n",
    "if True:\n",
    "    # only 14k rows, so it downloaded directly from BigQuery\n",
    "    df_content_tags = bqh.get_df_table('content_tags', max_results=None, dtypes=None)\n",
    "    df_questions = df_content_tags[df_content_tags.lecture_id.isna()]\n",
    "    del df_questions['lecture_id']\n",
    "    df_questions = df_questions.astype({d: t for d, t in dtypes.items() if d in df_questions.columns})\n",
    "    df_lectures = df_content_tags[df_content_tags.question_id.isna()][['ql_id', 'lecture_id', 'part', 'tags', 'tags_array', 'tag_0']]\n",
    "    df_lectures = df_lectures.astype({d: t for d, t in dtypes.items() if d in df_lectures.columns})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XD5hL-At_YxD"
   },
   "outputs": [],
   "source": [
    "df_questions.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_NH5WM3VjuRn"
   },
   "source": [
    "## Update Kaggle Submission Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6GV4lXzRKvE8"
   },
   "outputs": [],
   "source": [
    "if True:\n",
    "    exps={}\n",
    "    for e in [\n",
    "              'ae1aabe76df44072ae163b8068fe51ad'\n",
    "              ]:\n",
    "        \n",
    "        exp = APIExperiment(previous_experiment=e)\n",
    "        exp.download_model(e)\n",
    "        e_model = xgb.Booster(model_file=f'{e}.xgb')\n",
    "        exps[e] = e_model.attributes()\n",
    "\n",
    "        columns_train = json.loads(exp.get_parameters_summary(\n",
    "            parameter='feature_names')['valueCurrent'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x0B8sbkGjuRo"
   },
   "outputs": [],
   "source": [
    "# <hide-input>\n",
    "if True:\n",
    "    Path(KAGGLE_SUBMIT_DATASET).mkdir(exist_ok=True)\n",
    "\n",
    "    with open(f'{KAGGLE_SUBMIT_DATASET}/columns.json', 'w') as cj:\n",
    "            json.dump(columns_train, cj)\n",
    "\n",
    "    with open(f'{KAGGLE_SUBMIT_DATASET}/dtypes.json', 'w') as dj:\n",
    "            json.dump(dtypes, dj)\n",
    "    \n",
    "    with open(f'{KAGGLE_SUBMIT_DATASET}/models.json', 'w') as mj:\n",
    "        json.dump(exps, mj)\n",
    "\n",
    "    for m in exps:\n",
    "        src = Path(f'{m}.xgb')\n",
    "        (KAGGLE_SUBMIT_DATASET/src).write_bytes(src.read_bytes())\n",
    "    \n",
    "    df_files = {\n",
    "        'df_users.pkl': df_users,\n",
    "        'df_users_content.pkl': df_users_content,\n",
    "        'df_users_part.pkl': df_users_content,\n",
    "        'df_users_tag.pkl': df_users_tag,\n",
    "        'df_questions.pkl': df_questions,\n",
    "        'df_lectures.pkl': df_lectures\n",
    "    }\n",
    "\n",
    "    for file_path, df in df_files.items():\n",
    "        if Path(file_path).exists():\n",
    "            (Path(f'{KAGGLE_SUBMIT_DATASET}/{file_path}')\n",
    "            .write_bytes(Path(file_path).read_bytes()))\n",
    "        else:\n",
    "            df.to_pickle(f'{KAGGLE_SUBMIT_DATASET}/{file_path}')\n",
    "\n",
    "    (Path(f'{KAGGLE_SUBMIT_DATASET}/{weights_fn}')\n",
    "    .write_bytes(Path(weights_fn).read_bytes()))\n",
    "            \n",
    "    kaggle_id = f\"{os.getenv('KAGGLE_USERNAME')}/{KAGGLE_SUBMIT_DATASET}\"\n",
    "    \n",
    "    metadata = {\n",
    "        \"licenses\": [{\"name\": \"CC0-1.0\"}],\n",
    "        \"id\": kaggle_id,\n",
    "        \"title\": KAGGLE_SUBMIT_DATASET\n",
    "    }\n",
    "\n",
    "    with open(f'{KAGGLE_SUBMIT_DATASET}/dataset-metadata.json', 'w') as f:\n",
    "        json.dump(metadata, f)\n",
    "            \n",
    "    if kaggle_api.dataset_status(kaggle_id):\n",
    "        kaggle_api.dataset_create_version(KAGGLE_SUBMIT_DATASET,\n",
    "                                          version_notes='update dataset',\n",
    "                                          delete_old_versions=True,\n",
    "                                          dir_mode='tar',\n",
    "                                          quiet=True\n",
    "                                         )\n",
    "    else:\n",
    "        kaggle_api.dataset_create_new(KAGGLE_SUBMIT_DATASET,\n",
    "                                      dir_mode='tar', quiet=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d-RK0etUwdw3"
   },
   "source": [
    "## Push Kernel to Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XoJcDapFwhCT"
   },
   "outputs": [],
   "source": [
    "# <hide-input>\n",
    "if NOT_KAGGLE:\n",
    "    if False:\n",
    "        \n",
    "        code_file = 'riiid-2020.ipynb'\n",
    "        with open(DRIVE/REPO/code_file, 'r') as nb:\n",
    "            nb_json = json.load(nb)       \n",
    "        \n",
    "        for i, cell in enumerate(nb_json['cells']):\n",
    "            if cell['cell_type'] == 'code':\n",
    "                \n",
    "                # update show/hide code cells\n",
    "                for h in ['input', 'output']:c\n",
    "                    if cell['source'][0].find(f'<hide-{h}') > 1:\n",
    "                        nb_json['cells'][i]['metadata'].update({f'_kg_hide-{h}': True})\n",
    "                    else:\n",
    "                        nb_json['cells'][i]['metadata'].pop(f'_kg_hide-{h}', None)\n",
    "\n",
    "                # add modules as cells\n",
    "                if len(cell['source']) == 1:\n",
    "                    groups = re.search(r'(?<=\\<include-)(.*?)(?=\\>)', cell['source'][0])\n",
    "                    \n",
    "                    if groups:\n",
    "                        with open(DRIVE/REPO/groups.group(0), 'r') as m:\n",
    "                            nb_json['cells'][i]['source'] = m.readlines() + nb_json['cells'][i]['source']    \n",
    "\n",
    "\n",
    "        if Path(code_file).exists():\n",
    "            Path(code_file).unlink()\n",
    "        \n",
    "        with open(f'{code_file}', 'w') as f:\n",
    "            json.dump(nb_json, f)\n",
    "\n",
    "        data = {'id': 'calebeverett/riiid-bigquery-xgboost-end-to-end',\n",
    "                        'title': 'RIIID: BigQuery-XGBoost End-to-End',\n",
    "                        'code_file': code_file,\n",
    "                        'language': 'python',\n",
    "                        'kernel_type': 'notebook',\n",
    "                        'is_private': 'false',\n",
    "                        'enable_gpu': 'true',\n",
    "                        'enable_internet': 'true',\n",
    "                        'dataset_sources': [],\n",
    "                        'competition_sources': ['riiid-test-answer-prediction'],\n",
    "                        'kernel_sources': []}\n",
    "        \n",
    "        with open('kernel-metadata.json', 'w') as f:\n",
    "            json.dump(data, f)\n",
    "\n",
    "        kaggle_api.kernels_push('.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HN5k3_N-juRp"
   },
   "source": [
    "## Submit From Kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BGXlGXmjjuRq"
   },
   "source": [
    "* Go to [RIIID Submit](https://www.kaggle.com/calebeverett/riiid-submit), fork and update to reference your dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vmHGcMTtEDZi"
   },
   "source": [
    "## Push Submit Kernel to Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h5oJG61mU9ES"
   },
   "outputs": [],
   "source": [
    "dataset_status = None\n",
    "while dataset_status != 'ready':\n",
    "    time.sleep(1)\n",
    "    dataset_status = kaggle_api.datasets_status(CONFIG.get('KAGGLE_USERNAME'),\n",
    "                                                KAGGLE_SUBMIT_DATASET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iO6rPRH6EQJZ"
   },
   "outputs": [],
   "source": [
    "# <hide-input>\n",
    "if NOT_KAGGLE:\n",
    "    if True:\n",
    "        submit_kernel = 'calebeverett/riiid-submit-private'\n",
    "        kernel_path = Path('submit_kernel')\n",
    "        kaggle_api.kernels_pull('calebeverett/riiid-submit-private', kernel_path, metadata=True)\n",
    "        kaggle_api.kernels_push(kernel_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I4xRZVyHWFLT"
   },
   "outputs": [],
   "source": [
    "kaggle_api.kernels_status('calebeverett/riiid-submit-private')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_VVtvmvU3s9T"
   },
   "source": [
    "## Update Experiments with Submission Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Qfp_-0ZMN7xT"
   },
   "outputs": [],
   "source": [
    "kaggle_api.kernels_list(mine=True, competition='riiid-test-answer-prediction')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ct8kmjFx30aw"
   },
   "outputs": [],
   "source": [
    "kaggle_api.competition_submissions_cli('riiid-test-answer-prediction')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wDpy6AmWx_U8"
   },
   "outputs": [],
   "source": [
    "if True:\n",
    "    exp_keys = ['3fab22b012ca4ecb82421737065ca398', '4430f267de2540cb92917f52d2754ab9', '6fa487e0d09f44da8d461f7c576bd4ee', '94591bb1f9d1419fb121738fa950c3f7', '9e697d20ac3c4aecb67e0ccda53b7586']\n",
    "\n",
    "    for exp_key in exp_keys:\n",
    "        exp = APIExperiment(previous_experiment=exp_key)\n",
    "        exp.log_other('submitted', True)\n",
    "        exp.log_other('submitDate', '2020-12-04 04:55:01')\n",
    "        exp.log_other('publicScore', 0.776)\n",
    "        exp.log_other('kernelUrl', 'https://www.kaggle.com/calebeverett/riiid-submit-private?scriptVersionId=48487773')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "cs6bXr35juQM",
    "QhPzQmvUjuQV",
    "GOpOrtr9juQY",
    "sgTZkanIjuQk",
    "YTG_h2a2juQy",
    "zO6GM_CLjuQ5"
   ],
   "machine_shape": "hm",
   "name": "riiid-2020-new.ipynb",
   "private_outputs": true,
   "provenance": [],
   "toc_visible": true
  },
  "environment": {
   "name": "tf2-2-3-gpu.2-3.m59",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-2-3-gpu.2-3:m59"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "396.831px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
