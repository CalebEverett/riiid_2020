{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# <hide-input>\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "code_folding": [],
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1gbbiyXJdj4j",
    "outputId": "9e57da8c-42bf-4079-e720-3b73d17b94bc"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/tqdm/std.py:699: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version\n",
      "  from pandas import Panel\n"
     ]
    }
   ],
   "source": [
    "# <hide-input>\n",
    "from datetime import datetime\n",
    "import gc\n",
    "import json\n",
    "import math\n",
    "import os\n",
    "from pathlib import Path\n",
    "import re\n",
    "import subprocess\n",
    "import sys\n",
    "import time\n",
    "\n",
    "import ipywidgets as widgets\n",
    "from google.cloud import storage, bigquery\n",
    "from google.cloud.bigquery import SchemaField\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "\n",
    "AUTO = tf.data.experimental.AUTOTUNE\n",
    "BUCKET = 'caleb-riiid'\n",
    "DATASET = 'data'\n",
    "LOCATION = 'europe-west4'\n",
    "KAGGLE_SUBMIT_DATASET = 'riiid-submission-private'\n",
    "PROJECT = 'fastai-caleb'\n",
    "REPO = 'riiid_2020'\n",
    "NOT_KAGGLE = os.getenv('KAGGLE_URL_BASE') is None\n",
    "\n",
    "# if NOT_KAGGLE:\n",
    "#     from google.colab import drive\n",
    "#     DRIVE = Path('/content/drive/My Drive')\n",
    "#     if not DRIVE.exists():\n",
    "#         drive.mount(str(DRIVE.parent))\n",
    "#     sys.path.append(str(DRIVE))\n",
    "#     g_creds_path = 'credentials/riiid-caleb-faddd0c9d900.json'\n",
    "#     os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = str(DRIVE/g_creds_path)\n",
    "\n",
    "bucket = storage.Client(project=PROJECT).get_bucket(BUCKET)\n",
    "dataset = bigquery.Dataset(f'{PROJECT}.{DATASET}')\n",
    "bq_client = bigquery.Client(project=PROJECT, location=LOCATION)\n",
    "\n",
    "if NOT_KAGGLE:\n",
    "    CONFIG = json.loads(bucket.get_blob('config.json').download_as_string())\n",
    "    os.environ = {**os.environ, **CONFIG}\n",
    "    sys.path.append('/home/jupyter')\n",
    "    from riiid_2020.bqhelpers import BQHelper\n",
    "    from riiid_2020.queries import Queries\n",
    "\n",
    "    from comet_ml import APIExperiment, Experiment\n",
    "    from kaggle.api.kaggle_api_extended import KaggleApi\n",
    "    kaggle_api = KaggleApi()\n",
    "    kaggle_api.authenticate()\n",
    "\n",
    "import plotly\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "# pd.options.plotting.backend = 'plotly'\n",
    "tqdm.pandas()\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LRFinder(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, start=1e-7, end=5, steps=100):\n",
    "        self.losses = []\n",
    "        self.start = start\n",
    "        self.end = end\n",
    "        self.steps = steps\n",
    "        self.best_loss = np.inf\n",
    "\n",
    "    def on_batch_begin(self, step, logs):\n",
    "        scheduled_lr = self.start * (self.end / self.start) ** (step/self.steps)\n",
    "        tf.keras.backend.set_value(self.model.optimizer.lr, scheduled_lr)\n",
    "\n",
    "    def on_batch_end(self, step, logs):\n",
    "        loss = logs.get('loss')\n",
    "        self.losses.append(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OneCycleScheduler(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, total_steps=1000, steps_up_pct=0.3, steps_across_pct=.01, steps_down_pct=0.6, lr_max=.001,\n",
    "              lr_start_factor=.00003, lr_end_factor=.00001, decay=0.93,\n",
    "              mo_max=0.95, mo_min=0.85, verbose=1):\n",
    "        \n",
    "        self.step = -1\n",
    "        self.epoch = -1\n",
    "\n",
    "        super(OneCycleScheduler, self).__init__()\n",
    "\n",
    "        def one_cycle(step):\n",
    "            \n",
    "            steps_up = int(total_steps * steps_up_pct)\n",
    "            steps_across = int(total_steps * steps_across_pct)\n",
    "            steps_down = int(total_steps * steps_down_pct)\n",
    "            lr_start = lr_max * lr_start_factor\n",
    "            lr_end = lr_max * lr_end_factor\n",
    "\n",
    "            if step <= steps_up:\n",
    "                new_lr = (lr_max - lr_start)/2  * (-math.cos((math.pi * step) / steps_up) + 1) + lr_start\n",
    "                new_mo = (mo_max - mo_min)/2  * (math.cos((math.pi * step) / steps_up) + 1) + mo_min\n",
    "            \n",
    "            elif step <= (steps_up + steps_across):\n",
    "                new_lr = lr_max\n",
    "                new_mo = mo_min\n",
    "            \n",
    "            elif step <= (steps_up + steps_across + steps_down):\n",
    "                down_step = step - steps_across - steps_up\n",
    "                new_lr = (lr_max - lr_end)/2  * (math.cos((math.pi * down_step) / steps_down) + 1) + lr_end\n",
    "                new_mo = (mo_max - mo_min)/2  * (-math.cos((math.pi * down_step) / steps_down) + 1) + mo_min\n",
    "\n",
    "            else:\n",
    "                new_lr = lr_end * decay**(step - steps_up - steps_across - steps_down)\n",
    "                new_mo = mo_max\n",
    "            \n",
    "            return new_lr, new_mo\n",
    "\n",
    "        self.schedule = one_cycle\n",
    "        self.verbose = verbose\n",
    " \n",
    "    def on_batch_begin(self, step, logs):\n",
    "        self.step +=1\n",
    "        scheduled_lr, scheduled_mo = self.schedule(self.step)\n",
    "        tf.keras.backend.set_value(self.model.optimizer.lr, scheduled_lr)\n",
    "        tf.keras.backend.set_value(self.model.optimizer.beta_1, scheduled_mo)\n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs):\n",
    "        if self.verbose:\n",
    "            scheduled_lr, scheduled_mo = self.schedule(self.step)\n",
    "            auc_roc = logs.get('val_auc_roc')\n",
    "            auc_roc = auc_roc if auc_roc is not None else 0\n",
    "            print(f'\\nepoch {epoch+1:02d}: val_auc_roc={auc_roc:0.4f}, learning_rate={scheduled_lr:0.2e}, beta_1={scheduled_mo:0.3f}')\n",
    "            \n",
    "def plot_lr_sched(one_cycle, total_steps):\n",
    "    fig = make_subplots(specs=[[{\"secondary_y\": True}]])\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=list(range(total_steps)),\n",
    "                   y=[one_cycle.schedule(e)[0] for e in range(total_steps)],\n",
    "                   name=\"lr\"),\n",
    "        secondary_y=False,\n",
    "    )\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=list(range(total_steps)),\n",
    "                   y=[one_cycle.schedule(e)[1] for e in range(total_steps)],\n",
    "                   name=\"mom\"),\n",
    "        secondary_y=True,\n",
    "    )\n",
    "\n",
    "    fig.update_layout(title_text=\"Learning Rate Schedule\")\n",
    "    fig.update_xaxes(title_text=\"steps\")\n",
    "    fig.update_yaxes(title_text=\"learning rate\", secondary_y=False)\n",
    "    fig.update_yaxes(title_text=\"momentum\", secondary_y=True)\n",
    "\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "u8ltlKOHp4CR",
    "outputId": "b96cbd3a-68d0-47b3-fd7a-0f1d7c88d9b4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on TPU  grpc://10.165.92.218:8470\n",
      "INFO:tensorflow:Initializing the TPU system: tpu-1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Initializing the TPU system: tpu-1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Clearing out eager caches\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Clearing out eager caches\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Finished initializing TPU system.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Finished initializing TPU system.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Found TPU system:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Found TPU system:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Num TPU Cores: 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Num TPU Cores: 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Num TPU Workers: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Num TPU Workers: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
     ]
    }
   ],
   "source": [
    "def get_strategy():\n",
    "\n",
    "    try:\n",
    "        tpu = tf.distribute.cluster_resolver.TPUClusterResolver('tpu-1')\n",
    "        print('Running on TPU ', tpu.master())\n",
    "    except:\n",
    "        tpu = None\n",
    "\n",
    "    if tpu:\n",
    "        tf.config.experimental_connect_to_cluster(tpu)\n",
    "        tf.tpu.experimental.initialize_tpu_system(tpu)\n",
    "        strategy = tf.distribute.TPUStrategy(tpu)\n",
    "    \n",
    "    else:\n",
    "        strategy = tf.distribute.get_strategy()\n",
    "        for d in tf.config.list_physical_devices():\n",
    "            print(d)\n",
    "            \n",
    "    return strategy\n",
    "\n",
    "strategy = get_strategy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtypes=  {\n",
    "    'row_id': 'int64',\n",
    "    'timestamp': 'int64',\n",
    "    'user_id': 'int32',\n",
    "    'content_id': 'int16',\n",
    "    'content_type_id': 'int8',\n",
    "    'task_container_id': 'int16',\n",
    "    'user_answer': 'int8',\n",
    "    'answered_correctly': 'int8',\n",
    "    'prior_question_elapsed_time': 'float32', \n",
    "    'prior_question_had_explanation': 'bool',\n",
    "    'tid_orig': 'int16'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "bqh = BQHelper(bucket, DATASET, bq_client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "folds = range(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trn_attn = bqh.get_df_query_bqs(f\"\"\"\n",
    "    WITH t AS (\n",
    "        SELECT fold, timestamp, user_id, content_id + 1 content_id, answered_correctly + 1 answered_correctly,\n",
    "        MAX(task_container_id) OVER(PARTITION BY user_id) tid_max\n",
    "        FROM data.train\n",
    "        WHERE content_type_id = 0\n",
    "    )\n",
    "    SELECT fold, timestamp, user_id, content_id, answered_correctly\n",
    "    FROM t\n",
    "    WHERE tid_max > 9\n",
    "    AND fold in ({(',').join(map(str,folds))});\n",
    "\"\"\",'df_trn_attn.pkl', from_bq=False, dtypes=dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_Q = df_trn_attn.content_id.max() + 1\n",
    "NUM_A = df_trn_attn.answered_correctly.max() + 1\n",
    "LEN_SEQ = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def get_seq(group, max_seq_len=LEN_SEQ):\n",
    "    q, a = group\n",
    "    pad_len = max(max_seq_len - len(q), 0)\n",
    "    tq = tf.concat([tf.zeros(pad_len, dtype=tf.int64), tf.constant(q[:max_seq_len], dtype=tf.int64)], axis=0)\n",
    "    ta = tf.concat([tf.zeros(pad_len, dtype=tf.int64), tf.constant(a[:max_seq_len], dtype=tf.int64)], axis=0)\n",
    "    return tq, ta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_masks_to_batch(features, labels):\n",
    "    seq = tf.cast(tf.math.equal(features, 0), tf.float32)\n",
    "    pad_mask_enc = seq[:, tf.newaxis, tf.newaxis, :]\n",
    "    pad_mask_dec = tf.identity(pad_mask_enc)\n",
    "        \n",
    "    size = tf.shape(labels)[1]\n",
    "    look_ahead_mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
    "\n",
    "    combined_mask = tf.maximum(pad_mask_dec, look_ahead_mask)\n",
    "        \n",
    "    return  features, pad_mask_enc, combined_mask, pad_mask_dec, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ds(df, batch_size, repeat=True, shuffle=True):\n",
    "    group = (df[['user_id', 'content_id', 'answered_correctly']]\n",
    "             .groupby('user_id').apply(lambda r: ( r['content_id'].values,\n",
    "                                              r['answered_correctly'].values)))\n",
    "    q_seq, a_seq = zip(*map(get_seq, group))\n",
    "    t_slices = (tf.stack(q_seq, axis=0), tf.stack(a_seq, axis=0))\n",
    "    ds = tf.data.Dataset.from_tensor_slices(t_slices).shuffle(1000)\n",
    "    ds = ds.repeat() if repeat else ds\n",
    "    ds = ds.batch(batch_size) #.map(add_masks_to_batch, num_parallel_calls=AUTO)\n",
    "    \n",
    "    return ds.prefetch(AUTO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Positional Encoding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_angles(pos, i, d_model):\n",
    "    angle_rates = 1 / np.power(10000, (2 * (i//2)) / np.float32(d_model))\n",
    "    return pos * angle_rates\n",
    "\n",
    "def positional_encoding(position, d_model):\n",
    "    angle_rads = get_angles(np.arange(position)[:, np.newaxis],\n",
    "                          np.arange(d_model)[np.newaxis, :],\n",
    "                          d_model)\n",
    "\n",
    "    # apply sin to even indices in the array; 2i\n",
    "    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
    "\n",
    "    # apply cos to odd indices in the array; 2i+1\n",
    "    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
    "\n",
    "    pos_encoding = angle_rads[np.newaxis, ...]\n",
    "\n",
    "    return tf.cast(pos_encoding, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Masking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_padding_mask(seq):\n",
    "    seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
    "\n",
    "    # add extra dimensions to add the padding\n",
    "    # to the attention logits.\n",
    "    return seq[:, tf.newaxis, tf.newaxis, :]  # (batch_size, 1, 1, seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def create_look_ahead_mask(size):\n",
    "    mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
    "    return mask  # (seq_len, seq_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* query, key and value matrices are trainable weights\n",
    "* \n",
    "* question code --> embedding --> multiplied by querys and keys, which I believe are trainable weights, to produce a combination of attention weights and values for each word(question in this case) in the sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dot Product Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(q, k, v, mask):\n",
    "    \"\"\"Calculate the attention weights.\n",
    "    q, k, v must have matching leading dimensions.\n",
    "    k, v must have matching penultimate dimension, i.e.: seq_len_k = seq_len_v.\n",
    "    The mask has different shapes depending on its type(padding or look ahead) \n",
    "    but it must be broadcastable for addition.\n",
    "\n",
    "    Args:\n",
    "    q: query shape == (..., seq_len_q, depth)\n",
    "    k: key shape == (..., seq_len_k, depth)\n",
    "    v: value shape == (..., seq_len_v, depth_v)\n",
    "    mask: Float tensor with shape broadcastable \n",
    "          to (..., seq_len_q, seq_len_k). Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "    output, attention_weights\n",
    "    \"\"\"\n",
    "\n",
    "    matmul_qk = tf.matmul(q, k, transpose_b=True)  # (..., seq_len_q, seq_len_k)\n",
    "\n",
    "    # scale matmul_qk\n",
    "    dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
    "    scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
    "\n",
    "    # add the mask to the scaled tensor.\n",
    "    if mask is not None:\n",
    "        scaled_attention_logits += (mask * -1e9)  \n",
    "\n",
    "    # softmax is normalized on the last axis (seq_len_k) so that the scores\n",
    "    # add up to 1.\n",
    "    attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)  # (..., seq_len_q, seq_len_k)\n",
    "\n",
    "    output = tf.matmul(attention_weights, v)  # (..., seq_len_q, depth_v)\n",
    "\n",
    "    return output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#what gets returned are the weighted values for each word in the sequence along\n",
    "#with the weights that were applied to the values to get the weighted values\n",
    "#the weights were determined by multiplying the embedding representation of each\n",
    "#word three trainable matrices, q, k, v, and then multiplying q x k.\n",
    "# embed x (Q, K, V) --> q, k, v --> weights = softmax(q x k)/scaled  --> weighted values = weights * values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_out(q, k, v):\n",
    "    temp_out, temp_attn = scaled_dot_product_attention(\n",
    "      q, k, v, None)\n",
    "    print ('Attention weights are:')\n",
    "    print (temp_attn)\n",
    "    print ('Output is:')\n",
    "    print (temp_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotFoundError",
     "evalue": "'_MklMatMul' is neither a type of a primitive operation nor a name of a function registered in binary running on n-548b2f10-w-0. Make sure the operation or function is registered in the binary running in this process.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-101-3a09f34caef3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m# so the second `value` is returned.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mtemp_q\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# (1, 3)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mprint_out\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemp_q\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemp_k\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemp_v\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-100-fc2efc100e2a>\u001b[0m in \u001b[0;36mprint_out\u001b[0;34m(q, k, v)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mprint_out\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     temp_out, temp_attn = scaled_dot_product_attention(\n\u001b[0;32m----> 3\u001b[0;31m       q, k, v, None)\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'Attention weights are:'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtemp_attn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-99-210ff620eba2>\u001b[0m in \u001b[0;36mscaled_dot_product_attention\u001b[0;34m(q, k, v, mask)\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;31m# softmax is normalized on the last axis (seq_len_k) so that the scores\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0;31m# add up to 1.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m     \u001b[0mattention_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscaled_attention_logits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# (..., seq_len_q, seq_len_k)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattention_weights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# (..., seq_len_q, depth_v)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m       \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/ops/nn_ops.py\u001b[0m in \u001b[0;36msoftmax_v2\u001b[0;34m(logits, axis, name)\u001b[0m\n\u001b[1;32m   3666\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3667\u001b[0m     \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3668\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0m_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgen_nn_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3669\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3670\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/ops/nn_ops.py\u001b[0m in \u001b[0;36m_softmax\u001b[0;34m(logits, compute_op, dim, name)\u001b[0m\n\u001b[1;32m   3552\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3553\u001b[0m   \u001b[0;31m# We need its original shape for shape inference.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3554\u001b[0;31m   \u001b[0mshape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3555\u001b[0m   \u001b[0mis_last_dim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndims\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3556\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mget_shape\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1173\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mget_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1174\u001b[0m     \u001b[0;34m\"\"\"Alias of Tensor.shape.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1175\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1177\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_shape_as_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mshape\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1165\u001b[0m         \u001b[0;31m# `_tensor_shape` is declared and defined in the definition of\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1166\u001b[0m         \u001b[0;31m# `EagerTensor`, in C.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1167\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tensor_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtensor_shape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensorShape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shape_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1168\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1169\u001b[0m         \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNotFoundError\u001b[0m: '_MklMatMul' is neither a type of a primitive operation nor a name of a function registered in binary running on n-548b2f10-w-0. Make sure the operation or function is registered in the binary running in this process."
     ]
    }
   ],
   "source": [
    "np.set_printoptions(suppress=True)\n",
    "\n",
    "temp_k = tf.constant([[10,0,0],\n",
    "                      [0,10,0],\n",
    "                      [0,0,10],\n",
    "                      [0,0,10]], dtype=tf.float32)  # (4, 3)\n",
    "\n",
    "temp_v = tf.constant([[   1,0],\n",
    "                      [  10,0],\n",
    "                      [ 100,5],\n",
    "                      [1000,6]], dtype=tf.float32)  # (4, 2)\n",
    "\n",
    "# This `query` aligns with the second `key`,\n",
    "# so the second `value` is returned.\n",
    "temp_q = tf.constant([[0, 10, 0]], dtype=tf.float32)  # (1, 3)\n",
    "print_out(temp_q, temp_k, temp_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "\n",
    "        assert d_model % self.num_heads == 0\n",
    "\n",
    "        self.depth = d_model // self.num_heads\n",
    "\n",
    "        self.wq = tf.keras.layers.Dense(d_model)\n",
    "        self.wk = tf.keras.layers.Dense(d_model)\n",
    "        self.wv = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "        self.dense = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "    def split_heads(self, x, batch_size):\n",
    "        \"\"\"Split the last dimension into (num_heads, depth).\n",
    "        Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
    "        \"\"\"\n",
    "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "\n",
    "    def call(self, v, k, q, mask):\n",
    "        batch_size = tf.shape(q)[0]\n",
    "\n",
    "        q = self.wq(q)  # (batch_size, seq_len, d_model)\n",
    "        k = self.wk(k)  # (batch_size, seq_len, d_model)\n",
    "        v = self.wv(v)  # (batch_size, seq_len, d_model)\n",
    "\n",
    "        # (batch_size, num_heads, seq_len_q, depth)\n",
    "        q = self.split_heads(q, batch_size)\n",
    "        # (batch_size, num_heads, seq_len_k, depth)\n",
    "        k = self.split_heads(k, batch_size)\n",
    "        # (batch_size, num_heads, seq_len_v, depth)\n",
    "        v = self.split_heads(v, batch_size)\n",
    "\n",
    "        # scaled_attention.shape == (batch_size, num_heads, seq_len_q, depth)\n",
    "        # attention_weights.shape == (batch_size, num_heads, seq_len_q, seq_len_k)\n",
    "        scaled_attention, attention_weights = scaled_dot_product_attention(\n",
    "            q, k, v, mask)\n",
    "\n",
    "        # (batch_size, seq_len_q, num_heads, depth)\n",
    "        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])\n",
    "\n",
    "        concat_attention = tf.reshape(scaled_attention,\n",
    "                                      (batch_size, -1, self.d_model))  # (batch_size, seq_len_q, d_model)\n",
    "\n",
    "        # (batch_size, seq_len_q, d_model)\n",
    "        output = self.dense(concat_attention)\n",
    "\n",
    "        return output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotFoundError",
     "evalue": "'_MklMatMul' is neither a type of a primitive operation nor a name of a function registered in binary running on n-548b2f10-w-0. Make sure the operation or function is registered in the binary running in this process. [Op:BiasAdd]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-103-8578312cab50>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# values returned in the concat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtemp_mha\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    983\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_auto_cast_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compute_dtype_object\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 985\u001b[0;31m           \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    986\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    987\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_activity_regularizer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-102-d450c33df90d>\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, v, k, q, mask)\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         \u001b[0mq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# (batch_size, seq_len, d_model)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0mk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# (batch_size, seq_len, d_model)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# (batch_size, seq_len, d_model)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    983\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_auto_cast_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compute_dtype_object\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 985\u001b[0;31m           \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    986\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    987\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_activity_regularizer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/layers/core.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1197\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1198\u001b[0;31m         dtype=self._compute_dtype_object)\n\u001b[0m\u001b[1;32m   1199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1200\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcompute_output_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/layers/ops/core.py\u001b[0m in \u001b[0;36mdense\u001b[0;34m(inputs, kernel, bias, activation, dtype)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias_add\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mactivation\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m       \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/ops/nn_ops.py\u001b[0m in \u001b[0;36mbias_add\u001b[0;34m(value, bias, data_format, name)\u001b[0m\n\u001b[1;32m   3364\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3365\u001b[0m       return gen_nn_ops.bias_add(\n\u001b[0;32m-> 3366\u001b[0;31m           value, bias, data_format=data_format, name=name)\n\u001b[0m\u001b[1;32m   3367\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3368\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/ops/gen_nn_ops.py\u001b[0m in \u001b[0;36mbias_add\u001b[0;34m(value, bias, data_format, name)\u001b[0m\n\u001b[1;32m    679\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    680\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 681\u001b[0;31m       \u001b[0m_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    682\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    683\u001b[0m       \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mraise_from_not_ok_status\u001b[0;34m(e, name)\u001b[0m\n\u001b[1;32m   6841\u001b[0m   \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\" name: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6842\u001b[0m   \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6843\u001b[0;31m   \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6844\u001b[0m   \u001b[0;31m# pylint: enable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6845\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/six.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
      "\u001b[0;31mNotFoundError\u001b[0m: '_MklMatMul' is neither a type of a primitive operation nor a name of a function registered in binary running on n-548b2f10-w-0. Make sure the operation or function is registered in the binary running in this process. [Op:BiasAdd]"
     ]
    }
   ],
   "source": [
    "temp_mha = MultiHeadAttention(d_model=512, num_heads=8)\n",
    "y = tf.random.uniform((1, 60, 512))  # (batch_size, encoder_sequence, d_model)\n",
    "\n",
    "# d_model = 512, depth = 64\n",
    "# split d_model weights in to num_heads x d_model/depth matrices\n",
    "# calc output and weights for each head\n",
    "# weights for each head include a weight for each position in the sequence for \n",
    "# every position in the sequence\n",
    "# concat the output back together to have the same depth as the model again\n",
    "# so you essentially have 8 sets of output for each position in the sequence\n",
    "# and then run that through a dense layer to decide how to weight each of the d_model\n",
    "# values returned in the concat\n",
    "\n",
    "out, attn = temp_mha(y, k=y, q=y, mask=None)\n",
    "out.shape, attn.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dff means depth of feadforward\n",
    "def point_wise_feed_forward_network(d_model, dff):\n",
    "    return tf.keras.Sequential([\n",
    "      tf.keras.layers.Dense(dff, activation='relu'),  # (batch_size, seq_len, dff)\n",
    "      tf.keras.layers.Dense(d_model)  # (batch_size, seq_len, d_model)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "\n",
    "        self.mha = MultiHeadAttention(d_model, num_heads)\n",
    "        self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
    "\n",
    "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
    "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
    "\n",
    "    def call(self, x, training, mask):\n",
    "\n",
    "        # (batch_size, input_seq_len, d_model)\n",
    "        attn_output, _ = self.mha(x, x, x, mask)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        # (batch_size, input_seq_len, d_model)\n",
    "        out1 = self.layernorm1(x + attn_output)\n",
    "\n",
    "        ffn_output = self.ffn(out1)  # (batch_size, input_seq_len, d_model)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        # (batch_size, input_seq_len, d_model)\n",
    "        out2 = self.layernorm2(out1 + ffn_output)\n",
    "\n",
    "        return out2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([64, 43, 512])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_encoder_layer = EncoderLayer(512, 8, 2048)\n",
    "\n",
    "sample_encoder_layer_output = sample_encoder_layer(\n",
    "    tf.random.uniform((64, 43, 512)), False, None)\n",
    "\n",
    "sample_encoder_layer_output.shape  # (batch_size, input_seq_len, d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "\n",
    "        self.mha1 = MultiHeadAttention(d_model, num_heads)\n",
    "        self.mha2 = MultiHeadAttention(d_model, num_heads)\n",
    "\n",
    "        self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
    "\n",
    "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
    "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
    "        self.dropout3 = tf.keras.layers.Dropout(rate)\n",
    "\n",
    "    def call(self, x, enc_output, training,\n",
    "             look_ahead_mask, padding_mask):\n",
    "        # enc_output.shape == (batch_size, input_seq_len, d_model)\n",
    "\n",
    "        # (batch_size, target_seq_len, d_model)\n",
    "        attn1, attn_weights_block1 = self.mha1(x, x, x, look_ahead_mask)\n",
    "        attn1 = self.dropout1(attn1, training=training)\n",
    "        out1 = self.layernorm1(attn1 + x)\n",
    "\n",
    "        attn2, attn_weights_block2 = self.mha2(\n",
    "            enc_output, enc_output, out1, padding_mask)  # (batch_size, target_seq_len, d_model)\n",
    "        attn2 = self.dropout2(attn2, training=training)\n",
    "        # (batch_size, target_seq_len, d_model)\n",
    "        out2 = self.layernorm2(attn2 + out1)\n",
    "\n",
    "        ffn_output = self.ffn(out2)  # (batch_size, target_seq_len, d_model)\n",
    "        ffn_output = self.dropout3(ffn_output, training=training)\n",
    "        # (batch_size, target_seq_len, d_model)\n",
    "        out3 = self.layernorm3(ffn_output + out2)\n",
    "\n",
    "        return out3, attn_weights_block1, attn_weights_block2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([64, 50, 512])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_decoder_layer = DecoderLayer(512, 8, 2048)\n",
    "\n",
    "sample_decoder_layer_output, _, _ = sample_decoder_layer(\n",
    "    tf.random.uniform((64, 50, 512)), sample_encoder_layer_output, \n",
    "    False, None, None)\n",
    "\n",
    "sample_decoder_layer_output.shape  # (batch_size, target_seq_len, d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size,\n",
    "                 maximum_position_encoding, rate=0.1):\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.embedding = tf.keras.layers.Embedding(input_vocab_size, d_model)\n",
    "        self.pos_encoding = positional_encoding(maximum_position_encoding,\n",
    "                                                self.d_model)\n",
    "\n",
    "        self.enc_layers = [EncoderLayer(d_model, num_heads, dff, rate)\n",
    "                           for _ in range(num_layers)]\n",
    "\n",
    "        self.dropout = tf.keras.layers.Dropout(rate)\n",
    "\n",
    "    def call(self, x, training, mask):\n",
    "\n",
    "        seq_len = tf.shape(x)[1]\n",
    "\n",
    "        # adding embedding and position encoding.\n",
    "        x = self.embedding(x)  # (batch_size, input_seq_len, d_model)\n",
    "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "        x += self.pos_encoding[:, :seq_len, :]\n",
    "\n",
    "        x = self.dropout(x, training=training)\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "            x = self.enc_layers[i](x, training, mask)\n",
    "\n",
    "        return x  # (batch_size, input_seq_len, d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 62, 512)\n"
     ]
    }
   ],
   "source": [
    "sample_encoder = Encoder(num_layers=2, d_model=512, num_heads=8, \n",
    "                         dff=2048, input_vocab_size=8500,\n",
    "                         maximum_position_encoding=10000)\n",
    "temp_input = tf.random.uniform((64, 62), dtype=tf.int64, minval=0, maxval=200)\n",
    "\n",
    "sample_encoder_output = sample_encoder(temp_input, training=False, mask=None)\n",
    "\n",
    "print (sample_encoder_output.shape)  # (batch_size, input_seq_len, d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, target_vocab_size,\n",
    "                 maximum_position_encoding, rate=0.1):\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.embedding = tf.keras.layers.Embedding(target_vocab_size, d_model)\n",
    "        self.pos_encoding = positional_encoding(\n",
    "            maximum_position_encoding, d_model)\n",
    "\n",
    "        self.dec_layers = [DecoderLayer(d_model, num_heads, dff, rate)\n",
    "                           for _ in range(num_layers)]\n",
    "        self.dropout = tf.keras.layers.Dropout(rate)\n",
    "\n",
    "    def call(self, x, enc_output, training,\n",
    "             look_ahead_mask, padding_mask):\n",
    "\n",
    "        seq_len = tf.shape(x)[1]\n",
    "        attention_weights = {}\n",
    "\n",
    "        x = self.embedding(x)  # (batch_size, target_seq_len, d_model)\n",
    "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "        x += self.pos_encoding[:, :seq_len, :]\n",
    "\n",
    "        x = self.dropout(x, training=training)\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "            x, block1, block2 = self.dec_layers[i](x, enc_output, training,\n",
    "                                                   look_ahead_mask, padding_mask)\n",
    "\n",
    "            attention_weights['decoder_layer{}_block1'.format(i+1)] = block1\n",
    "            attention_weights['decoder_layer{}_block2'.format(i+1)] = block2\n",
    "\n",
    "        # x.shape == (batch_size, target_seq_len, d_model)\n",
    "        return x, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotFoundError",
     "evalue": "'_MklMatMul' is neither a type of a primitive operation nor a name of a function registered in binary running on n-548b2f10-w-0. Make sure the operation or function is registered in the binary running in this process. [Op:BiasAdd]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-111-5509d3a6b214>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m                               \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m                               \u001b[0mlook_ahead_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m                               padding_mask=None)\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattn\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'decoder_layer2_block2'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    983\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_auto_cast_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compute_dtype_object\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 985\u001b[0;31m           \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    986\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    987\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_activity_regularizer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-110-c409231d5541>\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, x, enc_output, training, look_ahead_mask, padding_mask)\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_layers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m             x, block1, block2 = self.dec_layers[i](x, enc_output, training,\n\u001b[0;32m---> 31\u001b[0;31m                                                    look_ahead_mask, padding_mask)\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m             \u001b[0mattention_weights\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'decoder_layer{}_block1'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mblock1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    983\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_auto_cast_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compute_dtype_object\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 985\u001b[0;31m           \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    986\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    987\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_activity_regularizer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-108-7b3b04b3c023>\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, x, enc_output, training, look_ahead_mask, padding_mask)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0;31m# (batch_size, target_seq_len, d_model)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0mattn1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattn_weights_block1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmha1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlook_ahead_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m         \u001b[0mattn1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattn1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mout1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayernorm1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattn1\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    983\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_auto_cast_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compute_dtype_object\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 985\u001b[0;31m           \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    986\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    987\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_activity_regularizer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-102-d450c33df90d>\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, v, k, q, mask)\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         \u001b[0mq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# (batch_size, seq_len, d_model)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0mk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# (batch_size, seq_len, d_model)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# (batch_size, seq_len, d_model)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    983\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_auto_cast_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compute_dtype_object\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 985\u001b[0;31m           \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    986\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    987\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_activity_regularizer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/layers/core.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1197\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1198\u001b[0;31m         dtype=self._compute_dtype_object)\n\u001b[0m\u001b[1;32m   1199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1200\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcompute_output_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/layers/ops/core.py\u001b[0m in \u001b[0;36mdense\u001b[0;34m(inputs, kernel, bias, activation, dtype)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias_add\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mactivation\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m       \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/ops/nn_ops.py\u001b[0m in \u001b[0;36mbias_add\u001b[0;34m(value, bias, data_format, name)\u001b[0m\n\u001b[1;32m   3364\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3365\u001b[0m       return gen_nn_ops.bias_add(\n\u001b[0;32m-> 3366\u001b[0;31m           value, bias, data_format=data_format, name=name)\n\u001b[0m\u001b[1;32m   3367\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3368\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/ops/gen_nn_ops.py\u001b[0m in \u001b[0;36mbias_add\u001b[0;34m(value, bias, data_format, name)\u001b[0m\n\u001b[1;32m    679\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    680\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 681\u001b[0;31m       \u001b[0m_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    682\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    683\u001b[0m       \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mraise_from_not_ok_status\u001b[0;34m(e, name)\u001b[0m\n\u001b[1;32m   6841\u001b[0m   \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\" name: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6842\u001b[0m   \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6843\u001b[0;31m   \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6844\u001b[0m   \u001b[0;31m# pylint: enable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6845\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/six.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
      "\u001b[0;31mNotFoundError\u001b[0m: '_MklMatMul' is neither a type of a primitive operation nor a name of a function registered in binary running on n-548b2f10-w-0. Make sure the operation or function is registered in the binary running in this process. [Op:BiasAdd]"
     ]
    }
   ],
   "source": [
    "sample_decoder = Decoder(num_layers=2, d_model=512, num_heads=8, \n",
    "                         dff=2048, target_vocab_size=8000,\n",
    "                         maximum_position_encoding=5000)\n",
    "temp_input = tf.random.uniform((64, 26), dtype=tf.int64, minval=0, maxval=200)\n",
    "\n",
    "output, attn = sample_decoder(temp_input, \n",
    "                              enc_output=sample_encoder_output, \n",
    "                              training=False,\n",
    "                              look_ahead_mask=None, \n",
    "                              padding_mask=None)\n",
    "\n",
    "output.shape, attn['decoder_layer2_block2'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(tf.keras.Model):\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size,\n",
    "                 target_vocab_size, pe_input, pe_target, rate=0.1):\n",
    "        super(Transformer, self).__init__()\n",
    "\n",
    "        self.encoder = Encoder(num_layers, d_model, num_heads, dff,\n",
    "                               input_vocab_size, pe_input, rate)\n",
    "\n",
    "        self.decoder = Decoder(num_layers, d_model, num_heads, dff,\n",
    "                               target_vocab_size, pe_target, rate)\n",
    "\n",
    "        self.final_layer = tf.keras.layers.Dense(target_vocab_size)\n",
    "\n",
    "    def call(self, inp, tar, training, enc_padding_mask,\n",
    "             look_ahead_mask, dec_padding_mask):\n",
    "\n",
    "        # (batch_size, inp_seq_len, d_model)\n",
    "        enc_output = self.encoder(inp, training, enc_padding_mask)\n",
    "\n",
    "        # dec_output.shape == (batch_size, tar_seq_len, d_model)\n",
    "        dec_output, attention_weights = self.decoder(\n",
    "            tar, enc_output, training, look_ahead_mask, dec_padding_mask)\n",
    "\n",
    "        # (batch_size, tar_seq_len, target_vocab_size)\n",
    "        final_output = self.final_layer(dec_output)\n",
    "\n",
    "        return final_output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([64, 36, 8000])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_transformer = Transformer(\n",
    "    num_layers=2, d_model=512, num_heads=8, dff=2048, \n",
    "    input_vocab_size=8500, target_vocab_size=8000, \n",
    "    pe_input=10000, pe_target=6000)\n",
    "\n",
    "temp_input = tf.random.uniform((64, 38), dtype=tf.int64, minval=0, maxval=200)\n",
    "temp_target = tf.random.uniform((64, 36), dtype=tf.int64, minval=0, maxval=200)\n",
    "\n",
    "fn_out, _ = sample_transformer(temp_input, temp_target, training=False, \n",
    "                               enc_padding_mask=None, \n",
    "                               look_ahead_mask=None,\n",
    "                               dec_padding_mask=None)\n",
    "\n",
    "fn_out.shape  # (batch_size, tar_seq_len, target_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_layers = 4\n",
    "d_model = 128\n",
    "dff = 512\n",
    "num_heads = 8\n",
    "\n",
    "input_vocab_size = NUM_Q\n",
    "target_vocab_size = NUM_A\n",
    "dropout_rate = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, d_model, warmup_steps=4000):\n",
    "        super(CustomSchedule, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.d_model = tf.cast(self.d_model, tf.float32)\n",
    "\n",
    "        self.warmup_steps = warmup_steps\n",
    "\n",
    "    def __call__(self, step):\n",
    "        arg1 = tf.math.rsqrt(step)\n",
    "        arg2 = step * (self.warmup_steps ** -1.5)\n",
    "\n",
    "        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = CustomSchedule(d_model)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98, \n",
    "                                     epsilon=1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0, 'Train Step')"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "temp_learning_rate_schedule = CustomSchedule(d_model)\n",
    "\n",
    "plt.plot(temp_learning_rate_schedule(tf.range(40000, dtype=tf.float32)))\n",
    "plt.ylabel(\"Learning Rate\")\n",
    "plt.xlabel(\"Train Step\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "\n",
    "    return tf.reduce_sum(loss_)/tf.reduce_sum(mask)\n",
    "\n",
    "\n",
    "def accuracy_function(real, pred):\n",
    "    accuracies = tf.equal(real, tf.argmax(pred, axis=2))\n",
    "\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    accuracies = tf.math.logical_and(mask, accuracies)\n",
    "\n",
    "    accuracies = tf.cast(accuracies, dtype=tf.float32)\n",
    "    mask = tf.cast(mask, dtype=tf.float32)\n",
    "    return tf.reduce_sum(accuracies)/tf.reduce_sum(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "train_accuracy = tf.keras.metrics.Mean(name='train_accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    transformer = Transformer(num_layers, d_model, num_heads, dff,\n",
    "                              input_vocab_size, target_vocab_size, \n",
    "                              pe_input=10000, \n",
    "                              pe_target=10000,\n",
    "                              rate=dropout_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "77472"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BATCH_SIZE = 64 * strategy.num_replicas_in_sync\n",
    "EPOCHS = 2\n",
    "folds_trn, folds_val = folds[1:], folds[:1]\n",
    "\n",
    "num_ex_trn = df_trn_attn.fold.isin(folds_trn).sum()\n",
    "num_ex_val = df_trn_attn.fold.isin(folds_val).sum()\n",
    "steps_per_epoch = num_ex_trn // BATCH_SIZE\n",
    "steps_val = num_ex_val // BATCH_SIZE\n",
    "steps_per_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    checkpoint_path = \"./checkpoints/train\"\n",
    "\n",
    "    ckpt = tf.train.Checkpoint(transformer=transformer,\n",
    "                               optimizer=optimizer)\n",
    "\n",
    "    ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n",
    "\n",
    "    # if a checkpoint exists, restore the latest checkpoint.\n",
    "    if ckpt_manager.latest_checkpoint:\n",
    "        ckpt.restore(ckpt_manager.latest_checkpoint)\n",
    "        print ('Latest checkpoint restored!!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_masks(inp, tar):\n",
    "    # Encoder padding mask\n",
    "    enc_padding_mask = create_padding_mask(inp)\n",
    "\n",
    "    # Used in the 2nd attention block in the decoder.\n",
    "    # This padding mask is used to mask the encoder outputs.\n",
    "    dec_padding_mask = create_padding_mask(inp)\n",
    "\n",
    "    # Used in the 1st attention block in the decoder.\n",
    "    # It is used to pad and mask future tokens in the input received by\n",
    "    # the decoder.\n",
    "    look_ahead_mask = create_look_ahead_mask(tf.shape(tar)[1])\n",
    "    dec_target_padding_mask = create_padding_mask(tar)\n",
    "    combined_mask = tf.maximum(dec_target_padding_mask, look_ahead_mask)\n",
    "\n",
    "    return enc_padding_mask, combined_mask, dec_padding_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The @tf.function trace-compiles train_step into a TF graph for faster\n",
    "# execution. The function specializes to the precise shape of the argument\n",
    "# tensors. To avoid re-tracing due to the variable sequence lengths or variable\n",
    "# batch sizes (the last batch is smaller), use input_signature to specify\n",
    "# more generic shapes.\n",
    "\n",
    "train_step_signature = [\n",
    "    tf.TensorSpec(shape=(None, None), dtype=tf.int64),\n",
    "    tf.TensorSpec(shape=(None, None), dtype=tf.int64),\n",
    "]\n",
    "\n",
    "@tf.function(input_signature=train_step_signature)\n",
    "def train_step(inp, tar):\n",
    "    tar_inp = tar[:, :-1]\n",
    "    tar_real = tar[:, 1:]\n",
    "\n",
    "    enc_padding_mask, combined_mask, dec_padding_mask = create_masks(\n",
    "        inp, tar_inp)\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions, _ = transformer(inp, tar_inp,\n",
    "                                     True,\n",
    "                                     enc_padding_mask,\n",
    "                                     combined_mask,\n",
    "                                     dec_padding_mask)\n",
    "        loss = loss_function(tar_real, predictions)\n",
    "\n",
    "    gradients = tape.gradient(loss, transformer.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n",
    "\n",
    "    train_loss(loss)\n",
    "    train_accuracy(accuracy_function(tar_real, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {m: [] for m in ['loss', 'accuracy', 'duration']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if False:\n",
    "    for epoch in range(EPOCHS):\n",
    "        start = time.time()\n",
    "\n",
    "        train_loss.reset_states()\n",
    "        train_accuracy.reset_states()\n",
    "\n",
    "        # inp -> portuguese, tar -> english\n",
    "        for (batch, (inp, tar)) in enumerate(ds_trn):\n",
    "            train_step(inp, tar)\n",
    "\n",
    "            if batch % 50 == 0:\n",
    "                print('Epoch {} Batch {} Loss {:.4f} Accuracy {:.4f}'.format(\n",
    "                    epoch + 1, batch, train_loss.result(), train_accuracy.result()))\n",
    "\n",
    "        if (epoch + 1) % 5 == 0:\n",
    "            ckpt_save_path = ckpt_manager.save()\n",
    "            print('Saving checkpoint for epoch {} at {}'.format(epoch+1,\n",
    "                                                                ckpt_save_path))\n",
    "\n",
    "        print('Epoch {} Loss {:.4f} Accuracy {:.4f}'.format(epoch + 1,\n",
    "                                                            train_loss.result(),\n",
    "                                                            train_accuracy.result()))\n",
    "\n",
    "        print('Time taken for 1 epoch: {} secs\\n'.format(time.time() - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "with strategy.scope():\n",
    "    transformer = Transformer(num_layers, d_model, num_heads, dff,\n",
    "                              input_vocab_size, target_vocab_size,\n",
    "                              pe_input=10000,\n",
    "                              pe_target=10000,\n",
    "                              rate=dropout_rate)\n",
    "\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98,\n",
    "                                         epsilon=1e-9)\n",
    "\n",
    "    loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "        from_logits=True, reduction='none')\n",
    "\n",
    "    def loss_function(real, pred):\n",
    "        mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "        loss_ = loss_object(real, pred)\n",
    "\n",
    "        mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "        loss_ *= mask\n",
    "\n",
    "        return tf.reduce_sum(loss_)/tf.reduce_sum(mask)\n",
    "\n",
    "    def accuracy_function(real, pred):\n",
    "        accuracies = tf.equal(real, tf.argmax(pred, axis=2))\n",
    "\n",
    "        mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "        accuracies = tf.math.logical_and(mask, accuracies)\n",
    "\n",
    "        accuracies = tf.cast(accuracies, dtype=tf.float32)\n",
    "        mask = tf.cast(mask, dtype=tf.float32)\n",
    "        return tf.reduce_sum(accuracies)/tf.reduce_sum(mask)\n",
    "    \n",
    "    train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "    train_accuracy = tf.keras.metrics.Mean(name='train_accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(transformer, optimizer, loss_function, train_loss, ds_iter, steps_per_epoch):\n",
    "    def train_step_fn(inp, tar):\n",
    "        tar_inp = tar[:, :-1]\n",
    "        tar_real = tar[:, 1:]\n",
    "\n",
    "        enc_padding_mask, combined_mask, dec_padding_mask = create_masks(\n",
    "            inp, tar_inp)\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            predictions, _ = transformer(inp, tar_inp,\n",
    "                                         True,\n",
    "                                         enc_padding_mask,\n",
    "                                         combined_mask,\n",
    "                                         dec_padding_mask)\n",
    "            loss = loss_function(tar_real, predictions)\n",
    "\n",
    "        gradients = tape.gradient(loss, transformer.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n",
    "        \n",
    "        train_loss(loss)\n",
    "        \n",
    "    for _ in tf.range(steps_per_epoch):\n",
    "        strategy.run(train_step_fn, next(ds_iter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, train_loss: 0.0\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/tensorflow/python/data/ops/multi_device_iterator_ops.py:601: get_next_as_optional (from tensorflow.python.data.ops.iterator_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.Iterator.get_next_as_optional()` instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/tensorflow/python/data/ops/multi_device_iterator_ops.py:601: get_next_as_optional (from tensorflow.python.data.ops.iterator_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.Iterator.get_next_as_optional()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1, train_loss: 0.5655438899993896\n"
     ]
    }
   ],
   "source": [
    "ds_trn = get_ds(df_trn_attn[df_trn_attn.fold == 0], BATCH_SIZE)\n",
    "ds_trn_iter = iter(strategy.experimental_distribute_dataset(ds_trn))\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f'epoch: {epoch}, train_loss: {train_loss.result()}')\n",
    "    train_loss.reset_states()\n",
    "    epoch_start = time.perf_counter()\n",
    "    train_step(transformer, optimizer, loss_function, train_loss, ds_trn_iter, steps_per_epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "id": "5LF9JIpCDQs3"
   },
   "source": [
    "## Tensorflow Collaborative Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 66,
     "referenced_widgets": [
      "235676a015614259bc3d1372b7e0c020",
      "5663d015758f430caeb16b63ff43841e",
      "b1b6224e2bc2416bb344798c1add2537",
      "81d505ad30d34683b646252cc1e830cb",
      "c16536e2ef3e440da7f1fd3d6af0944b",
      "81a515c98c30497f81232d44e94cb123",
      "6f4ece440f634846a98fd6aa65585679",
      "c41cf7975b7740ecba4acda661d32434"
     ]
    },
    "hidden": true,
    "id": "Is6JkeYDd2qu",
    "outputId": "fbbcb863-58a8-4502-fa76-369b23e1a31b"
   },
   "outputs": [],
   "source": [
    "folds = range(40)\n",
    "\n",
    "if False:\n",
    "    df_data = pd.read_gbq((f\"\"\"\n",
    "        WITH data AS (\n",
    "        SELECT row_id, user_id, content_id, part, tags_array, answered_correctly,\n",
    "        ROW_NUMBER() OVER(PARTITION BY user_id, content_id ORDER BY timestamp DESC) row_num\n",
    "        FROM {DATASET}.train t\n",
    "        JOIN {DATASET}.folds f\n",
    "        ON t.user_id = f.user_id_s\n",
    "        JOIN {DATASET}.content_tags c\n",
    "        ON t.ql_id = c.ql_id\n",
    "        WHERE content_type_id = 0\n",
    "        AND t.task_container_id < f.task_container_id_min\n",
    "        AND f.fold in ({(', '.join(list(map(str, folds))))}) \n",
    "        )\n",
    "        SELECT row_id, user_id, content_id, part, tags_array, answered_correctly\n",
    "        FROM data\n",
    "        WHERE row_num = 1\n",
    "        \"\"\", '_q_'), use_bqstorage_api=True, progress_bar_type='tqdm_notebook')        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 66,
     "referenced_widgets": [
      "235676a015614259bc3d1372b7e0c020",
      "5663d015758f430caeb16b63ff43841e",
      "b1b6224e2bc2416bb344798c1add2537",
      "81d505ad30d34683b646252cc1e830cb",
      "c16536e2ef3e440da7f1fd3d6af0944b",
      "81a515c98c30497f81232d44e94cb123",
      "6f4ece440f634846a98fd6aa65585679",
      "c41cf7975b7740ecba4acda661d32434"
     ]
    },
    "hidden": true,
    "id": "Is6JkeYDd2qu",
    "outputId": "fbbcb863-58a8-4502-fa76-369b23e1a31b"
   },
   "outputs": [],
   "source": [
    "if False:\n",
    "    df_data = pd.read_gbq((f\"\"\"\n",
    "        WITH data AS (\n",
    "        SELECT row_id, user_id, content_id, answered_correctly,\n",
    "        ROW_NUMBER() OVER(PARTITION BY user_id, content_id ORDER BY timestamp DESC) row_num\n",
    "        FROM {DATASET}.train t\n",
    "        JOIN {DATASET}.folds f\n",
    "        ON t.user_id = f.user_id_s\n",
    "        WHERE content_type_id = 0\n",
    "        )\n",
    "        SELECT row_id, user_id, content_id, answered_correctly\n",
    "        FROM data\n",
    "        WHERE row_num = 1\n",
    "        \"\"\", '_q_'), use_bqstorage_api=True, progress_bar_type='tqdm_notebook')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 66,
     "referenced_widgets": [
      "5918d69949e443bbb43353bfb48fc803",
      "5f5799f25b1d4f4e853a48660007fadf",
      "09792541227541a1872ae6f20d18651f",
      "781442d17ed048b8a8b1602673ba0a48",
      "7877600264494ffe910f2edf1f20e8e4",
      "58fb8e31ffae410aa9788c36487ff2ab",
      "b20574a0dd89443d997334e4d5cec717",
      "e8bea655634949328e13aa6e52f87e34"
     ]
    },
    "hidden": true,
    "id": "M5WcPSPUaUup",
    "outputId": "a1f5264d-cd74-4182-813e-180ed5e922ca"
   },
   "outputs": [],
   "source": [
    "folds = [0,1]\n",
    "\n",
    "if True:\n",
    "    df_test = pd.read_gbq((f\"\"\"\n",
    "        SELECT row_id, user_id, content_id, part, tags_array, answered_correctly,\n",
    "        FROM {DATASET}.train t\n",
    "        JOIN {DATASET}.content_tags c\n",
    "        ON t.ql_id = c.ql_id\n",
    "        WHERE content_type_id = 0\n",
    "        AND fold in ({(', '.join(list(map(str, folds))))}) \n",
    "        ORDER BY user_id, content_id\n",
    "        \"\"\", '_q_'), use_bqstorage_api=True, progress_bar_type='tqdm_notebook')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "cln-oVZgrjqP"
   },
   "outputs": [],
   "source": [
    "# keeping out unseen users and questions to start with\n",
    "df_test = df_test[df_test.user_id.isin(df_data.user_id)\n",
    "                    & df_test.content_id.isin(df_data.content_id)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df_ct = bq_client.query('select * from data.content_tags where question_id is not null').to_dataframe()\n",
    "df_ct.question_id = df_ct.question_id.astype('category')\n",
    "df_ct = df_ct.set_index('question_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def fix_len(r):\n",
    "    new = np.repeat(188, 6)\n",
    "    new[range(len(r))] = r\n",
    "    return new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df_ct['tags'] = df_ct.tags_array.progress_apply(fix_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "all_tags = np.unique(np.concatenate(df_ct.tags_array))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "tag_cols = [f'tag_{t}' for t in range(6)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df_tags = pd.DataFrame(df_ct.tags.to_list(), columns=tag_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "xrx_aPFZjDcl"
   },
   "outputs": [],
   "source": [
    "cat_cols = ['user_id', 'content_id', 'part']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "PqLaN8kojXIk"
   },
   "outputs": [],
   "source": [
    "def categorify(df_data, df_test, df_tags, cat_cols, tag_cols):\n",
    "    df_data = df_data.merge(df_tags, how='left', left_on='content_id', right_index=True)\n",
    "    df_test = df_test.merge(df_tags, how='left', left_on='content_id', right_index=True)\n",
    "\n",
    "    for col in cat_cols:        \n",
    "        df_data[col] = df_data[col].astype('category')\n",
    "        df_test[col] = pd.Categorical(df_test[col], df_data[col].cat.categories)\n",
    "\n",
    "    for t in tag_cols:\n",
    "        df_data[t] = pd.Categorical(df_data[t], all_tags)\n",
    "        df_test[t] = pd.Categorical(df_test[t], all_tags)\n",
    "        \n",
    "    for col in cat_cols + tag_cols:\n",
    "        df_data[col] = df_data[col].cat.codes\n",
    "        df_test[col] = df_test[col].cat.codes\n",
    "    \n",
    "    train_cols = ['row_id'] + cat_cols + tag_cols + ['answered_correctly']\n",
    "\n",
    "    return df_data[train_cols].sample(frac=1, random_state=42), df_test[train_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "4JYmnD1zOTQU"
   },
   "outputs": [],
   "source": [
    "df_train, df_valid = categorify(df_data, df_test, df_tags, cat_cols, tag_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Dataset from DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "sRhx9yBlNeTR"
   },
   "outputs": [],
   "source": [
    "def get_ds(df, batch_size=1024, repeat=True):\n",
    "    row_id = df.pop('row_id')\n",
    "    y = df.pop('answered_correctly')   \n",
    "    ds = tf.data.Dataset.from_tensor_slices(df, y)\n",
    "    ds = ds.shuffle(int(5e6))\n",
    "    ds = ds.repeat() if repeat else ds\n",
    "    ds = ds.batch(batch_size)\n",
    "    return ds.prefetch(AUTO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Dataset from TFRecords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "HAU1InYGlA35"
   },
   "outputs": [],
   "source": [
    "def _int64_feature(value):\n",
    "    \"\"\"Returns an int64_list from a bool / enum / int / uint.\"\"\"\n",
    "    \n",
    "    if type(value) != type(list()):\n",
    "        value = [value]\n",
    "\n",
    "    return tf.train.Feature(int64_list=tf.train.Int64List(value=value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "JsI2kr57iB3u"
   },
   "outputs": [],
   "source": [
    "def serialize_example(rec):\n",
    "    \n",
    "    feature = {\n",
    "        'row_id': _int64_feature([rec[0]]),\n",
    "        'features': _int64_feature(list(rec[1:-1])),\n",
    "        'target': _int64_feature([rec[-1]])\n",
    "    }\n",
    "\n",
    "    return tf.train.Example(features=tf.train.Features(feature=feature)).SerializeToString()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "-8Pfgfg8luO4"
   },
   "outputs": [],
   "source": [
    "def parse_example(example, test=False):\n",
    "    features = {'row_id': tf.io.FixedLenFeature([], tf.int64),\n",
    "                'features': tf.io.FixedLenFeature([9], tf.int64),\n",
    "                'target': tf.io.FixedLenFeature([], tf.int64),\n",
    "                }\n",
    "\n",
    "    example = tf.io.parse_single_example(example, features)\n",
    "\n",
    "    return example['features'], example['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "-uck9GUtyl6e"
   },
   "outputs": [],
   "source": [
    "def get_ds_tfrec(split, batch_size=4096, repeat=True):\n",
    "    file_pat = f'gs://{BUCKET}/tfrecords/deep_collab-{split}*.tfrec'\n",
    "    options = tf.data.Options()\n",
    "    \n",
    "    ds = (tf.data.Dataset.list_files(file_pat, shuffle=True)\n",
    "          .with_options(options)\n",
    "          .interleave(tf.data.TFRecordDataset, num_parallel_calls=AUTO)\n",
    "          .map(parse_example, num_parallel_calls=AUTO)\n",
    "          )\n",
    "    \n",
    "    ds = ds.shuffle(int(1e6))\n",
    "    ds = ds.repeat() if repeat else ds\n",
    "    ds = ds.batch(batch_size)\n",
    "    return ds.prefetch(AUTO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "hidden": true,
    "id": "w1-Vca7vngs0",
    "outputId": "b39a5a33-0460-4f0f-b9fa-d5d839c43104"
   },
   "outputs": [],
   "source": [
    "parse_example(serialize_example(df_valid.head().to_numpy()[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "yPywLg_snI7E",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if False:\n",
    "\n",
    "    split_dict = {\n",
    "        'train': df_train,\n",
    "        'valid': df_valid\n",
    "    }\n",
    "\n",
    "    for s in split_dict:\n",
    "        df = split_dict[s]\n",
    "        n_files = len(df) // int(1e6)\n",
    "\n",
    "        for i, split in enumerate(np.array_split(df, n_files)):\n",
    "            out_path = f'gs://{BUCKET}/tfrecords'\n",
    "            filename = f'deep_collab-{s}-{i:02d}-{len(split)}.tfrec'\n",
    "            record_file = f'{out_path}/{filename}'\n",
    "\n",
    "            with tf.io.TFRecordWriter(record_file) as writer:\n",
    "                for rec in tqdm(split.to_numpy()):\n",
    "                    writer.write(serialize_example(rec))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "WDGUis7AXsGv"
   },
   "outputs": [],
   "source": [
    "embed_spec = {\n",
    "    'user_id': (343810, 600, 0),\n",
    "    'content_id': (13513, 300, 1),\n",
    "    'part': (7, 5, 2),\n",
    "    'tags': (189, 30, slice(3,9))\n",
    "    }\n",
    "\n",
    "class KnowledgeNet(tf.keras.Model):\n",
    "    def __init__(self, embed_spec, n_act, l2_reg=1e-6, **kwargs):\n",
    "        super(KnowledgeNet, self).__init__(**kwargs)\n",
    "        self.embed_spec = embed_spec\n",
    "        self.n_act = n_act\n",
    "        self.l2_reg = l2_reg\n",
    "        self.embed_dict = {}\n",
    "        \n",
    "        for k, v in self.embed_spec.items():\n",
    "            input_dim, output_dim, _ = v\n",
    "            self.embed_dict[k] = tf.keras.layers.Embedding(\n",
    "                input_dim, output_dim,\n",
    "                embeddings_initializer=\"he_normal\",\n",
    "                embeddings_regularizer=tf.keras.regularizers.l2(self.l2_reg),\n",
    "                name=f'{k}_embedding'\n",
    "            )\n",
    "                        \n",
    "        self.dense_1 = tf.keras.layers.Dense(\n",
    "            input_dim=sum([v[0] for v in self.embed_spec.values()]), units=n_act, activation='relu', use_bias=True,\n",
    "            kernel_initializer='glorot_uniform',\n",
    "            bias_initializer='zeros', kernel_regularizer=tf.keras.regularizers.l2(self.l2_reg),\n",
    "            bias_regularizer=None, activity_regularizer=None, kernel_constraint=None,\n",
    "            bias_constraint=None, name='dense_1'\n",
    "        )\n",
    "        \n",
    "        self.dense_2 = tf.keras.layers.Dense(\n",
    "            input_dim=n_act, units=1, activation='sigmoid', use_bias=True,\n",
    "            kernel_initializer='glorot_uniform',\n",
    "            bias_initializer='zeros', kernel_regularizer=tf.keras.regularizers.l2(self.l2_reg),\n",
    "            bias_regularizer=None, activity_regularizer=None, kernel_constraint=None,\n",
    "            bias_constraint=None, name='dense_1'\n",
    "        )\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        embeds = [self.embed_dict[k](inputs[:,v[-1]]) for k, v in self.embed_spec.items() if k != 'tags']\n",
    "        embed_tags = self.embed_dict['tags'](inputs[:,self.embed_spec['tags'][-1]])\n",
    "        embed_tags_agg = tf.reduce_prod(embed_tags, axis=1)\n",
    "        x = self.dense_1(tf.concat(embeds + [embed_tags_agg], axis=1))        \n",
    "        return self.dense_2(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "zLBSnJdZQrPZ"
   },
   "outputs": [],
   "source": [
    "with strategy.scope():\n",
    "    model = KnowledgeNet(embed_spec, 1024)\n",
    "    opt = tf.keras.optimizers.Adam(lr=.0001)\n",
    "    loss_fn = tf.keras.losses.BinaryCrossentropy()\n",
    "    metrics = ['binary_accuracy', 'AUC']\n",
    "    model.compile(loss=loss_fn, optimizer=opt, metrics=metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "epochs = 2\n",
    "batch_size = 2048 * strategy.num_replicas_in_sync\n",
    "split_counts = {'train': 0, 'valid': 0}\n",
    "\n",
    "for s in split_counts:\n",
    "    for b in bucket.list_blobs(prefix=f'tfrecords/deep_collab-{s}'):\n",
    "        split_counts[s] += int(b.name.split('-')[3].split('.')[0])\n",
    "\n",
    "steps_per_epoch = split_counts['train'] // batch_size\n",
    "val_steps = split_counts['valid'] // batch_size\n",
    "total_steps = steps_per_epoch * epochs\n",
    "\n",
    "split_counts, steps_per_epoch, val_steps, total_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "if False:\n",
    "    lr_finder = LRFinder(start=1e-7, end=0.1, steps=100)\n",
    "\n",
    "    with strategy.scope():\n",
    "        model.save_weights(f'gs://{BUCKET}/temp/temp-weights.ckpt')\n",
    "        model.fit(get_ds_tfrec('train', batch_size), steps_per_epoch=lr_finder.steps, callbacks=[lr_finder])\n",
    "        model.load_weights(f'gs://{BUCKET}/temp/temp-weights.ckpt')\n",
    "\n",
    "    x = [lr_finder.start * (lr_finder.end / lr_finder.start) ** (s/lr_finder.steps) for s in range(lr_finder.steps)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "px.line(x=x, y=lr_finder.losses, log_x=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "reference_lr = .0001\n",
    "print(f'max_lr: {reference_lr:1.2e}')\n",
    "\n",
    "one_cycle_kwargs = dict(\n",
    "    total_steps=total_steps,\n",
    "    steps_up_pct=0.3,\n",
    "    steps_across_pct=.01,\n",
    "    steps_down_pct=0.6,\n",
    "    lr_max=reference_lr,\n",
    "    lr_start_factor=1/25,\n",
    "    lr_end_factor=1/10,\n",
    "    decay=0.9999,\n",
    "    mo_max=0.95,\n",
    "    mo_min=0.85,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "\n",
    "one_cycle = OneCycleScheduler(**one_cycle_kwargs)\n",
    "plot_lr_sched(one_cycle, total_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "hidden": true,
    "id": "8fjC6m_pZJvg",
    "outputId": "6cbc5eaf-8efa-42c1-fc1d-58ce3641d224",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    get_ds_tfrec('train', batch_size),\n",
    "    steps_per_epoch=steps_per_epoch,\n",
    "    epochs=epochs,\n",
    "    validation_data=get_ds_tfrec('valid', batch_size),\n",
    "    validation_steps=val_steps,\n",
    "    callbacks=[one_cycle],\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "hidden": true,
    "id": "YEcjUsrxMQZY",
    "outputId": "ad355b54-db88-495a-f562-b8f067a8c311"
   },
   "outputs": [],
   "source": [
    "model.save(f'gs://{BUCKET}/models/deep_collab_model', include_optimizer=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "Ec5donekKQDV"
   },
   "outputs": [],
   "source": [
    "model = tf.saved_model.load(f'gs://{BUCKET}/models/collab_model_all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "zyHpMo71eRwI"
   },
   "outputs": [],
   "source": [
    "weights = {c: df_data[c].cat.categories for c in cat_cols}\n",
    "for v in model.variables:\n",
    "    name = v.name.split('/')[1]\n",
    "    weights[name] = v.numpy()\n",
    "    weights[name] = np.append(weights[name], np.mean(weights[name], axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "UqfCLFD2NXm0"
   },
   "outputs": [],
   "source": [
    "def get_pred(u_code, c_code, logits=False):\n",
    "    user_vector = weights['user_embedding'][u_code]\n",
    "    user_bias = weights['user_bias'][u_code]\n",
    "    question_vector = weights['question_embedding'][c_code]\n",
    "    question_bias = weights['question_bias'][c_code]\n",
    "    \n",
    "    logit = np.squeeze((user_vector * question_vector).sum() + user_bias + question_bias)\n",
    "    \n",
    "    if logits:\n",
    "        return logit\n",
    "    else:\n",
    "        return 1 / (1 + np.math.exp(-logit))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "9NwjCQwrGLsB"
   },
   "outputs": [],
   "source": [
    "np.save('weights_all.npy', weights, allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "9yyrQvasfCal"
   },
   "outputs": [],
   "source": [
    "weights = np.load('weights_all.npy', allow_pickle=True).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "VbbYs_vFJJmt"
   },
   "outputs": [],
   "source": [
    "bucket.blob('weights_all.npy').upload_from_filename('weights_all.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "617fSnLWODtU"
   },
   "outputs": [],
   "source": [
    "def get_code_cols(df):\n",
    "    return pd.concat([df[col].cat.codes for col in cat_cols], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "hKR7gZKvOjea"
   },
   "outputs": [],
   "source": [
    "df_cat_cols = get_code_cols(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "YD11KSjKPEgR"
   },
   "outputs": [],
   "source": [
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "hidden": true,
    "id": "NcFqgsUfQFK0",
    "outputId": "2e0c6a87-1552-47e4-edc4-59b7b5796e0e"
   },
   "outputs": [],
   "source": [
    "get_pred(43124, 9276, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 66,
     "referenced_widgets": [
      "488f106b90e64e509091124e15ac86ea",
      "7371a2da83c047fc921425b910048107",
      "8c277c0d926c4c2ebb53cdf71e2df986",
      "0ea64c8534aa4184b0c7c3eba217aec0",
      "74fdbb0ef57a46e4b589a46c18ec042d",
      "08df8883af494b70b718a7ce5b4f9386",
      "011d43b43aed485a8751ec48d9d95bab",
      "d5e4c6e070c0413491222f9edf1bba2c"
     ]
    },
    "hidden": true,
    "id": "0ExuXHnpO1om",
    "outputId": "d8021d2d-077a-42a7-a0b7-c7315d02c663"
   },
   "outputs": [],
   "source": [
    "df_preds = df_cat_cols.progress_apply(lambda r: get_pred(*r, logits=True), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "hidden": true,
    "id": "KNDUJDYRX7kT",
    "outputId": "76a4ad3c-6563-41fc-c049-9a9449a3eebe"
   },
   "outputs": [],
   "source": [
    "roc_auc_score(df_valid.answered_correctly, df_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 66,
     "referenced_widgets": [
      "da20e26860b04d3f9b7aec2866f1a43c",
      "7dc3eef3c43b468093264474aec72870",
      "1068312cf5104622af904af5507ffdfb",
      "014d6e6088084f31ac20b506f8e9d0a6",
      "46f7c96c423f43938916d9f3ff8f8d59",
      "68b08e46b6b94cb7907dd3259deeead1",
      "0036b023c2224f458414b8053f42ee55",
      "c835dd42207646a1a06a02725c3732c6"
     ]
    },
    "hidden": true,
    "id": "x6NcnS49Y_6I",
    "outputId": "1764c374-4730-4723-bdb4-6e647ad5b269"
   },
   "outputs": [],
   "source": [
    "folds = range(5)\n",
    "\n",
    "df_test_all = pd.read_gbq((f\"\"\"\n",
    "    SELECT row_id, user_id, content_id, answered_correctly,\n",
    "    FROM data.train t\n",
    "    WHERE content_type_id = 0\n",
    "    AND fold in ({(', '.join(list(map(str, folds))))}) \n",
    "    ORDER BY user_id, content_id\n",
    "    \"\"\", '_q_'), use_bqstorage_api=True, progress_bar_type='tqdm_notebook')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "_j5twpQOkZS5"
   },
   "outputs": [],
   "source": [
    "df_test_all.to_pickle('df_test_all.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "8ByKunS2c0km"
   },
   "outputs": [],
   "source": [
    "for c in cat_cols:\n",
    "    df_test_all[c] = pd.Categorical(df_test_all[c], categories=weights[c])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "fsJQRQmikmoP"
   },
   "outputs": [],
   "source": [
    "df_code_cols = get_code_cols(df_test_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 66,
     "referenced_widgets": [
      "084f2d9358df47df8cdb1874c203fcb8",
      "214f3fcaeb4d49e0a47be36f3c6f854f",
      "f25723a2fc8f4192b8b3adde2cd0d70f",
      "e96d00e7c2c44755866ebbbf77b5af42",
      "9e709767b51a4e5ebfbaa5d747c4ad4f",
      "c9e63655621b4e12ad8bcac2712ca812",
      "4ad22a4a79e74753b9fedac8e2b469ea",
      "d5a3b39de6d3404d910394ddf6439548"
     ]
    },
    "hidden": true,
    "id": "hlbeMSIsk0cB",
    "outputId": "ac809c5f-f586-4aba-f56a-be677f9bcd28"
   },
   "outputs": [],
   "source": [
    "preds_all = df_code_cols.progress_apply(lambda r: get_pred(*r, logits=True), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "eQJh448ln1rU"
   },
   "outputs": [],
   "source": [
    "df_test_all['pred_collab_logit'] = preds_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "cUvY2NJloYTZ"
   },
   "outputs": [],
   "source": [
    "df_test_all[['row_id', 'pred_collab', 'pred_collab_logit']].to_pickle('df_pred_collab.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "1k0G3pKmo2Eo"
   },
   "outputs": [],
   "source": [
    "bucket.blob('df_pred_collab.pkl').upload_from_filename('df_pred_collab.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "hidden": true,
    "id": "nQBGy_BMnsn3",
    "outputId": "94b6e2c9-058f-496e-ecf9-caee9d9555eb"
   },
   "outputs": [],
   "source": [
    "roc_auc_score(df_test_all.answered_correctly, preds_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tZHDa7HT8m_V"
   },
   "source": [
    "## Custom Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5g4oCKSpPc5-"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE_PER_REPLICA = 64\n",
    "BATCH_SIZE_GLOBAL = BATCH_SIZE_PER_REPLICA * strategy.num_replicas_in_sync\n",
    "\n",
    "ds_trn_fit_iter = iter(strategy.experimental_distribute_dataset(get_ds_tfrec('train', batch_size=BATCH_SIZE_GLOBAL)))\n",
    "ds_val_fit_iter = iter(strategy.experimental_distribute_dataset(get_ds_tfrec('valid', batch_size=BATCH_SIZE_GLOBAL)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CNLJME5WQFcD"
   },
   "outputs": [],
   "source": [
    "with strategy.scope():\n",
    "    model = KnowledgeNet(n_users, n_questions, EMBEDDING_SIZE)\n",
    "    \n",
    "    loss_fn = tf.keras.losses.BinaryCrossentropy(reduction=tf.keras.losses.Reduction.NONE)\n",
    "    \n",
    "    def compute_loss(targets, outputs):\n",
    "        per_example_loss = loss_fn(targets, outputs)\n",
    "        return tf.nn.compute_average_loss(per_example_loss, global_batch_size=BATCH_SIZE_GLOBAL)\n",
    "    \n",
    "    optimizer = tf.keras.optimizers.Adam(lr=0.0001)\n",
    "    # metrics_trn = [tf.keras.metrics.Mean(name='loss'), tf.keras.metrics.BinaryAccuracy('accuracy'), tf.keras.metrics.AUC(name='roc_auc')]\n",
    "    # metrics_val = [tf.keras.metrics.Mean(name='val_loss'), tf.keras.metrics.BinaryAccuracy('val_accuracy'), tf.keras.metrics.AUC(name='val_roc_auc')]\n",
    "    metrics_trn = [tf.keras.metrics.Mean(name='loss')]\n",
    "    metrics_val = [tf.keras.metrics.Mean(name='val_loss')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HMDIN4jvFLmc"
   },
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(model, optimizer, loss_fn, metrics, ds_iter, steps_per_epoch):\n",
    "    def train_step_fn(inputs, targets):\n",
    "        with tf.GradientTape() as tape:\n",
    "            outputs = model(inputs, training=True)\n",
    "            loss = loss_fn(targets, outputs)\n",
    "                            \n",
    "        grads = tape.gradient(loss, model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "        \n",
    "        for i, metric in enumerate(metrics):\n",
    "            if i == 0:\n",
    "                metric.update_state(loss)\n",
    "            else:\n",
    "                metric.update_state(targets, outputs)\n",
    "    \n",
    "    for _ in tf.range(steps_per_epoch):\n",
    "        strategy.run(train_step_fn, next(ds_iter))\n",
    "        \n",
    "@tf.function\n",
    "def val_step(model, loss_fn, metrics, ds_iter, steps_per_epoch):\n",
    "    def val_step_fn(inputs, targets):\n",
    "        outputs = model(inputs, training=False)\n",
    "        loss = loss_fn(targets, outputs)\n",
    "        \n",
    "        for i, metric in enumerate(metrics):\n",
    "            if i == 0:\n",
    "                metric.update_state(loss)\n",
    "            else:\n",
    "                metric.update_state(targets, outputs)\n",
    "    \n",
    "    for _ in tf.range(steps_per_epoch):\n",
    "        strategy.run(val_step_fn, next(ds_iter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uyqLBApERUCJ",
    "outputId": "714bcfba-3f5c-423d-ace5-36f320e712dd"
   },
   "outputs": [],
   "source": [
    "epochs = 5\n",
    "steps_per_epoch = 10000\n",
    "val_steps = 100\n",
    "results = {metric.name: [] for metric in metrics_trn + metrics_val}\n",
    "results['duration'] = []\n",
    "\n",
    "start = time.perf_counter()\n",
    "for epoch in range(epochs):\n",
    "    epoch_start = time.perf_counter()\n",
    "    train_step(model, optimizer, compute_loss, metrics_trn, ds_trn_fit_iter, steps_per_epoch)    \n",
    "    val_step(model, compute_loss, metrics_val, ds_val_fit_iter, val_steps)\n",
    "    \n",
    "    for metric in metrics_trn + metrics_val:\n",
    "        results[metric.name].append(metric.result().numpy())\n",
    "        metric.reset_states()\n",
    "        \n",
    "    results['duration'].append((time.perf_counter() - epoch_start))\n",
    "    \n",
    "    print(f'epoch {epoch:02d} - ', (', ').join([f'{k}: {v[-1]:0.4f}' for k,v in results.items()]))\n",
    "\n",
    "end = time.perf_counter()\n",
    "seconds = end - start\n",
    "minutes = seconds / 60 + (seconds % 60) / 60\n",
    "print(f'Total duration: {minutes:0.1f} minutes - {seconds / epochs:0.1f} seconds per epoch')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "collaborative_filtering.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "environment": {
   "name": "tf2-2-3-gpu.2-3.m59",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-2-3-gpu.2-3:m59"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0036b023c2224f458414b8053f42ee55": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "011d43b43aed485a8751ec48d9d95bab": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "014d6e6088084f31ac20b506f8e9d0a6": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c835dd42207646a1a06a02725c3732c6",
      "placeholder": "​",
      "style": "IPY_MODEL_0036b023c2224f458414b8053f42ee55",
      "value": " 6478025/6478025 [24:49&lt;00:00, 4350.54rows/s]"
     }
    },
    "084f2d9358df47df8cdb1874c203fcb8": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_f25723a2fc8f4192b8b3adde2cd0d70f",
       "IPY_MODEL_e96d00e7c2c44755866ebbbf77b5af42"
      ],
      "layout": "IPY_MODEL_214f3fcaeb4d49e0a47be36f3c6f854f"
     }
    },
    "08df8883af494b70b718a7ce5b4f9386": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "09792541227541a1872ae6f20d18651f": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Downloading: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_58fb8e31ffae410aa9788c36487ff2ab",
      "max": 1294272,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_7877600264494ffe910f2edf1f20e8e4",
      "value": 1294272
     }
    },
    "0ea64c8534aa4184b0c7c3eba217aec0": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d5e4c6e070c0413491222f9edf1bba2c",
      "placeholder": "​",
      "style": "IPY_MODEL_011d43b43aed485a8751ec48d9d95bab",
      "value": " 1049559/1049559 [1:09:12&lt;00:00, 252.76it/s]"
     }
    },
    "1068312cf5104622af904af5507ffdfb": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Downloading: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_68b08e46b6b94cb7907dd3259deeead1",
      "max": 6478025,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_46f7c96c423f43938916d9f3ff8f8d59",
      "value": 6478025
     }
    },
    "214f3fcaeb4d49e0a47be36f3c6f854f": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "235676a015614259bc3d1372b7e0c020": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_b1b6224e2bc2416bb344798c1add2537",
       "IPY_MODEL_81d505ad30d34683b646252cc1e830cb"
      ],
      "layout": "IPY_MODEL_5663d015758f430caeb16b63ff43841e"
     }
    },
    "46f7c96c423f43938916d9f3ff8f8d59": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "488f106b90e64e509091124e15ac86ea": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_8c277c0d926c4c2ebb53cdf71e2df986",
       "IPY_MODEL_0ea64c8534aa4184b0c7c3eba217aec0"
      ],
      "layout": "IPY_MODEL_7371a2da83c047fc921425b910048107"
     }
    },
    "4ad22a4a79e74753b9fedac8e2b469ea": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "5663d015758f430caeb16b63ff43841e": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "58fb8e31ffae410aa9788c36487ff2ab": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5918d69949e443bbb43353bfb48fc803": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_09792541227541a1872ae6f20d18651f",
       "IPY_MODEL_781442d17ed048b8a8b1602673ba0a48"
      ],
      "layout": "IPY_MODEL_5f5799f25b1d4f4e853a48660007fadf"
     }
    },
    "5f5799f25b1d4f4e853a48660007fadf": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "68b08e46b6b94cb7907dd3259deeead1": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6f4ece440f634846a98fd6aa65585679": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "7371a2da83c047fc921425b910048107": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "74fdbb0ef57a46e4b589a46c18ec042d": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "781442d17ed048b8a8b1602673ba0a48": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e8bea655634949328e13aa6e52f87e34",
      "placeholder": "​",
      "style": "IPY_MODEL_b20574a0dd89443d997334e4d5cec717",
      "value": " 1294272/1294272 [03:30&lt;00:00, 6138.32rows/s]"
     }
    },
    "7877600264494ffe910f2edf1f20e8e4": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "7dc3eef3c43b468093264474aec72870": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "81a515c98c30497f81232d44e94cb123": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "81d505ad30d34683b646252cc1e830cb": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c41cf7975b7740ecba4acda661d32434",
      "placeholder": "​",
      "style": "IPY_MODEL_6f4ece440f634846a98fd6aa65585679",
      "value": " 5340377/5340377 [03:45&lt;00:00, 23656.96rows/s]"
     }
    },
    "8c277c0d926c4c2ebb53cdf71e2df986": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_08df8883af494b70b718a7ce5b4f9386",
      "max": 1049559,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_74fdbb0ef57a46e4b589a46c18ec042d",
      "value": 1049559
     }
    },
    "9e709767b51a4e5ebfbaa5d747c4ad4f": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "b1b6224e2bc2416bb344798c1add2537": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Downloading: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_81a515c98c30497f81232d44e94cb123",
      "max": 5340377,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_c16536e2ef3e440da7f1fd3d6af0944b",
      "value": 5340377
     }
    },
    "b20574a0dd89443d997334e4d5cec717": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "c16536e2ef3e440da7f1fd3d6af0944b": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "c41cf7975b7740ecba4acda661d32434": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c835dd42207646a1a06a02725c3732c6": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c9e63655621b4e12ad8bcac2712ca812": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d5a3b39de6d3404d910394ddf6439548": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d5e4c6e070c0413491222f9edf1bba2c": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "da20e26860b04d3f9b7aec2866f1a43c": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_1068312cf5104622af904af5507ffdfb",
       "IPY_MODEL_014d6e6088084f31ac20b506f8e9d0a6"
      ],
      "layout": "IPY_MODEL_7dc3eef3c43b468093264474aec72870"
     }
    },
    "e8bea655634949328e13aa6e52f87e34": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e96d00e7c2c44755866ebbbf77b5af42": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d5a3b39de6d3404d910394ddf6439548",
      "placeholder": "​",
      "style": "IPY_MODEL_4ad22a4a79e74753b9fedac8e2b469ea",
      "value": " 6478025/6478025 [01:54&lt;00:00, 56744.46it/s]"
     }
    },
    "f25723a2fc8f4192b8b3adde2cd0d70f": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c9e63655621b4e12ad8bcac2712ca812",
      "max": 6478025,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_9e709767b51a4e5ebfbaa5d747c4ad4f",
      "value": 6478025
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
