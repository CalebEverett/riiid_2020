{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# <hide-input>\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "code_folding": [],
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1gbbiyXJdj4j",
    "outputId": "9e57da8c-42bf-4079-e720-3b73d17b94bc"
   },
   "outputs": [],
   "source": [
    "# <hide-input>\n",
    "from datetime import datetime\n",
    "from functools import partial\n",
    "import gc\n",
    "import json\n",
    "import math\n",
    "import os\n",
    "from pathlib import Path\n",
    "import re\n",
    "import subprocess\n",
    "import sys\n",
    "import time\n",
    "\n",
    "import ipywidgets as widgets\n",
    "from google.cloud import storage, bigquery\n",
    "from google.cloud.bigquery import SchemaField\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "\n",
    "AUTO = tf.data.experimental.AUTOTUNE\n",
    "BUCKET = 'caleb-riiid'\n",
    "DATASET = 'data'\n",
    "LOCATION = 'europe-west4'\n",
    "KAGGLE_SUBMIT_DATASET = 'riiid-submission-private'\n",
    "PROJECT = 'fastai-caleb'\n",
    "REPO = 'riiid_2020'\n",
    "NOT_KAGGLE = os.getenv('KAGGLE_URL_BASE') is None\n",
    "\n",
    "# if NOT_KAGGLE:\n",
    "#     from google.colab import drive\n",
    "#     DRIVE = Path('/content/drive/My Drive')\n",
    "#     if not DRIVE.exists():\n",
    "#         drive.mount(str(DRIVE.parent))\n",
    "#     sys.path.append(str(DRIVE))\n",
    "#     g_creds_path = 'credentials/riiid-caleb-faddd0c9d900.json'\n",
    "#     os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = str(DRIVE/g_creds_path)\n",
    "\n",
    "bucket = storage.Client(project=PROJECT).get_bucket(BUCKET)\n",
    "dataset = bigquery.Dataset(f'{PROJECT}.{DATASET}')\n",
    "bq_client = bigquery.Client(project=PROJECT, location=LOCATION)\n",
    "\n",
    "if NOT_KAGGLE:\n",
    "    CONFIG = json.loads(bucket.get_blob('config.json').download_as_string())\n",
    "    os.environ = {**os.environ, **CONFIG}\n",
    "    sys.path.append('/home/jupyter')\n",
    "    from riiid_2020.bqhelpers import BQHelper\n",
    "    from riiid_2020.queries import Queries\n",
    "\n",
    "#     from comet_ml import APIExperiment, Experiment\n",
    "#     from kaggle.api.kaggle_api_extended import KaggleApi\n",
    "#     kaggle_api = KaggleApi()\n",
    "#     kaggle_api.authenticate()\n",
    "\n",
    "# import plotly\n",
    "# import plotly.express as px\n",
    "# import plotly.graph_objects as go\n",
    "# from plotly.subplots import make_subplots\n",
    "# from sklearn.metrics import roc_auc_score\n",
    "# from sklearn.preprocessing import MultiLabelBinarizer\n",
    "# # pd.options.plotting.backend = 'plotly'\n",
    "# tqdm.pandas()\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class LRFinder(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, start=1e-7, end=5, steps=100):\n",
    "        self.losses = []\n",
    "        self.start = start\n",
    "        self.end = end\n",
    "        self.steps = steps\n",
    "        self.best_loss = np.inf\n",
    "\n",
    "    def on_batch_begin(self, step, logs):\n",
    "        scheduled_lr = self.start * (self.end / self.start) ** (step/self.steps)\n",
    "        tf.keras.backend.set_value(self.model.optimizer.lr, scheduled_lr)\n",
    "\n",
    "    def on_batch_end(self, step, logs):\n",
    "        loss = logs.get('loss')\n",
    "        self.losses.append(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class OneCycleScheduler(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, total_steps=1000, steps_up_pct=0.3, steps_across_pct=.01, steps_down_pct=0.6, lr_max=.001,\n",
    "              lr_start_factor=.00003, lr_end_factor=.00001, decay=0.93,\n",
    "              mo_max=0.95, mo_min=0.85, verbose=1):\n",
    "        \n",
    "        self.step = -1\n",
    "        self.epoch = -1\n",
    "\n",
    "        super(OneCycleScheduler, self).__init__()\n",
    "\n",
    "        def one_cycle(step):\n",
    "            \n",
    "            steps_up = int(total_steps * steps_up_pct)\n",
    "            steps_across = int(total_steps * steps_across_pct)\n",
    "            steps_down = int(total_steps * steps_down_pct)\n",
    "            lr_start = lr_max * lr_start_factor\n",
    "            lr_end = lr_max * lr_end_factor\n",
    "\n",
    "            if step <= steps_up:\n",
    "                new_lr = (lr_max - lr_start)/2  * (-math.cos((math.pi * step) / steps_up) + 1) + lr_start\n",
    "                new_mo = (mo_max - mo_min)/2  * (math.cos((math.pi * step) / steps_up) + 1) + mo_min\n",
    "            \n",
    "            elif step <= (steps_up + steps_across):\n",
    "                new_lr = lr_max\n",
    "                new_mo = mo_min\n",
    "            \n",
    "            elif step <= (steps_up + steps_across + steps_down):\n",
    "                down_step = step - steps_across - steps_up\n",
    "                new_lr = (lr_max - lr_end)/2  * (math.cos((math.pi * down_step) / steps_down) + 1) + lr_end\n",
    "                new_mo = (mo_max - mo_min)/2  * (-math.cos((math.pi * down_step) / steps_down) + 1) + mo_min\n",
    "\n",
    "            else:\n",
    "                new_lr = lr_end * decay**(step - steps_up - steps_across - steps_down)\n",
    "                new_mo = mo_max\n",
    "            \n",
    "            return new_lr, new_mo\n",
    "\n",
    "        self.schedule = one_cycle\n",
    "        self.verbose = verbose\n",
    " \n",
    "    def on_batch_begin(self, step, logs):\n",
    "        self.step +=1\n",
    "        scheduled_lr, scheduled_mo = self.schedule(self.step)\n",
    "        tf.keras.backend.set_value(self.model.optimizer.lr, scheduled_lr)\n",
    "        tf.keras.backend.set_value(self.model.optimizer.beta_1, scheduled_mo)\n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs):\n",
    "        if self.verbose:\n",
    "            scheduled_lr, scheduled_mo = self.schedule(self.step)\n",
    "            auc_roc = logs.get('val_auc_roc')\n",
    "            auc_roc = auc_roc if auc_roc is not None else 0\n",
    "            print(f'\\nepoch {epoch+1:02d}: val_auc_roc={auc_roc:0.4f}, learning_rate={scheduled_lr:0.2e}, beta_1={scheduled_mo:0.3f}')\n",
    "            \n",
    "def plot_lr_sched(one_cycle, total_steps):\n",
    "    fig = make_subplots(specs=[[{\"secondary_y\": True}]])\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=list(range(total_steps)),\n",
    "                   y=[one_cycle.schedule(e)[0] for e in range(total_steps)],\n",
    "                   name=\"lr\"),\n",
    "        secondary_y=False,\n",
    "    )\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=list(range(total_steps)),\n",
    "                   y=[one_cycle.schedule(e)[1] for e in range(total_steps)],\n",
    "                   name=\"mom\"),\n",
    "        secondary_y=True,\n",
    "    )\n",
    "\n",
    "    fig.update_layout(title_text=\"Learning Rate Schedule\")\n",
    "    fig.update_xaxes(title_text=\"steps\")\n",
    "    fig.update_yaxes(title_text=\"learning rate\", secondary_y=False)\n",
    "    fig.update_yaxes(title_text=\"momentum\", secondary_y=True)\n",
    "\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "u8ltlKOHp4CR",
    "outputId": "b96cbd3a-68d0-47b3-fd7a-0f1d7c88d9b4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on TPU  grpc://10.46.150.234:8470\n",
      "INFO:tensorflow:Initializing the TPU system: tpu-4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Initializing the TPU system: tpu-4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Clearing out eager caches\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Clearing out eager caches\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Finished initializing TPU system.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Finished initializing TPU system.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Found TPU system:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Found TPU system:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Num TPU Cores: 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Num TPU Cores: 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Num TPU Workers: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Num TPU Workers: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
     ]
    }
   ],
   "source": [
    "def get_strategy():\n",
    "\n",
    "    try:\n",
    "        tpu = tf.distribute.cluster_resolver.TPUClusterResolver('tpu-4')\n",
    "        print('Running on TPU ', tpu.master())\n",
    "    except:\n",
    "        tpu = None\n",
    "\n",
    "    if tpu:\n",
    "        tf.config.experimental_connect_to_cluster(tpu)\n",
    "        tf.tpu.experimental.initialize_tpu_system(tpu)\n",
    "        strategy = tf.distribute.experimental.TPUStrategy(tpu)\n",
    "    \n",
    "    else:\n",
    "        strategy = tf.distribute.get_strategy()\n",
    "        for d in tf.config.list_physical_devices():\n",
    "            print(d)\n",
    "            \n",
    "    return strategy\n",
    "\n",
    "strategy = get_strategy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtypes = {\n",
    "    'row_id': 'int64',\n",
    "    'timestamp': 'int64',\n",
    "    'user_id': 'int32',\n",
    "    'content_id': 'int16',\n",
    "    'content_type_id': 'int8',\n",
    "    'task_container_id': 'int16',\n",
    "    'user_answer': 'int8',\n",
    "    'answered_correctly': 'int8',\n",
    "    'prior_question_elapsed_time': 'float32',\n",
    "    'prior_question_had_explanation': 'bool',\n",
    "    'tid_orig': 'int16'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "bqh = BQHelper(bucket, DATASET, bq_client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "folds = range(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trn_attn = bqh.get_df_query_bqs(f\"\"\"\n",
    "    WITH t AS (\n",
    "        SELECT fold, timestamp, user_id, content_id + 1 content_id, answered_correctly + 1 answered_correctly,\n",
    "        MAX(task_container_id) OVER(PARTITION BY user_id) tid_max\n",
    "        FROM data.train\n",
    "        WHERE content_type_id = 0\n",
    "    )\n",
    "    SELECT fold, timestamp, user_id, content_id, answered_correctly\n",
    "    FROM t\n",
    "    WHERE tid_max > 9\n",
    "    AND fold in ({(',').join(map(str,folds))});\n",
    "\"\"\",'df_trn_attn.pkl', from_bq=False, dtypes=dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def get_seq(exer, resp, max_seq_len=16):\n",
    "    pad_len = tf.maximum(max_seq_len - tf.shape(exer)[0], 0)\n",
    "    t_exer = tf.concat([tf.zeros(pad_len, dtype=tf.int64), tf.constant(exer[:max_seq_len], dtype=tf.int64)], axis=0)\n",
    "    t_resp = tf.concat([tf.zeros(pad_len, dtype=tf.int64), tf.constant(resp[:max_seq_len], dtype=tf.int64)], axis=0)\n",
    "    return t_exer, t_resp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def seq_fix_len(exer, resp, len_seq=100):\n",
    "    pad = tf.maximum(len_seq - tf.shape(exer)[0], 0)\n",
    "    exer = tf.pad(exer[-len_seq:], [[pad, 0]])\n",
    "    resp = tf.pad(resp[-len_seq:], [[pad, 0]])\n",
    "    return exer, resp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def add_mask_to_batch(exer, resp):\n",
    "    seq = tf.cast(tf.math.equal(exer, 0), tf.float32)\n",
    "    mask_pad = seq[:, tf.newaxis, tf.newaxis, :]\n",
    "        \n",
    "    seq_size = tf.shape(resp)[1]\n",
    "    mask_future = 1 - tf.linalg.band_part(tf.ones((seq_size, seq_size)), -1, 0)\n",
    "    \n",
    "    mask_current = tf.maximum(mask_pad, mask_future)\n",
    "    mask_prior = tf.maximum(mask_pad[:, :, :, :-1], mask_future[:-1, :-1])\n",
    "    \n",
    "    label = tf.cast(tf.math.equal(resp[:, -1], 2), tf.float32)\n",
    "    \n",
    "    resp_prior = tf.identity(resp)[:, :-1]\n",
    "            \n",
    "    return exer, resp_prior, mask_current, mask_prior, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TFRecords "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _int64_feature(value):\n",
    "    \"\"\"Returns an int64_list from a bool / enum / int / uint.\"\"\"\n",
    "    \n",
    "    if type(value) != type(list()):\n",
    "        value = [value]\n",
    "\n",
    "    return tf.train.Feature(int64_list=tf.train.Int64List(value=value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def serialize_example(group):\n",
    "    \n",
    "    feature = {\n",
    "        'content_id': _int64_feature(list(group[0])),\n",
    "        'answered_correctly': _int64_feature(list(group[1]))\n",
    "    }\n",
    "\n",
    "    return tf.train.Example(features=tf.train.Features(feature=feature)).SerializeToString()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_example(example, len_seq_max=20):\n",
    "    \n",
    "    features = {'content_id': tf.io.VarLenFeature(tf.int64),\n",
    "                'answered_correctly': tf.io.VarLenFeature(tf.int64)\n",
    "                }\n",
    "\n",
    "    example = tf.io.parse_single_example(example, features)\n",
    "\n",
    "    return (example['content_id'].values, example['answered_correctly'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "\n",
    "    for f in folds:\n",
    "        groups = (df_trn_attn[df_trn_attn.fold == f][['user_id', 'content_id', 'answered_correctly']]\n",
    "                  .groupby('user_id').apply(lambda r: (r['content_id'].values,\n",
    "                                                       r['answered_correctly'].values)))\n",
    "        n_files = len(groups) // int(3e3)\n",
    "\n",
    "        for i, split in enumerate(np.array_split(groups, n_files)):\n",
    "            out_path = f'gs://{BUCKET}/tfrecords'\n",
    "            filename = f'sequence-{f}-{i:02d}-{len(split)}.tfrec'\n",
    "            record_file = f'{out_path}/{filename}'\n",
    "\n",
    "            with tf.io.TFRecordWriter(record_file) as writer:\n",
    "                for g in tqdm(split.to_numpy()):\n",
    "                    writer.write(serialize_example(g))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datasets from TFRecords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ds_tfrec(folds, len_seq=20, batch_size=64, repeat=True, shuffle=1000):\n",
    "    file_pat = 'gs://{BUCKET}/tfrecords/sequence-{f}*.tfrec'\n",
    "    file_pats = [file_pat.format(BUCKET=BUCKET,f=f) for f in folds]\n",
    "    options = tf.data.Options()\n",
    "    \n",
    "    ds = (tf.data.Dataset.list_files(file_pats, shuffle=True)\n",
    "          .with_options(options)\n",
    "          .interleave(tf.data.TFRecordDataset, num_parallel_calls=AUTO)\n",
    "          .map(parse_example, num_parallel_calls=AUTO)\n",
    "          .map(partial(seq_fix_len, len_seq=len_seq), num_parallel_calls=AUTO)\n",
    "          )\n",
    "    \n",
    "    if shuffle is not None:\n",
    "        ds = ds.shuffle(shuffle)\n",
    "    \n",
    "    ds = ds.repeat() if repeat else ds\n",
    "    ds = ds.batch(batch_size).map(add_mask_to_batch, num_parallel_calls=AUTO)\n",
    "    \n",
    "    return ds.prefetch(AUTO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset from DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "sRhx9yBlNeTR"
   },
   "outputs": [],
   "source": [
    "def get_ds(df, batch_size=1024, repeat=True):\n",
    "    row_id = df.pop('row_id')\n",
    "    y = df.pop('answered_correctly')   \n",
    "    ds = tf.data.Dataset.from_tensor_slices(df, y)\n",
    "    ds = ds.shuffle(int(5e6))\n",
    "    ds = ds.repeat() if repeat else ds\n",
    "    ds = ds.batch(batch_size)\n",
    "    return ds.prefetch(AUTO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ds_df(df, len_seq, batch_size, repeat=True, shuffle=None):\n",
    "    group = (df[['user_id', 'content_id', 'answered_correctly']]\n",
    "             .groupby('user_id').apply(lambda r: ( r['content_id'].values,\n",
    "                                              r['answered_correctly'].values)))\n",
    "    \n",
    "    ex_seq, res_seq = zip(*map(partial(get_seq, max_seq_len=len_seq), group))\n",
    "    t_slices = (tf.stack(ex_seq, axis=0), tf.stack(res_seq, axis=0))\n",
    "    ds = tf.data.Dataset.from_tensor_slices(t_slices)\n",
    "    \n",
    "    if shuffle is not None:\n",
    "        ds = ds.shuffle(shuffle)\n",
    "    \n",
    "    ds = ds.repeat() if repeat else ds\n",
    "    ds = ds.batch(batch_size).map(add_mask_to_batch, num_parallel_calls=AUTO)\n",
    "    \n",
    "    return ds.prefetch(AUTO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "with strategy.scope():\n",
    "    def get_angles(pos, i, d_model):\n",
    "        angle_rates = 1 / np.power(10000, (2 * (i//2)) / np.float32(d_model))\n",
    "        return pos * angle_rates\n",
    "\n",
    "    def positional_encoding(position, d_model):\n",
    "        angle_rads = get_angles(np.arange(position)[:, np.newaxis],\n",
    "                              np.arange(d_model)[np.newaxis, :],\n",
    "                              d_model)\n",
    "\n",
    "        # apply sin to even indices in the array; 2i\n",
    "        angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
    "\n",
    "        # apply cos to odd indices in the array; 2i+1\n",
    "        angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
    "\n",
    "        pos_encoding = angle_rads[np.newaxis, ...]\n",
    "\n",
    "        return tf.cast(pos_encoding, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "with strategy.scope():\n",
    "    def scaled_dot_product_attention(q, k, v, mask):\n",
    "        \"\"\"Calculate the attention weights.\n",
    "        q, k, v must have matching leading dimensions.\n",
    "        k, v must have matching penultimate dimension, i.e.: seq_len_k = seq_len_v.\n",
    "        The mask has different shapes depending on its type(padding or look ahead) \n",
    "        but it must be broadcastable for addition.\n",
    "\n",
    "        Args:\n",
    "        q: query shape == (..., seq_len_q, depth)\n",
    "        k: key shape == (..., seq_len_k, depth)\n",
    "        v: value shape == (..., seq_len_v, depth_v)\n",
    "        mask: Float tensor with shape broadcastable \n",
    "              to (..., seq_len_q, seq_len_k). Defaults to None.\n",
    "\n",
    "        Returns:\n",
    "        output, attention_weights\n",
    "        \"\"\"\n",
    "\n",
    "        matmul_qk = tf.matmul(q, k, transpose_b=True)  # (..., seq_len_q, seq_len_k)\n",
    "\n",
    "        # scale matmul_qk\n",
    "        dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
    "        scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
    "\n",
    "        # add the mask to the scaled tensor.\n",
    "        if mask is not None:\n",
    "            scaled_attention_logits += (mask * -1e9)  \n",
    "\n",
    "        # softmax is normalized on the last axis (seq_len_k) so that the scores\n",
    "        # add up to 1.\n",
    "        attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)  # (..., seq_len_q, seq_len_k)\n",
    "\n",
    "        output = tf.matmul(attention_weights, v)  # (..., seq_len_q, depth_v)\n",
    "\n",
    "        return output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#what gets returned are the weighted values for each word in the sequence along\n",
    "#with the weights that were applied to the values to get the weighted values\n",
    "#the weights were determined by multiplying the embedding representation of each\n",
    "#word three trainable matrices, q, k, v, and then multiplying q x k.\n",
    "# embed x (Q, K, V) --> q, k, v --> weights = softmax(q x k)/scaled  --> weighted values = weights * values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_out(q, k, v):\n",
    "    temp_out, temp_attn = scaled_dot_product_attention(\n",
    "      q, k, v, None)\n",
    "    print ('Attention weights are:')\n",
    "    print (temp_attn)\n",
    "    print ('Output is:')\n",
    "    print (temp_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights are:\n",
      "tf.Tensor([[0. 1. 0. 0.]], shape=(1, 4), dtype=float32)\n",
      "Output is:\n",
      "tf.Tensor([[10.  0.]], shape=(1, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "np.set_printoptions(suppress=True)\n",
    "\n",
    "temp_k = tf.constant([[10,0,0],\n",
    "                      [0,10,0],\n",
    "                      [0,0,10],\n",
    "                      [0,0,10]], dtype=tf.float32)  # (4, 3)\n",
    "\n",
    "temp_v = tf.constant([[   1,0],\n",
    "                      [  10,0],\n",
    "                      [ 100,5],\n",
    "                      [1000,6]], dtype=tf.float32)  # (4, 2)\n",
    "\n",
    "# This `query` aligns with the second `key`,\n",
    "# so the second `value` is returned.\n",
    "temp_q = tf.constant([[0, 10, 0]], dtype=tf.float32)  # (1, 3)\n",
    "print_out(temp_q, temp_k, temp_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "with strategy.scope():\n",
    "    class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "        def __init__(self, d_model, num_heads):\n",
    "            super(MultiHeadAttention, self).__init__()\n",
    "            self.num_heads = num_heads\n",
    "            self.d_model = d_model\n",
    "\n",
    "            assert d_model % self.num_heads == 0\n",
    "\n",
    "            self.depth = d_model // self.num_heads\n",
    "\n",
    "            self.wq = tf.keras.layers.Dense(d_model)\n",
    "            self.wk = tf.keras.layers.Dense(d_model)\n",
    "            self.wv = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "            self.dense = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "        def split_heads(self, x, batch_size):\n",
    "            \"\"\"Split the last dimension into (num_heads, depth).\n",
    "            Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
    "            \"\"\"\n",
    "            x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
    "            return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "\n",
    "        def call(self, v, k, q, mask):\n",
    "            batch_size = tf.shape(q)[0]\n",
    "\n",
    "            q = self.wq(q)  # (batch_size, seq_len, d_model)\n",
    "            k = self.wk(k)  # (batch_size, seq_len, d_model)\n",
    "            v = self.wv(v)  # (batch_size, seq_len, d_model)\n",
    "\n",
    "            # (batch_size, num_heads, seq_len_q, depth)\n",
    "            q = self.split_heads(q, batch_size)\n",
    "            # (batch_size, num_heads, seq_len_k, depth)\n",
    "            k = self.split_heads(k, batch_size)\n",
    "            # (batch_size, num_heads, seq_len_v, depth)\n",
    "            v = self.split_heads(v, batch_size)\n",
    "\n",
    "            # scaled_attention.shape == (batch_size, num_heads, seq_len_q, depth)\n",
    "            # attention_weights.shape == (batch_size, num_heads, seq_len_q, seq_len_k)\n",
    "            scaled_attention, attention_weights = scaled_dot_product_attention(\n",
    "                q, k, v, mask)\n",
    "\n",
    "            # (batch_size, seq_len_q, num_heads, depth)\n",
    "            scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])\n",
    "\n",
    "            concat_attention = tf.reshape(scaled_attention,\n",
    "                                          (batch_size, -1, self.d_model))  # (batch_size, seq_len_q, d_model)\n",
    "\n",
    "            # (batch_size, seq_len_q, d_model)\n",
    "            output = self.dense(concat_attention)\n",
    "\n",
    "            return output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([1, 60, 512]), TensorShape([1, 8, 60, 60]))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_mha = MultiHeadAttention(d_model=512, num_heads=8)\n",
    "y = tf.random.uniform((1, 60, 512))  # (batch_size, encoder_sequence, d_model)\n",
    "\n",
    "# d_model = 512, depth = 64\n",
    "# split d_model weights in to num_heads x d_model/depth matrices\n",
    "# calc output and weights for each head\n",
    "# weights for each head include a weight for each position in the sequence for \n",
    "# every position in the sequence\n",
    "# concat the output back together to have the same depth as the model again\n",
    "# so you essentially have 8 sets of output for each position in the sequence\n",
    "# and then run that through a dense layer to decide how to weight each of the d_model\n",
    "# values returned in the concat\n",
    "\n",
    "out, attn = temp_mha(y, k=y, q=y, mask=None)\n",
    "out.shape, attn.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dff means depth of feadforward\n",
    "with strategy.scope():\n",
    "    def point_wise_feed_forward_network(d_model, dff):\n",
    "        return tf.keras.Sequential([\n",
    "          tf.keras.layers.Dense(dff, activation='relu'),  # (batch_size, seq_len, dff)\n",
    "          tf.keras.layers.Dense(d_model)  # (batch_size, seq_len, d_model)\n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "with strategy.scope():\n",
    "    class EncoderLayer(tf.keras.layers.Layer):\n",
    "        def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
    "            super(EncoderLayer, self).__init__()\n",
    "\n",
    "            self.mha = MultiHeadAttention(d_model, num_heads)\n",
    "            self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
    "\n",
    "            self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "            self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "            self.dropout1 = tf.keras.layers.Dropout(rate)\n",
    "            self.dropout2 = tf.keras.layers.Dropout(rate)\n",
    "\n",
    "        def call(self, exer, mask, training):\n",
    "\n",
    "            # (batch_size, input_seq_len, d_model)\n",
    "            attn_output, _ = self.mha(exer, exer, exer, mask)\n",
    "            attn_output = self.dropout1(attn_output, training=training)\n",
    "            # (batch_size, input_seq_len, d_model)\n",
    "            out1 = self.layernorm1(exer + attn_output)\n",
    "\n",
    "            ffn_output = self.ffn(out1)  # (batch_size, input_seq_len, d_model)\n",
    "            ffn_output = self.dropout2(ffn_output, training=training)\n",
    "            # (batch_size, input_seq_len, d_model)\n",
    "            out2 = self.layernorm2(out1 + ffn_output)\n",
    "\n",
    "            return out2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "with strategy.scope():\n",
    "    class DecoderLayer(tf.keras.layers.Layer):\n",
    "        def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
    "            super(DecoderLayer, self).__init__()\n",
    "\n",
    "            self.mha1 = MultiHeadAttention(d_model, num_heads)\n",
    "            self.mha2 = MultiHeadAttention(d_model, num_heads)\n",
    "\n",
    "            self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
    "\n",
    "            self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "            self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "            self.layernorm3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "            self.dropout1 = tf.keras.layers.Dropout(rate)\n",
    "            self.dropout2 = tf.keras.layers.Dropout(rate)\n",
    "            self.dropout3 = tf.keras.layers.Dropout(rate)\n",
    "\n",
    "        def call(self, resp_prior, enc_output, mask_prior, training):\n",
    "            # enc_output.shape == (batch_size, input_seq_len, d_model)\n",
    "\n",
    "            # (batch_size, target_seq_len, d_model)\n",
    "            attn1, attn_weights_block1 = self.mha1(resp_prior, resp_prior, resp_prior, mask_prior)\n",
    "            attn1 = self.dropout1(attn1, training=training)\n",
    "            out1 = self.layernorm1(attn1 + resp_prior)\n",
    "\n",
    "            # no need to include the mask here as it has already been included\n",
    "            # in the generation of the encoder output and the first decoder layer\n",
    "            attn2, attn_weights_block2 = self.mha2(enc_output, enc_output, out1, None)  # (batch_size, target_seq_len, d_model)\n",
    "            attn2 = self.dropout2(attn2, training=training)\n",
    "            # (batch_size, target_seq_len, d_model)\n",
    "            out2 = self.layernorm2(attn2 + out1)\n",
    "\n",
    "            ffn_output = self.ffn(out2)  # (batch_size, target_seq_len, d_model)\n",
    "            ffn_output = self.dropout3(ffn_output, training=training)\n",
    "            # (batch_size, target_seq_len, d_model)\n",
    "            out3 = self.layernorm3(ffn_output + out2)\n",
    "\n",
    "            return out3, attn_weights_block1, attn_weights_block2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "with strategy.scope():\n",
    "    class Encoder(tf.keras.layers.Layer):\n",
    "        def __init__(self, num_layers, d_model, num_heads, dff, exer_size,\n",
    "                     maximum_position_encoding, rate=0.1):\n",
    "            super(Encoder, self).__init__()\n",
    "\n",
    "            self.d_model = d_model\n",
    "            self.num_layers = num_layers\n",
    "\n",
    "            self.embedding = tf.keras.layers.Embedding(exer_size, d_model)\n",
    "            self.pos_encoding = positional_encoding(maximum_position_encoding,\n",
    "                                                    self.d_model)\n",
    "\n",
    "            self.enc_layers = [EncoderLayer(d_model, num_heads, dff, rate)\n",
    "                               for _ in range(num_layers)]\n",
    "\n",
    "            self.dropout = tf.keras.layers.Dropout(rate)\n",
    "\n",
    "        def call(self, exer, mask, training):\n",
    "\n",
    "            seq_len = tf.shape(exer)[1]\n",
    "\n",
    "            # adding embedding and position encoding.\n",
    "            x = self.embedding(exer)  # (batch_size, input_seq_len, d_model)\n",
    "            x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "            x += self.pos_encoding[:, :seq_len, :]\n",
    "\n",
    "            x = self.dropout(x, training=training)\n",
    "\n",
    "            for i in range(self.num_layers):\n",
    "                x = self.enc_layers[i](x, mask, training)\n",
    "\n",
    "            return x  # (batch_size, input_seq_len, d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "with strategy.scope():\n",
    "    class Decoder(tf.keras.layers.Layer):\n",
    "        def __init__(self, num_layers, d_model, num_heads, dff, resp_size,\n",
    "                     maximum_position_encoding, rate=0.1):\n",
    "            super(Decoder, self).__init__()\n",
    "\n",
    "            self.d_model = d_model\n",
    "            self.num_layers = num_layers\n",
    "\n",
    "            self.embedding = tf.keras.layers.Embedding(resp_size, d_model)\n",
    "            self.pos_encoding = positional_encoding(\n",
    "                maximum_position_encoding, d_model)\n",
    "\n",
    "            self.dec_layers = [DecoderLayer(d_model, num_heads, dff, rate)\n",
    "                               for _ in range(num_layers)]\n",
    "            self.dropout = tf.keras.layers.Dropout(rate)\n",
    "\n",
    "        def call(self, resp_prior, enc_output, mask_prior, training):\n",
    "\n",
    "            seq_len = tf.shape(resp_prior)[1]\n",
    "            attention_weights = {}\n",
    "\n",
    "            x = self.embedding(resp_prior)  # (batch_size, target_seq_len, d_model)\n",
    "            x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "            x += self.pos_encoding[:, :seq_len, :]\n",
    "\n",
    "            x = self.dropout(x, training=training)\n",
    "\n",
    "            for i in range(self.num_layers):\n",
    "                x, block1, block2 = self.dec_layers[i](x, enc_output, mask_prior, training)\n",
    "\n",
    "                attention_weights['decoder_layer{}_block1'.format(i+1)] = block1\n",
    "                attention_weights['decoder_layer{}_block2'.format(i+1)] = block2\n",
    "\n",
    "            # x.shape == (batch_size, target_seq_len, d_model)\n",
    "            return x, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "with strategy.scope():\n",
    "    class Transformer(tf.keras.Model):\n",
    "        def __init__(self, num_layers, d_model, num_heads, dff, exer_size,\n",
    "                     resp_size, pe_input, pe_target, rate=0.1):\n",
    "            super(Transformer, self).__init__()\n",
    "\n",
    "            self.encoder = Encoder(num_layers, d_model, num_heads, dff,\n",
    "                                   exer_size, pe_input, rate)\n",
    "\n",
    "            self.decoder = Decoder(num_layers, d_model, num_heads, dff,\n",
    "                                   resp_size, pe_target, rate)\n",
    "\n",
    "            self.final_layer = tf.keras.layers.Dense(1)\n",
    "\n",
    "        def call(self, exer, resp_prior, mask_current, mask_prior, training):\n",
    "\n",
    "            # (batch_size, inp_seq_len, d_model)\n",
    "            enc_output = self.encoder(exer, mask_current, training)\n",
    "\n",
    "            # dec_output.shape == (batch_size, tar_seq_len, d_model)\n",
    "            dec_output, attention_weights = self.decoder(resp_prior, enc_output, mask_prior, training)\n",
    "\n",
    "            # (batch_size, tar_seq_len, 1)\n",
    "            final_output = self.final_layer(dec_output)\n",
    "\n",
    "    #         final_final_output = self.final_final_layer(tf.reshape(final_output, shape[:-1]))\n",
    "\n",
    "            return tf.reduce_sum(final_output, -2), attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "with strategy.scope():\n",
    "    class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "        def __init__(self, d_model, warmup_steps=4000):\n",
    "            super(CustomSchedule, self).__init__()\n",
    "\n",
    "            self.d_model = d_model\n",
    "            self.d_model = tf.cast(self.d_model, tf.float32)\n",
    "\n",
    "            self.warmup_steps = warmup_steps\n",
    "\n",
    "        def __call__(self, step):\n",
    "            arg1 = tf.math.rsqrt(step)\n",
    "            arg2 = step * (self.warmup_steps ** -1.5)\n",
    "\n",
    "            return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(resp, pred):\n",
    "    resp_current = resp[:, 1:]\n",
    "    resp_binary = tf.cast(tf.math.equal(resp_current, 2), tf.float32)\n",
    "    \n",
    "    mask_current = tf.cast(tf.math.not_equal(resp_current, 0), tf.float32)\n",
    "    loss_ = loss_object(tf.expand_dims(resp_binary, -1), tf.expand_dims(pred, -1))\n",
    "    loss_ *= mask_current\n",
    "\n",
    "    return tf.reduce_sum(loss_)/tf.reduce_sum(mask_current)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(resp, pred):\n",
    "    resp_current = resp[:, -1]\n",
    "    resp_binary = tf.cast(tf.math.equal(resp_current, 2), tf.float32)\n",
    "    \n",
    "    loss = loss_object(resp_binary, pred)\n",
    "\n",
    "    return tf.reduce_sum(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24.0"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1536/8/8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9684"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EXER_SIZE = df_trn_attn.content_id.max() + 1\n",
    "RESP_SIZE = df_trn_attn.answered_correctly.max() + 1\n",
    "LEN_SEQ = 128\n",
    "\n",
    "num_layers = 8\n",
    "d_model = 512\n",
    "dff = 2048\n",
    "num_heads = 8\n",
    "dropout_rate = 0.5\n",
    "\n",
    "BATCH_SIZE = 64 * strategy.num_replicas_in_sync\n",
    "EPOCHS = 2\n",
    "folds_trn, folds_val = folds[1:], folds[:1]\n",
    "\n",
    "num_ex_trn = df_trn_attn.fold.isin(folds_trn).sum()\n",
    "num_ex_val = df_trn_attn.fold.isin(folds_val).sum()\n",
    "steps_per_epoch = num_ex_trn // BATCH_SIZE\n",
    "steps_val = num_ex_val // BATCH_SIZE\n",
    "steps_per_epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TPU Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "with strategy.scope():\n",
    "    transformer = Transformer(num_layers, d_model, num_heads, dff,\n",
    "                              EXER_SIZE, RESP_SIZE, \n",
    "                              pe_input=10000, \n",
    "                              pe_target=10000,\n",
    "                              rate=dropout_rate)\n",
    "\n",
    "    learning_rate = CustomSchedule(d_model)\n",
    "\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98, \n",
    "                                         epsilon=1e-9)\n",
    "\n",
    "    loss_function = tf.keras.losses.BinaryCrossentropy(\n",
    "        from_logits=True, reduction=tf.keras.losses.Reduction.SUM)\n",
    "    \n",
    "    splits = ['train', 'val']\n",
    "    metrics = {s: {} for s in splits}\n",
    "    metric_fns = {\n",
    "        'loss': tf.keras.metrics.Mean,\n",
    "        'accuracy': tf.keras.metrics.BinaryAccuracy,\n",
    "        'auc': tf.keras.metrics.AUC\n",
    "    }\n",
    "    \n",
    "    for s in splits:\n",
    "        for m in metric_fns:\n",
    "            metrics[s][m] = metric_fns[m](name=f'{s}_{m}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(transformer, optimizer, loss_function, metrics, ds_iter, steps_per_epoch):\n",
    "    def train_step_fn(exer, resp_prior, mask_current, mask_prior, label):\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            pred, _ = transformer(exer, resp_prior, mask_current, mask_prior, True)\n",
    "            loss = loss_function(label, pred)\n",
    "#             loss = tf.nn.compute_average_loss(loss, global_batch_size=BATCH_SIZE)\n",
    "\n",
    "        gradients = tape.gradient(loss, transformer.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n",
    "\n",
    "        pred_prob = tf.math.sigmoid(pred)\n",
    "        \n",
    "        for n, m in metrics.items():\n",
    "            if n == 'loss':\n",
    "                m(loss)\n",
    "            else:\n",
    "                m(label, pred_prob)\n",
    "        \n",
    "    for _ in tf.range(steps_per_epoch):\n",
    "        strategy.run(train_step_fn, next(ds_iter))\n",
    "        \n",
    "@tf.function\n",
    "def val_step(transformer, loss_function, metrics, ds_iter, steps_per_epoch):\n",
    "    def val_step_fn(exer, resp_prior, mask_current, mask_prior, label):\n",
    "        pred, _ = transformer(exer, resp_prior, mask_current, mask_prior, False)\n",
    "        loss = loss_function(label, pred)\n",
    "        \n",
    "        pred_prob = tf.math.sigmoid(pred)\n",
    "        \n",
    "        for n, m in metrics.items():\n",
    "            if n == 'loss':\n",
    "                m(loss)\n",
    "            else:\n",
    "                m(label, pred_prob)\n",
    "        \n",
    "    for _ in tf.range(steps_per_epoch):\n",
    "        strategy.run(val_step_fn, next(ds_iter))        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ds_trn = get_ds_tfrec([0], LEN_SEQ, BATCH_SIZE)\n",
    "ds_trn_iter = iter(strategy.experimental_distribute_dataset(ds_trn))\n",
    "\n",
    "ds_val = get_ds_tfrec([1], LEN_SEQ, BATCH_SIZE)\n",
    "ds_val_iter = iter(strategy.experimental_distribute_dataset(ds_val))\n",
    "\n",
    "results = {s: {m: [] for m in metrics[s]} for s in splits}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1, train_loss: 16.5610, train_accuracy: 0.5113, train_auc: 0.5108, val_loss: 39.4261, val_accuracy: 0.5366, val_auc: 0.5000, time: 2316.2 sec.\n",
      "2, train_loss: 6.9257, train_accuracy: 0.5145, train_auc: 0.5131, val_loss: 14.4494, val_accuracy: 0.5371, val_auc: 0.5000, time: 1899.6 sec.\n",
      "3, train_loss: 4.7807, train_accuracy: 0.5201, train_auc: 0.5198, val_loss: 23.0202, val_accuracy: 0.5374, val_auc: 0.5000, time: 1899.5 sec.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-42-3807508005a2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mm\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mm_dict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mm_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m             \u001b[0mepoch_results\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'{s}_{m}: {result:0.4f}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m             \u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m             \u001b[0mm_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_states\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m__format__\u001b[0;34m(self, format_spec)\u001b[0m\n\u001b[1;32m    887\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__format__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat_spec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__format__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mformat_spec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__reduce__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/core.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, message, code)\u001b[0m\n\u001b[1;32m     37\u001b[0m   \u001b[0;34m\"\"\"Exception class to handle not ok Status.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m   \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m     \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(10):\n",
    "    start = time.perf_counter()\n",
    "    train_step(transformer, optimizer, loss_function, metrics['train'], ds_trn_iter, 100)\n",
    "    val_step(transformer, loss_function, metrics['val'], ds_val_iter, 100)\n",
    "    \n",
    "    epoch_results = [f'{epoch + 1}']\n",
    "    for s, m_dict in metrics.items():\n",
    "        for m in m_dict:\n",
    "            result = m_dict[m].result()\n",
    "            epoch_results.append(f'{s}_{m}: {result:0.4f}')\n",
    "            results[s][m].append(result)\n",
    "            m_dict[m].reset_states()\n",
    "    \n",
    "    epoch_results.append(f'time: {time.perf_counter() - start:0.1f} sec.')\n",
    "    print((', ').join(epoch_results))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## CPU Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "cpu_training = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "if cpu_training:\n",
    "    learning_rate = CustomSchedule(d_model)\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98, \n",
    "                                     epsilon=1e-9)\n",
    "    loss_function = tf.keras.losses.BinaryCrossentropy(\n",
    "        from_logits=True, reduction=tf.keras.losses.Reduction.SUM)\n",
    "    train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "    train_accuracy = tf.keras.metrics.BinaryAccuracy(name='train_accuracy')\n",
    "    train_auc = tf.keras.metrics.AUC(name='train_auc')\n",
    "       \n",
    "    plt.plot(learning_rate(tf.range(40000, dtype=tf.float32)))\n",
    "    plt.ylabel(\"Learning Rate\")\n",
    "    plt.xlabel(\"Train Step\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "if cpu_training:\n",
    "    checkpoint_path = \"./checkpoints/train\"\n",
    "\n",
    "    ckpt = tf.train.Checkpoint(transformer=transformer,\n",
    "                               optimizer=optimizer)\n",
    "\n",
    "    ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n",
    "\n",
    "    # if a checkpoint exists, restore the latest checkpoint.\n",
    "    if ckpt_manager.latest_checkpoint:\n",
    "        ckpt.restore(ckpt_manager.latest_checkpoint)\n",
    "        print ('Latest checkpoint restored!!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# The @tf.function trace-compiles train_step into a TF graph for faster\n",
    "# execution. The function specializes to the precise shape of the argument\n",
    "# tensors. To avoid re-tracing due to the variable sequence lengths or variable\n",
    "# batch sizes (the last batch is smaller), use input_signature to specify\n",
    "# more generic shapes.\n",
    "\n",
    "if cpu_training:\n",
    "    train_step_signature = [\n",
    "        tf.TensorSpec(shape=(None, None), dtype=tf.int64),\n",
    "        tf.TensorSpec(shape=(None, None), dtype=tf.int64),\n",
    "        tf.TensorSpec(shape=(None, None, None, None), dtype=tf.float32),\n",
    "        tf.TensorSpec(shape=(None, None, None, None), dtype=tf.float32)\n",
    "    ]\n",
    "\n",
    "\n",
    "    @tf.function(input_signature=train_step_signature)\n",
    "    def train_step(exer, resp, mask_pad, mask_future):\n",
    "        resp_current = resp[:, -1]\n",
    "        resp_binary = tf.cast(tf.math.equal(resp_current, 2), tf.float32)\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            pred, _ = transformer(exer, resp, mask_pad, mask_future, True)\n",
    "            loss = loss_function(resp_binary, pred)\n",
    "\n",
    "        gradients = tape.gradient(loss, transformer.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n",
    "\n",
    "        pred_prob = tf.math.sigmoid(pred)\n",
    "        train_loss(loss)\n",
    "        train_accuracy(resp_binary, pred_prob)\n",
    "        train_auc(resp_binary, pred_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "if cpu_training:\n",
    "    results = {m: [] for m in ['loss', 'accuracy', 'duration']}\n",
    "    ds_trn = get_ds(df_trn_attn[df_trn_attn.fold == 0], len_seq=LEN_SEQ, batch_size=BATCH_SIZE)\n",
    "    b = next(iter(ds_trn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "if cpu_training:\n",
    "    transformer = Transformer(num_layers, d_model, num_heads, dff,\n",
    "                              EXER_SIZE, RESP_SIZE, \n",
    "                              pe_input=10000, \n",
    "                              pe_target=10000,\n",
    "                              rate=dropout_rate)\n",
    "    \n",
    "    pred, _ = transformer(*b, True)\n",
    "\n",
    "    print(transformer.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if cpu_training:\n",
    "    for epoch in range(EPOCHS):\n",
    "        start = time.time()\n",
    "\n",
    "        train_loss.reset_states()\n",
    "        train_accuracy.reset_states()\n",
    "        train_auc.reset_states()\n",
    "\n",
    "        # inp -> portuguese, tar -> english\n",
    "        for (batch, (exer, resp, mask_pad, mask_future)) in enumerate(ds_trn):\n",
    "            train_step(exer, resp, mask_pad, mask_future)\n",
    "\n",
    "            if batch % 50 == 0:\n",
    "                print('Batch {} Loss {:.4f} Accuracy {:.4f} AUC {:.4f}'.format(batch,\n",
    "                        train_loss.result(), train_accuracy.result(), train_auc.result()))\n",
    "                \n",
    "            if batch == 1000:\n",
    "                break\n",
    "\n",
    "#         if (epoch + 1) % 5 == 0:\n",
    "#             ckpt_save_path = ckpt_manager.save()\n",
    "#             print('Saving checkpoint for epoch {} at {}'.format(epoch+1,\n",
    "#                                                                 ckpt_save_path))\n",
    "\n",
    "        print('Epoch {} Loss {:.4f} Accuracy {:.4f}'.format(epoch + 1, train_loss.result(), train_accuracy.result()))\n",
    "\n",
    "        print('Time taken for 1 epoch: {} secs\\n'.format(time.time() - start))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "collaborative_filtering.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "environment": {
   "name": "tf2-2-3-gpu.2-3.m59",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-2-3-gpu.2-3:m59"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "285px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0036b023c2224f458414b8053f42ee55": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "011d43b43aed485a8751ec48d9d95bab": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "014d6e6088084f31ac20b506f8e9d0a6": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c835dd42207646a1a06a02725c3732c6",
      "placeholder": "",
      "style": "IPY_MODEL_0036b023c2224f458414b8053f42ee55",
      "value": " 6478025/6478025 [24:49&lt;00:00, 4350.54rows/s]"
     }
    },
    "084f2d9358df47df8cdb1874c203fcb8": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_f25723a2fc8f4192b8b3adde2cd0d70f",
       "IPY_MODEL_e96d00e7c2c44755866ebbbf77b5af42"
      ],
      "layout": "IPY_MODEL_214f3fcaeb4d49e0a47be36f3c6f854f"
     }
    },
    "08df8883af494b70b718a7ce5b4f9386": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "09792541227541a1872ae6f20d18651f": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Downloading: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_58fb8e31ffae410aa9788c36487ff2ab",
      "max": 1294272,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_7877600264494ffe910f2edf1f20e8e4",
      "value": 1294272
     }
    },
    "0ea64c8534aa4184b0c7c3eba217aec0": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d5e4c6e070c0413491222f9edf1bba2c",
      "placeholder": "",
      "style": "IPY_MODEL_011d43b43aed485a8751ec48d9d95bab",
      "value": " 1049559/1049559 [1:09:12&lt;00:00, 252.76it/s]"
     }
    },
    "1068312cf5104622af904af5507ffdfb": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Downloading: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_68b08e46b6b94cb7907dd3259deeead1",
      "max": 6478025,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_46f7c96c423f43938916d9f3ff8f8d59",
      "value": 6478025
     }
    },
    "214f3fcaeb4d49e0a47be36f3c6f854f": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "235676a015614259bc3d1372b7e0c020": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_b1b6224e2bc2416bb344798c1add2537",
       "IPY_MODEL_81d505ad30d34683b646252cc1e830cb"
      ],
      "layout": "IPY_MODEL_5663d015758f430caeb16b63ff43841e"
     }
    },
    "46f7c96c423f43938916d9f3ff8f8d59": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "488f106b90e64e509091124e15ac86ea": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_8c277c0d926c4c2ebb53cdf71e2df986",
       "IPY_MODEL_0ea64c8534aa4184b0c7c3eba217aec0"
      ],
      "layout": "IPY_MODEL_7371a2da83c047fc921425b910048107"
     }
    },
    "4ad22a4a79e74753b9fedac8e2b469ea": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "5663d015758f430caeb16b63ff43841e": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "58fb8e31ffae410aa9788c36487ff2ab": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5918d69949e443bbb43353bfb48fc803": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_09792541227541a1872ae6f20d18651f",
       "IPY_MODEL_781442d17ed048b8a8b1602673ba0a48"
      ],
      "layout": "IPY_MODEL_5f5799f25b1d4f4e853a48660007fadf"
     }
    },
    "5f5799f25b1d4f4e853a48660007fadf": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "68b08e46b6b94cb7907dd3259deeead1": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6f4ece440f634846a98fd6aa65585679": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "7371a2da83c047fc921425b910048107": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "74fdbb0ef57a46e4b589a46c18ec042d": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "781442d17ed048b8a8b1602673ba0a48": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e8bea655634949328e13aa6e52f87e34",
      "placeholder": "",
      "style": "IPY_MODEL_b20574a0dd89443d997334e4d5cec717",
      "value": " 1294272/1294272 [03:30&lt;00:00, 6138.32rows/s]"
     }
    },
    "7877600264494ffe910f2edf1f20e8e4": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "7dc3eef3c43b468093264474aec72870": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "81a515c98c30497f81232d44e94cb123": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "81d505ad30d34683b646252cc1e830cb": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c41cf7975b7740ecba4acda661d32434",
      "placeholder": "",
      "style": "IPY_MODEL_6f4ece440f634846a98fd6aa65585679",
      "value": " 5340377/5340377 [03:45&lt;00:00, 23656.96rows/s]"
     }
    },
    "8c277c0d926c4c2ebb53cdf71e2df986": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_08df8883af494b70b718a7ce5b4f9386",
      "max": 1049559,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_74fdbb0ef57a46e4b589a46c18ec042d",
      "value": 1049559
     }
    },
    "9e709767b51a4e5ebfbaa5d747c4ad4f": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "b1b6224e2bc2416bb344798c1add2537": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Downloading: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_81a515c98c30497f81232d44e94cb123",
      "max": 5340377,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_c16536e2ef3e440da7f1fd3d6af0944b",
      "value": 5340377
     }
    },
    "b20574a0dd89443d997334e4d5cec717": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "c16536e2ef3e440da7f1fd3d6af0944b": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "c41cf7975b7740ecba4acda661d32434": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c835dd42207646a1a06a02725c3732c6": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c9e63655621b4e12ad8bcac2712ca812": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d5a3b39de6d3404d910394ddf6439548": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d5e4c6e070c0413491222f9edf1bba2c": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "da20e26860b04d3f9b7aec2866f1a43c": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_1068312cf5104622af904af5507ffdfb",
       "IPY_MODEL_014d6e6088084f31ac20b506f8e9d0a6"
      ],
      "layout": "IPY_MODEL_7dc3eef3c43b468093264474aec72870"
     }
    },
    "e8bea655634949328e13aa6e52f87e34": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e96d00e7c2c44755866ebbbf77b5af42": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d5a3b39de6d3404d910394ddf6439548",
      "placeholder": "",
      "style": "IPY_MODEL_4ad22a4a79e74753b9fedac8e2b469ea",
      "value": " 6478025/6478025 [01:54&lt;00:00, 56744.46it/s]"
     }
    },
    "f25723a2fc8f4192b8b3adde2cd0d70f": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c9e63655621b4e12ad8bcac2712ca812",
      "max": 6478025,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_9e709767b51a4e5ebfbaa5d747c4ad4f",
      "value": 6478025
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
